{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is Boosting in Machine Learning?\n",
        "Ans.Boosting in Machine Learning (Python Perspective)\n",
        "Boosting is an ensemble learning technique that combines multiple weak learners (typically decision trees) to create a strong predictive model. It works by training models sequentially, where each new model corrects the errors of the previous ones.\n",
        "How Boosting Works\n",
        "A base model (weak learner) is trained on the dataset.\n",
        "The model's errors (misclassified samples) are identified.\n",
        "More weight is given to misclassified samples, and a new weak learner is trained to focus on these errors.\n",
        "Steps 2-3 are repeated multiple times.\n",
        "The final prediction is made by combining all weak learners (often through weighted voting or averaging).\n",
        "Popular Boosting Algorithms in Python\n",
        "Python has several well-known boosting libraries:\n",
        "\n",
        "1. AdaBoost (Adaptive Boosting)\n",
        "Uses decision stumps (one-level decision trees).\n",
        "Adjusts sample weights to focus on misclassified points."
      ],
      "metadata": {
        "id": "KwRwlEixRg97"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXeIiz52Rfhi"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost with Decision Tree as a weak learner\n",
        "model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Gradient Boosting (GBM)\n",
        "Uses gradient descent to minimize loss.\n",
        "Works well but can be slow for large datasets."
      ],
      "metadata": {
        "id": "vsAEYgP7R561"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Train Gradient Boosting Model\n",
        "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "dfRkPZvqR73i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How does Boosting differ from Bagging\n",
        "ans.Difference Between Boosting and Bagging in Python\n",
        "Both Boosting and Bagging are ensemble learning techniques, but they differ in how they train multiple models and combine their predictions.\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "Trains multiple models in parallel.\n",
        "Uses random subsets (bootstrap sampling) of data for each model.\n",
        "Final prediction is based on majority voting (classification) or averaging (regression).\n",
        "Reduces variance, making models more stable."
      ],
      "metadata": {
        "id": "7dhm82FdSAhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees\n",
        "bagging_model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "QwBeCqhRSFXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Boosting\n",
        "Trains multiple models sequentially.\n",
        "Each model learns from the mistakes of the previous one.\n",
        "Focuses on hard-to-classify samples by adjusting their weights.\n",
        "Reduces bias, making models more accurate."
      ],
      "metadata": {
        "id": "RIwOhucTSJiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Train an AdaBoost Classifier\n",
        "boosting_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\n",
        "boosting_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = boosting_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mL6VecPhSK3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.What is the key idea behind AdaBoost?\n",
        "Ans.Key Idea Behind AdaBoost (Adaptive Boosting) in Python\n",
        "AdaBoost (Adaptive Boosting) is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a strong classifier. The key idea is to focus more on misclassified samples by adjusting their weights and training subsequent models to correct previous mistakes.\n",
        "\n",
        "How AdaBoost Works\n",
        "Train a weak learner (e.g., a decision stump).\n",
        "Identify misclassified samples and increase their weights.\n",
        "Train the next weak learner with updated weights (giving more importance to previously misclassified samples).\n",
        "Repeat steps 1-3 for multiple iterations.\n",
        "Final prediction is made using a weighted majority vote of all weak learners."
      ],
      "metadata": {
        "id": "VoaZzGeNSONa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an AdaBoost Classifier with Decision Trees as weak learners\n",
        "adaboost_model = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner (Decision Stump)\n",
        "    n_estimators=50,  # Number of weak learners\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "x6eAbJlFSWRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Explain the working of AdaBoost with an example\n",
        "Ans.Working of AdaBoost with an Example (Python Implementation)\n",
        "Adaptive Boosting (AdaBoost) is an ensemble learning technique that builds a strong classifier by combining multiple weak learners (usually decision stumps).\n",
        "\n",
        "Step-by-Step Working of AdaBoost\n",
        "1. Initialize Weights\n",
        "Each sample is given an equal weight\n",
        "ùë§\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëÅ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " =\n",
        "N\n",
        "1\n",
        "‚Äã\n",
        "  (where\n",
        "ùëÅ\n",
        "N is the total number of samples).\n",
        "2. Train a Weak Learner (Decision Stump)\n",
        "A weak model (e.g., a decision stump) is trained on the dataset.\n",
        "It predicts the labels, and misclassified samples are identified.\n",
        "3. Compute Model Error (\n",
        "ùúñ\n",
        "œµ)\n",
        "The error rate of the weak learner is calculated as:\n",
        "ùúñ\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëÅ\n",
        "ùë§\n",
        "ùëñ\n",
        "[\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚â†\n",
        "‚Ñé\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ")\n",
        "]\n",
        "œµ=\n",
        "i=1\n",
        "‚àë\n",
        "N\n",
        "‚Äã\n",
        " w\n",
        "i\n",
        "‚Äã\n",
        " [y\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "ÓÄ†\n",
        "=h(x\n",
        "i\n",
        "‚Äã\n",
        " )]\n",
        "where\n",
        "‚Ñé\n",
        "(\n",
        "ùë•\n",
        "ùëñ\n",
        ")\n",
        "h(x\n",
        "i\n",
        "‚Äã\n",
        " ) is the predicted label and\n",
        "ùë¶\n",
        "ùëñ\n",
        "y\n",
        "i\n",
        "‚Äã\n",
        "  is the actual label.\n",
        "4. Compute Model Weight (\n",
        "ùõº\n",
        "Œ±)\n",
        "The model's importance is calculated as:\n",
        "ùõº\n",
        "=\n",
        "1\n",
        "2\n",
        "ln\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùúñ\n",
        "ùúñ\n",
        ")\n",
        "Œ±=\n",
        "2\n",
        "1\n",
        "‚Äã\n",
        " ln(\n",
        "œµ\n",
        "1‚àíœµ\n",
        "‚Äã\n",
        " )\n",
        "A lower error means a higher alpha, so the model is given more importance.\n",
        "5. Update Sample Weights\n",
        "Misclassified samples get higher weights to ensure the next weak learner focuses on them.\n",
        "Weights are updated as:\n",
        "ùë§\n",
        "ùëñ\n",
        "=\n",
        "ùë§\n",
        "ùëñ\n",
        "√ó\n",
        "ùëí\n",
        "ùõº\n",
        "(for¬†misclassified¬†samples)\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " =w\n",
        "i\n",
        "‚Äã\n",
        " √óe\n",
        "Œ±\n",
        " (for¬†misclassified¬†samples)\n",
        "ùë§\n",
        "ùëñ\n",
        "=\n",
        "ùë§\n",
        "ùëñ\n",
        "√ó\n",
        "ùëí\n",
        "‚àí\n",
        "ùõº\n",
        "(for¬†correctly¬†classified¬†samples)\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " =w\n",
        "i\n",
        "‚Äã\n",
        " √óe\n",
        "‚àíŒ±\n",
        " (for¬†correctly¬†classified¬†samples)\n",
        "6. Normalize Weights\n",
        "The weights are normalized so that they sum to 1.\n",
        "7. Repeat Steps 2-6\n",
        "The process repeats for multiple weak learners.\n",
        "8. Final Prediction\n",
        "The final prediction is based on the weighted sum of all weak learners' outputs.\n"
      ],
      "metadata": {
        "id": "PCiogFyHSbd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Create a synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Train AdaBoost Classifier with Decision Stump\n",
        "adaboost = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),  # Weak learner\n",
        "    n_estimators=50,  # Number of weak learners\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Make Predictions\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Step 4: Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Accuracy:\", accuracy)\n",
        "\n",
        "# Step 5: Visualize Decision Boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary\n",
        "plot_decision_boundary(adaboost, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "vtfcTd6lSjMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "Ans.Gradient Boosting vs. AdaBoost in Python\n",
        "Both Gradient Boosting and AdaBoost are boosting algorithms that improve weak learners by training them sequentially. However, they differ in how they adjust models to correct errors.\n",
        "\n",
        "1. What is Gradient Boosting?\n",
        "Gradient Boosting is a boosting technique where models are trained to correct the residual errors (difference between actual and predicted values) using gradient descent.\n",
        "\n",
        "Key Idea of Gradient Boosting\n",
        " Unlike AdaBoost, which adjusts sample weights, Gradient Boosting minimizes the loss function directly using gradients.\n",
        " It builds trees sequentially, where each tree learns from the errors (residuals) of the previous trees.\n",
        "Works well for both classification and regression tasks.\n",
        "\n",
        "2. How Gradient Boosting Works (Step-by-Step)\n",
        "Start with a weak model (usually a Decision Tree).\n",
        "Calculate the residual errors (differences between actual and predicted values).\n",
        "Train a new model to predict these residuals.\n",
        "Add this new model to improve the overall prediction.\n",
        "Repeat the process for multiple iterations.\n",
        "The final prediction is the sum of all weak learners.\n",
        "3. Python Implementation of Gradient Boosting\n",
        "Let‚Äôs implement Gradient Boosting on a dataset."
      ],
      "metadata": {
        "id": "KqV3jJ0_Sk1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "aMUdaX4nS1A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.What is the loss function in Gradient Boosting?\n",
        "Ans.Loss Function in Gradient Boosting (Python Explanation)\n",
        "In Gradient Boosting, the loss function measures how well the model‚Äôs predictions match the actual values. The algorithm minimizes this loss function by adding weak learners that correct previous errors using gradient descent.\n",
        "\n",
        "1. Common Loss Functions in Gradient Boosting\n",
        "Gradient Boosting is flexible and can use different loss functions depending on the task:\n",
        "\n",
        "For Regression Tasks:\n",
        "Mean Squared Error (MSE):\n",
        "ùêø\n",
        "(\n",
        "ùë¶\n",
        ",\n",
        "ùë¶\n",
        "^\n",
        ")\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚àë\n",
        "(\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        " )=\n",
        "n\n",
        "1\n",
        "‚Äã\n",
        " ‚àë(y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "Used when predicting continuous values.\n",
        "Penalizes large errors more heavily.\n",
        "Mean Absolute Error (MAE):\n",
        "ùêø\n",
        "(\n",
        "ùë¶\n",
        ",\n",
        "ùë¶\n",
        "^\n",
        ")\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚àë\n",
        "‚à£\n",
        "ùë¶\n",
        "ùëñ\n",
        "‚àí\n",
        "ùë¶\n",
        "^\n",
        "ùëñ\n",
        "‚à£\n",
        "L(y,\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        " )=\n",
        "n\n",
        "1\n",
        "‚Äã\n",
        " ‚àë‚à£y\n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "y\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " ‚à£\n",
        "More robust to outliers than MSE.\n",
        "For Classification Tasks:\n",
        "Log Loss (Binary Classification):\n",
        "ùêø\n",
        "(\n",
        "ùë¶\n",
        ",\n",
        "ùëù\n",
        "^\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "1\n",
        "ùëõ\n",
        "‚àë\n",
        "[\n",
        "ùë¶\n",
        "log\n",
        "‚Å°\n",
        "ùëù\n",
        "^\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùë¶\n",
        ")\n",
        "log\n",
        "‚Å°\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëù\n",
        "^\n",
        ")\n",
        "]\n",
        "L(y,\n",
        "p\n",
        "^\n",
        "‚Äã\n",
        " )=‚àí\n",
        "n\n",
        "1\n",
        "‚Äã\n",
        " ‚àë[ylog\n",
        "p\n",
        "^\n",
        "‚Äã\n",
        " +(1‚àíy)log(1‚àí\n",
        "p\n",
        "^\n",
        "‚Äã\n",
        " )]\n",
        "Used when predicting binary classes (0 or 1).\n",
        "Encourages the model to produce probability outputs close to true labels.\n",
        "Multi-class Log Loss (Cross-Entropy):\n",
        "ùêø\n",
        "(\n",
        "ùë¶\n",
        ",\n",
        "ùëù\n",
        "^\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùë¶\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "ùëù\n",
        "^\n",
        "ùëñ\n",
        "L(y,\n",
        "p\n",
        "^\n",
        "‚Äã\n",
        " )=‚àí‚àëy\n",
        "i\n",
        "‚Äã\n",
        " log\n",
        "p\n",
        "^\n",
        "‚Äã\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        "\n",
        "Used when predicting more than two classes.\n",
        "Encourages correct class probability estimates.\n",
        "2. How Loss Function Works in Gradient Boosting\n",
        "Gradient Boosting builds models sequentially to minimize the loss function.\n",
        "Step 1: Train an initial weak model.\n",
        "Step 2: Compute residuals (errors) based on the loss function.\n",
        "Step 3: Train a new model to predict these residuals.\n",
        "Step 4: Update the predictions using gradient descent.\n",
        "Step 5: Repeat the process for multiple iterations.\n",
        "\n",
        "3. Python Implementation Using Different Loss Functions\n",
        "Let‚Äôs train a Gradient Boosting model for both classification and regression using different loss functions.\n",
        "\n",
        "Gradient Boosting for Classification (Log Loss)"
      ],
      "metadata": {
        "id": "y1JHKJaaTIiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create synthetic classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting with Log Loss\n",
        "gb_clf = GradientBoostingClassifier(loss='log_loss', n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Gradient Boosting (Log Loss) Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "F8FF1vIBTWk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting with Mean Squared Error (MSE) Loss\n",
        "gb_reg = GradientBoostingRegressor(loss='squared_error', n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Gradient Boosting (MSE Loss) Error:\", mse)\n"
      ],
      "metadata": {
        "id": "vi-ZBp0fTdmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "Ans.How XGBoost Improves Over Traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized version of Gradient Boosting that is faster, more efficient, and more accurate than traditional Gradient Boosting. It introduces several key improvements:\n",
        "\n",
        "1. Key Improvements of XGBoost Over Traditional Gradient Boosting\n",
        "(1) Regularization (L1 & L2) to Prevent Overfitting\n",
        " Gradient Boosting does not include built-in regularization.\n",
        " XGBoost adds L1 (Lasso) and L2 (Ridge) regularization terms in the objective function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "‚àë\n",
        "Residuals\n",
        "+\n",
        "ùúÜ\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "2\n",
        "+\n",
        "ùõº\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "1\n",
        "Loss=‚àëResiduals+Œª‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "2\n",
        "‚Äã\n",
        " +Œ±‚à£‚à£w‚à£‚à£\n",
        "1\n",
        "‚Äã\n",
        "\n",
        " Prevents overfitting by penalizing large weights.\n",
        " Helps in better generalization to new data.\n",
        "\n",
        "(2) Handling Missing Values Automatically\n",
        " Gradient Boosting does not handle missing values natively.\n",
        " XGBoost automatically learns the best direction to take when encountering missing data.\n",
        "\n",
        " No need to manually impute missing values!\n",
        "\n",
        "(3) Faster Training Using Parallelization\n",
        " Traditional Gradient Boosting builds trees sequentially.\n",
        " XGBoost uses multi-threading and parallelization to build trees efficiently.\n",
        "\n",
        " Much faster than traditional Gradient Boosting.\n",
        "\n",
        "(4) Weighted Quantile Sketch for Better Splitting\n",
        " Traditional Gradient Boosting selects splits based on greedy approaches.\n",
        " XGBoost uses a weighted quantile sketch algorithm to find better split points.\n",
        "\n",
        " Leads to better feature selection and more accurate splits.\n",
        "\n",
        "(5) Tree Pruning for Better Performance\n",
        " Gradient Boosting stops growing trees when they reach a max depth.\n",
        " XGBoost uses \"Max Depth + Pruning\" (depth-first approach).\n",
        "\n",
        " Prevents unnecessary splits and speeds up training.\n",
        " Reduces memory usage.\n",
        "\n",
        "(6) Shrinkage (Learning Rate) & Column Subsampling\n",
        " XGBoost applies shrinkage (learning rate) after each boosting step to reduce overfitting.\n",
        " Column subsampling (like Random Forest) is used to reduce correlation between trees.\n",
        "\n",
        " Better generalization and faster training.\n",
        "\n",
        "(7) GPU Acceleration for Large Datasets\n",
        " XGBoost can use GPU processing, which speeds up training on large datasets.\n",
        " Traditional Gradient Boosting is CPU-based and slower for big data.\n",
        "\n",
        " Faster model training for massive datasets.\n",
        "\n",
        "2. Python Implementation: XGBoost vs. Gradient Boosting\n",
        "Let's compare Gradient Boosting and XGBoost on the same dataset.\n",
        "\n",
        "Traditional Gradient Boosting (Sklearn)\n",
        "python\n",
        "\n",
        "\n",
        "How XGBoost Improves Over Traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized version of Gradient Boosting that is faster, more efficient, and more accurate than traditional Gradient Boosting. It introduces several key improvements:\n",
        "\n",
        "1. Key Improvements of XGBoost Over Traditional Gradient Boosting\n",
        "(1) Regularization (L1 & L2) to Prevent Overfitting\n",
        " Gradient Boosting does not include built-in regularization.\n",
        " XGBoost adds L1 (Lasso) and L2 (Ridge) regularization terms in the objective function:\n",
        "\n",
        "Loss\n",
        "=\n",
        "‚àë\n",
        "Residuals\n",
        "+\n",
        "ùúÜ\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "2\n",
        "+\n",
        "ùõº\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "1\n",
        "Loss=‚àëResiduals+Œª‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "2\n",
        "‚Äã\n",
        " +Œ±‚à£‚à£w‚à£‚à£\n",
        "1\n",
        "‚Äã\n",
        "\n",
        " Prevents overfitting by penalizing large weights.\n",
        " Helps in better generalization to new data.\n",
        "\n",
        "(2) Handling Missing Values Automatically\n",
        " Gradient Boosting does not handle missing values natively.\n",
        " XGBoost automatically learns the best direction to take when encountering missing data.\n",
        "\n",
        " No need to manually impute missing values!\n",
        "\n",
        "(3) Faster Training Using Parallelization\n",
        " Traditional Gradient Boosting builds trees sequentially.\n",
        " XGBoost uses multi-threading and parallelization to build trees efficiently.\n",
        "\n",
        " Much faster than traditional Gradient Boosting.\n",
        "\n",
        "(4) Weighted Quantile Sketch for Better Splitting\n",
        " Traditional Gradient Boosting selects splits based on greedy approaches.\n",
        " XGBoost uses a weighted quantile sketch algorithm to find better split points.\n",
        "\n",
        " Leads to better feature selection and more accurate splits.\n",
        "\n",
        "(5) Tree Pruning for Better Performance\n",
        " Gradient Boosting stops growing trees when they reach a max depth.\n",
        " XGBoost uses \"Max Depth + Pruning\" (depth-first approach).\n",
        "\n",
        " Prevents unnecessary splits and speeds up training.\n",
        " Reduces memory usage.\n",
        "\n",
        "(6) Shrinkage (Learning Rate) & Column Subsampling\n",
        " XGBoost applies shrinkage (learning rate) after each boosting step to reduce overfitting.\n",
        " Column subsampling (like Random Forest) is used to reduce correlation between trees.\n",
        "\n",
        " Better generalization and faster training.\n",
        "\n",
        "(7) GPU Acceleration for Large Datasets\n",
        " XGBoost can use GPU processing, which speeds up training on large datasets.\n",
        " Traditional Gradient Boosting is CPU-based and slower for big data.\n",
        "\n",
        "Faster model training for massive datasets.\n",
        "\n",
        "2. Python Implementation: XGBoost vs. Gradient Boosting\n",
        "Let's compare Gradient Boosting and XGBoost on the same dataset.\n",
        "\n",
        "Traditional Gradient Boosting (Sklearn)\n",
        "python\n",
        "Copy\n",
        "Edit\n"
      ],
      "metadata": {
        "id": "hRotTTIpTeW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is the difference between XGBoost and CatBoost?\n",
        "AnsXGBoost vs. CatBoost: Key Differences in Python\n",
        "Both XGBoost and CatBoost are powerful gradient boosting algorithms, but they differ in performance, speed, and handling of categorical data.\n",
        "\n",
        "1. Key Differences Between XGBoost and CatBoost\n",
        "Feature\tXGBoost üöÄ\tCatBoost üê±\n",
        "Best For\tStructured numerical data\tCategorical data-heavy datasets\n",
        "Speed\tFaster than traditional Gradient Boosting, supports GPU\tüöÄ Faster than XGBoost for categorical data\n",
        "Handling Categorical Features\tRequires label encoding or one-hot encoding\t‚úÖ Automatically encodes categorical features (best for categorical-heavy datasets)\n",
        "Missing Values\tHandles missing values automatically\tHandles missing values automatically\n",
        "Overfitting Handling\tUses L1/L2 regularization\tUses Ordered Boosting to prevent overfitting\n",
        "Memory Usage\tHigher due to one-hot encoding\t‚úÖ Lower (handles categorical features efficiently)\n",
        "Hyperparameter Tuning\tMore sensitive, needs careful tuning\t‚úÖ Fewer hyperparameters, easier tuning\n",
        "GPU Support\tSupports GPU for acceleration\tüöÄ Native GPU acceleration, often faster\n",
        "2. When to Use XGBoost vs. CatBoost?\n",
        "‚úÖ Use XGBoost when:\n",
        "\n",
        "Your dataset is mostly numerical.\n",
        "You need a highly optimized model with fine-tuned hyperparameters.\n",
        "You are working with structured data (like finance, medical, fraud detection).\n",
        "‚úÖ Use CatBoost when:\n",
        "\n",
        "Your dataset has many categorical features.\n",
        "You want a fast, out-of-the-box solution with minimal preprocessing.\n",
        "You work in e-commerce, NLP, social media, or recommendation systems.\n",
        "3. Python Implementation: XGBoost vs. CatBoost\n",
        "Let's compare XGBoost and CatBoost on the same dataset."
      ],
      "metadata": {
        "id": "FkLpnqb7UM4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "ajBZBB5QUb87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
        "\n",
        "# Convert dataset into pandas DataFrame (for categorical feature handling)\n",
        "X = pd.DataFrame(X)\n",
        "\n",
        "# Introduce some categorical columns for testing\n",
        "X[0] = np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=X.shape[0])  # Simulating categorical data\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical feature indices\n",
        "cat_features = [0]  # Column index of categorical feature\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(n_estimators=100, learning_rate=0.1, depth=6, random_state=42, cat_features=cat_features, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_cat = cat_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"CatBoost Accuracy:\", accuracy_score(y_test, y_pred_cat))\n"
      ],
      "metadata": {
        "id": "QEypZyd5UfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What are some real-world applications of Boosting techniques?\n",
        "Ans.Real-World Applications of Boosting Techniques (XGBoost, CatBoost, AdaBoost, Gradient Boosting) in Python üöÄ\n",
        "Boosting algorithms are widely used across industries due to their high accuracy, ability to handle missing data, and robustness against overfitting. Let‚Äôs explore some real-world applications:\n",
        "\n",
        "1. Fraud Detection (Banking & Finance) üí∞\n",
        "Boosting algorithms like XGBoost and CatBoost are used by banks and financial institutions to detect fraudulent transactions. These models analyze transaction patterns and flag anomalies.\n",
        "\n",
        "Python Example: Fraud Detection with XGBoost"
      ],
      "metadata": {
        "id": "g4O0gs2eUji6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic fraud detection dataset\n",
        "X, y = make_classification(n_samples=10000, n_features=20, weights=[0.99, 0.01], random_state=42)  # Imbalanced dataset\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42, eval_metric=\"logloss\")\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "print(\"Fraud Detection Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "LjMiyzLCUo8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Customer Churn Prediction (Telecom, Banking, SaaS) üìû\n",
        "Businesses use Gradient Boosting and XGBoost to predict which customers are likely to leave (churn). This helps in proactive retention strategies.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Detects hidden patterns in customer behavior.\n",
        "Helps reduce customer churn by providing early warnings.\n",
        "3. Disease Prediction & Medical Diagnosis (Healthcare) üè•\n",
        "Boosting models help in cancer detection, heart disease prediction, and medical image analysis.\n",
        "\n",
        "CatBoost is effective because medical datasets often contain many categorical variables.\n",
        "XGBoost is widely used in predicting diabetes, heart disease, and COVID-19 severity.\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Handles missing values (common in medical datasets).\n",
        "Provides high accuracy for disease classification.\n",
        "4. Stock Market Prediction & Algorithmic Trading üìà\n",
        "Hedge funds and financial analysts use XGBoost and LightGBM for predicting stock prices, volatility, and trading signals.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Can capture non-linear relationships in financial data.\n",
        "Helps in high-frequency trading strategies.\n",
        "5. Credit Scoring & Loan Default Prediction (Banking) üí≥\n",
        "Banks use XGBoost and CatBoost to assess a borrower's risk by analyzing credit history, transaction behavior, and demographics.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Handles large datasets efficiently.\n",
        "Improves risk assessment accuracy.\n",
        "6. Spam Detection & Email Filtering (Cybersecurity) ‚úâÔ∏è\n",
        "Spam filters in Gmail, Yahoo, and Outlook use AdaBoost and Gradient Boosting to classify emails as spam or not spam.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Learns from misclassified spam emails.\n",
        "Improves detection accuracy over time.\n",
        "7. Image Recognition & Object Detection (Computer Vision) üì∑\n",
        "Boosting techniques are used in facial recognition, OCR (Optical Character Recognition), and self-driving cars.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Works well for high-dimensional image data.\n",
        "Used in traffic sign recognition and medical imaging (MRI scans, X-rays).\n",
        "8. Recommender Systems (Netflix, Amazon, YouTube) üì∫\n",
        "Companies like Netflix, Amazon, and YouTube use Gradient Boosting and CatBoost to recommend content based on user behavior.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Can model complex user preferences effectively.\n",
        "Improves engagement rates by offering personalized recommendations.\n",
        "9. Predictive Maintenance (Manufacturing & IoT) üè≠\n",
        "Factories use boosting models to predict machine failures before they happen.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Analyzes sensor data in real-time.\n",
        "Reduces operational downtime and maintenance costs.\n",
        "10. Natural Language Processing (NLP) & Sentiment Analysis üìù\n",
        "Boosting techniques help in text classification, sentiment analysis, and chatbots.\n",
        "\n",
        "‚úÖ Why Boosting?\n",
        "\n",
        "Handles text data efficiently.\n",
        "Improves chatbot accuracy (customer service applications)."
      ],
      "metadata": {
        "id": "NTY7J-DGUvHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.How does regularization help in XGBoost?\n",
        "Ans.How Regularization Helps in XGBoost? üöÄ\n",
        "Regularization in XGBoost helps prevent overfitting, improve generalization, and enhance model stability. XGBoost uses both L1 (Lasso) and L2 (Ridge) regularization, making it more robust than traditional Gradient Boosting.\n",
        "\n",
        "1. Regularization Terms in XGBoost\n",
        "XGBoost's objective function includes regularization terms to penalize complex models:\n",
        "\n",
        "ùêø\n",
        "=\n",
        "‚àë\n",
        "Loss\n",
        "+\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùë§\n",
        "ùëó\n",
        "2\n",
        "+\n",
        "ùõº\n",
        "‚àë\n",
        "‚à£\n",
        "ùë§\n",
        "ùëó\n",
        "‚à£\n",
        "L=‚àëLoss+Œª‚àëw\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        " +Œ±‚àë‚à£w\n",
        "j\n",
        "‚Äã\n",
        " ‚à£\n",
        "Where:\n",
        "\n",
        "‚àë\n",
        "Loss\n",
        "‚àëLoss = The primary loss function (e.g., Log Loss for classification).\n",
        "ùúÜ\n",
        "‚àë\n",
        "ùë§\n",
        "ùëó\n",
        "2\n",
        "Œª‚àëw\n",
        "j\n",
        "2\n",
        "‚Äã\n",
        "  (L2 Regularization) = Prevents large weight values (Ridge Regression).\n",
        "ùõº\n",
        "‚àë\n",
        "‚à£\n",
        "ùë§\n",
        "ùëó\n",
        "‚à£\n",
        "Œ±‚àë‚à£w\n",
        "j\n",
        "‚Äã\n",
        " ‚à£ (L1 Regularization) = Shrinks some feature weights to zero (Lasso Regression).\n",
        "‚úÖ Controls model complexity and prevents overfitting!\n",
        "\n",
        "2. Types of Regularization in XGBoost\n",
        "Regularization Type\tXGBoost Parameter\tEffect\n",
        "L1 Regularization (Lasso)\treg_alpha\tShrinks less important feature weights to zero (feature selection).\n",
        "L2 Regularization (Ridge)\treg_lambda\tReduces extreme weight values, making the model more stable.\n",
        "Tree-Specific Regularization\tgamma\tPrunes unnecessary splits to avoid overfitting.\n",
        "3. How Regularization Prevents Overfitting?\n",
        "Without regularization, XGBoost may create deep trees that memorize the training data, leading to poor generalization. Regularization:\n",
        "‚úÖ Reduces model complexity (simpler models perform better on new data).\n",
        "‚úÖ Prevents large swings in predictions (avoids over-reliance on certain features).\n",
        "‚úÖ Encourages sparsity in weights (L1 helps in feature selection).\n",
        "\n",
        "4. Python Example: XGBoost with Regularization\n",
        "Let‚Äôs train an XGBoost model with L1 and L2 regularization."
      ],
      "metadata": {
        "id": "BhGY777fUv_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a classification dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost with Regularization\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4,\n",
        "                            reg_alpha=0.1,  # L1 Regularization\n",
        "                            reg_lambda=0.5,  # L2 Regularization\n",
        "                            gamma=0.2,  # Tree pruning\n",
        "                            random_state=42, eval_metric=\"logloss\")\n",
        "\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = xgb_clf.predict(X_test)\n",
        "print(\"XGBoost Accuracy with Regularization:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ReFGgclcU7V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.What are some hyperparameters to tune in Gradient Boosting models?\n",
        "Ans.Hyperparameter Tuning in Gradient Boosting Models (XGBoost, LightGBM, CatBoost) üöÄ\n",
        "Tuning hyperparameters in Gradient Boosting models is crucial for achieving the best performance and avoiding overfitting. Below are the key hyperparameters to tune for XGBoost, LightGBM, and CatBoost along with their effects and recommended values.\n",
        "\n",
        "1. Key Hyperparameters for Gradient Boosting Models\n",
        "Hyperparameter\tDescription\tEffect\tTypical Range\n",
        "n_estimators\tNumber of boosting trees\tMore trees = better learning, but may overfit\t50 - 500\n",
        "learning_rate\tShrinks each tree‚Äôs contribution\tLower = better generalization but slower training\t0.01 - 0.3\n",
        "max_depth\tMaximum depth of each tree\tControls model complexity\t3 - 10\n",
        "min_child_weight\tMinimum sum of weights for child nodes\tPrevents small, noisy splits\t1 - 10\n",
        "gamma (XGBoost) / min_gain_to_split (LightGBM)\tMinimum loss reduction for a split\tHigher = less overfitting\t0 - 5\n",
        "subsample\tFraction of data used per boosting round\tHelps prevent overfitting\t0.5 - 1.0\n",
        "colsample_bytree\tFraction of features used per tree\tPrevents overfitting\t0.5 - 1.0\n",
        "reg_alpha (L1 Regularization)\tShrinks feature weights to zero\tHelps feature selection\t0 - 10\n",
        "reg_lambda (L2 Regularization)\tReduces large weights\tPrevents overfitting\t0 - 10\n",
        "2. Hyperparameter Tuning in Python\n",
        "Let‚Äôs tune an XGBoost model using GridSearchCV and RandomizedSearchCV.\n",
        "\n",
        "üîπ Method 1: Grid Search (Exhaustive but Slow)"
      ],
      "metadata": {
        "id": "7CDHyCVeVA9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define model\n",
        "xgb_clf = xgb.XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(xgb_clf, param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "-iErFiNtVGxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "    'reg_lambda': [0.1, 0.5, 1, 5]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(xgb_clf, param_dist, n_iter=20, scoring='accuracy', cv=3, n_jobs=-1, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "oNgqnDgiVNNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What is the concept of Feature Importance in Boosting?\n",
        "Ans.Feature Importance in Boosting (XGBoost, LightGBM, CatBoost) üöÄ\n",
        "Feature Importance helps us understand which features contribute the most to the model's predictions. It is crucial for:\n",
        "‚úÖ Feature selection (removing unimportant features)\n",
        "‚úÖ Interpretability (explaining model decisions)\n",
        "‚úÖ Performance optimization (reducing computation time)\n",
        "\n",
        "1. Types of Feature Importance in Boosting Models\n",
        "üîπ Gain-Based Importance (Default in XGBoost & LightGBM)\n",
        "Measures the average information gain from each feature when making splits.\n",
        "Higher gain = More contribution to reducing error.\n",
        "üîπ Split Count (Frequency-Based Importance)\n",
        "Measures how many times a feature is used for splitting.\n",
        "More splits = More importance.\n",
        "üîπ SHAP Values (Shapley Additive Explanations)\n",
        "Advanced method that measures individual feature contributions per prediction.\n",
        "Used for model interpretability in CatBoost & XGBoost.\n",
        "2. Visualizing Feature Importance in XGBoost\n",
        "We can use XGBoost‚Äôs plot_importance() to see which features matter most.\n",
        "\n",
        "Python Example: Feature Importance in XGBoost"
      ],
      "metadata": {
        "id": "s2L3mbXDVQt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "xgb.plot_importance(xgb_clf, importance_type=\"gain\")  # Use \"weight\" for split count\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vxK3pIk2VXNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train LightGBM model\n",
        "lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
        "lgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "lgb.plot_importance(lgb_clf, importance_type=\"gain\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GxfSpGP_VZO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(xgb_clf)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "id": "ZJlpB1NlVa4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. Why is CatBoost efficient for categorical data?\n",
        "Ans.Why is CatBoost Efficient for Categorical Data? üöÄ\n",
        "CatBoost (Categorical Boosting) is designed specifically to handle categorical features efficiently without requiring manual preprocessing like one-hot encoding or label encoding. Here‚Äôs why it's superior for categorical data:\n",
        "\n",
        "1. Native Categorical Feature Handling (No One-Hot Encoding!)\n",
        "Unlike XGBoost and LightGBM, CatBoost does not require one-hot encoding or label encoding for categorical variables. Instead, it uses a technique called Order-Based Target Encoding, which:\n",
        "‚úÖ Prevents data leakage (ensures encoding is based on past data only).\n",
        "‚úÖ Reduces memory usage (avoids high-dimensional sparse matrices).\n",
        "‚úÖ Improves accuracy by capturing meaningful category relationships.\n",
        "\n",
        "Example: How CatBoost Handles Categorical Data"
      ],
      "metadata": {
        "id": "prIN050kVe4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample data (with categorical features)\n",
        "import pandas as pd\n",
        "data = pd.DataFrame({\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
        "    'City': ['NY', 'LA', 'SF', 'LA', 'NY'],\n",
        "    'Income': [50000, 60000, 70000, 80000, 75000],\n",
        "    'Purchased': [0, 1, 1, 0, 1]\n",
        "})\n",
        "\n",
        "# Define categorical feature indices\n",
        "cat_features = ['Gender', 'City']\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=['Purchased']), data['Purchased'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to CatBoost Pool\n",
        "train_pool = Pool(X_train, label=y_train, cat_features=cat_features)\n",
        "test_pool = Pool(X_test, label=y_test, cat_features=cat_features)\n",
        "\n",
        "# Train CatBoost Model\n",
        "model = CatBoostClassifier(iterations=100, depth=4, learning_rate=0.1, verbose=0)\n",
        "model.fit(train_pool)\n",
        "\n",
        "# Predict\n",
        "preds = model.predict(test_pool)\n",
        "print(\"Predictions:\", preds)\n"
      ],
      "metadata": {
        "id": "ElBqMWxoVlWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "0imMygCMWSuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. Train an AdaBoost Classifier on a sample dataset and print model accuracy?\n",
        "Ans.Train an AdaBoost Classifier and Print Accuracy in Python\n",
        "We will use AdaBoostClassifier from sklearn.ensemble and train it on a sample dataset (make_classification). Then, we will evaluate its accuracy on a test set."
      ],
      "metadata": {
        "id": "HHLAulBhWWYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "fgjCnlHmWgLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a synthetic dataset (binary classification)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "zSGzBspJWl0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize AdaBoost with 50 weak learners (default is DecisionTree with depth=1)\n",
        "adaboost_clf = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_clf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "K_I4rb3TWnVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data\n",
        "y_pred = adaboost_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "f2KsOtC4Wo_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Plot\n",
        "plt.bar(range(X.shape[1]), adaboost_clf.feature_importances_)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Feature Importance in AdaBoost\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nNGSfD_RWq7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)\n",
        "Ans.Train an AdaBoost Regressor and Evaluate Performance using MAE in Python\n",
        "We will train an AdaBoost Regressor on a synthetic dataset and evaluate its performance using Mean Absolute Error (MAE)."
      ],
      "metadata": {
        "id": "Zu-GTnZcYmMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# Create synthetic regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=5, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize AdaBoost Regressor with 50 weak learners\n",
        "adaboost_reg = AdaBoostRegressor(n_estimators=50, learning_rate=0.8, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_reg.fit(X_train, y_train)\n",
        "# Predict on test data\n",
        "y_pred = adaboost_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print MAE\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "# Plot true vs predicted values\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"AdaBoost Regression: True vs Predicted\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zrCS0q0wYzOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance?\n",
        "Ans.We will use the Breast Cancer dataset from sklearn.datasets, train a GradientBoostingClassifier, and visualize feature importance."
      ],
      "metadata": {
        "id": "2IbhtKX0ZcUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Store feature names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_clf.fit(X_train, y_train)\n",
        "# Predict on test data\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "# Get feature importance scores\n",
        "feature_importance = gb_clf.feature_importances_\n",
        "\n",
        "# Sort features by importance\n",
        "sorted_indices = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Important Features:\")\n",
        "for i in range(5):\n",
        "    print(f\"{feature_names[sorted_indices[i]]}: {feature_importance[sorted_indices[i]]:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(feature_importance)), feature_importance[sorted_indices], align=\"center\")\n",
        "plt.yticks(range(len(feature_importance)), np.array(feature_names)[sorted_indices])\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Feature Importance in Gradient Boosting\")\n",
        "plt.gca().invert_yaxis()  # Highest importance at top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "StXJaXjtZiFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Train a Gradient Boosting Regressor and evaluate using R-Squared Score?\n",
        "Ans.Here‚Äôs how you can train a Gradient Boosting Regressor and evaluate it using the R-squared score in Python:\n",
        "\n",
        "Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.make_regression for synthetic data).\n",
        "Split it into training and testing sets.\n",
        "Train a GradientBoostingRegressor from sklearn.ensemble.\n",
        "Predict on the test set.\n",
        "Compute the R-squared score."
      ],
      "metadata": {
        "id": "RinhCt6ubhhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'R-squared Score: {r2:.4f}')\n"
      ],
      "metadata": {
        "id": "4RzhI9XtbsVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train an XGBoost Classifier.\n",
        "Train a Gradient Boosting Classifier.\n",
        "Predict on the test set.\n",
        "Compare accuracy scores."
      ],
      "metadata": {
        "id": "VKf-gFRAbxN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "y_pred_gbc = gbc.predict(X_test)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_gbc = accuracy_score(y_test, y_pred_gbc)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f'Gradient Boosting Classifier Accuracy: {accuracy_gbc:.4f}')\n",
        "print(f'XGBoost Classifier Accuracy: {accuracy_xgb:.4f}')\n"
      ],
      "metadata": {
        "id": "1ynQY8JBZj-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Train a CatBoost Classifier and evaluate using F1-Score.\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train a CatBoost Classifier.\n",
        "Predict on the test set.\n",
        "Compute the F1-score.\n"
      ],
      "metadata": {
        "id": "IOcqglq0cEHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f'CatBoost Classifier F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "id": "9XOA_kjPcRFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)\n",
        "AnsSteps:\n",
        "Load a dataset (e.g., sklearn.datasets.make_regression).\n",
        "Split it into training and testing sets.\n",
        "Train an XGBoost Regressor.\n",
        "Predict on the test set.\n",
        "Compute the Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "rhd46I0ncaFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "xgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f'XGBoost Regressor Mean Squared Error (MSE): {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "0KoCFWyocm-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Train an AdaBoost Classifier and visualize feature importance?\n",
        "Ans"
      ],
      "metadata": {
        "id": "LZyoyyYSc1d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier with Decision Tree as base estimator\n",
        "adaboost_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                                    n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'AdaBoost Classifier Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = adaboost_model.feature_importances_\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_names, feature_importance, color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"AdaBoost Feature Importance\")\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zcKWDmcLc6s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Train a Gradient Boosting Regressor and plot learning curves?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.make_regression).\n",
        "Split it into training and testing sets.\n",
        "Train a Gradient Boosting Regressor while tracking performance.\n",
        "Plot the learning curve using training and validation loss."
      ],
      "metadata": {
        "id": "Z8AylSBEc7hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor and track training progress\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Compute learning curves (MSE for each iteration)\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for y_train_pred, y_test_pred in zip(gb_regressor.staged_predict(X_train), gb_regressor.staged_predict(X_test)):\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "    test_errors.append(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "# Plot Learning Curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_errors) + 1), train_errors, label=\"Training MSE\", marker='o', color=\"blue\")\n",
        "plt.plot(range(1, len(test_errors) + 1), test_errors, label=\"Validation MSE\", marker='s', color=\"red\")\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Gradient Boosting Learning Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gft_oZerdGwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Train an XGBoost Classifier and visualize feature importance?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train an XGBoost Classifier.\n",
        "Predict on the test set and evaluate accuracy.\n",
        "Extract feature importance from the model.\n",
        "Visualize feature importance using a bar chart."
      ],
      "metadata": {
        "id": "O1CvYGEWdIj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'XGBoost Classifier Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(xgb_model, importance_type='weight', xlabel=\"Feature Importance\", grid=False)\n",
        "plt.title(\"XGBoost Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A3OT1TJwdSe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Train a CatBoost Classifier and plot the confusion matrix?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train a CatBoost Classifier.\n",
        "Predict on the test set.\n",
        "Compute and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "DcODzlTndUPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'CatBoost Classifier Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FaajH-E9ddS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Train an AdaBoost Classifier with different numbers of estimators and compare accuracy?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train multiple AdaBoost Classifiers with different estimators.\n",
        "Predict on the test set and compute accuracy.\n",
        "Plot accuracy vs. number of estimators."
      ],
      "metadata": {
        "id": "2Ek0gmfjdfKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Range of estimators to test\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "# Train AdaBoost Classifier with different numbers of estimators\n",
        "for n_estimators in n_estimators_list:\n",
        "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                               n_estimators=n_estimators, learning_rate=1.0, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f'Number of Estimators: {n_estimators}, Accuracy: {acc:.4f}')\n",
        "\n",
        "# Plot accuracy vs. number of estimators\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')\n",
        "plt.xlabel(\"Number of Estimators\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"AdaBoost Classifier - Accuracy vs. Number of Estimators\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2SjqWj_hdqLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26.Train a Gradient Boosting Classifier and visualize the ROC curve?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train a Gradient Boosting Classifier.\n",
        "Predict probabilities on the test set.\n",
        "Compute and visualize the ROC curve.\n",
        "\n"
      ],
      "metadata": {
        "id": "fw5o5MGMdrsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = gb_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC score\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', lw=2)  # Random guess line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Gradient Boosting Classifier - ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FohWV0dKdv8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27.Train an XGBoost Regressor and tune the learning rate using GridSearchCV?\n",
        "Ans.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.make_regression).\n",
        "Split it into training and testing sets.\n",
        "Define an XGBoost Regressor model.\n",
        "Use GridSearchCV to find the best learning_rate.\n",
        "Train the model with the best hyperparameter.\n",
        "Predict on the test set and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "Hy2_aBmxeIuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression dataset\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define XGBoost Regressor\n",
        "xgb_regressor = xgb.XGBRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for learning rate\n",
        "param_grid = {'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(xgb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best learning rate\n",
        "best_learning_rate = grid_search.best_params_['learning_rate']\n",
        "print(f'Best Learning Rate: {best_learning_rate}')\n",
        "\n",
        "# Train XGBoost Regressor with the best learning rate\n",
        "best_xgb_regressor = xgb.XGBRegressor(n_estimators=100, learning_rate=best_learning_rate, max_depth=3, random_state=42)\n",
        "best_xgb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_xgb_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'XGBoost Regressor MSE: {mse:.4f}')\n"
      ],
      "metadata": {
        "id": "T7nfq0ISeUzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting\n",
        "Ans.Steps:\n",
        "Load an imbalanced dataset (e.g., sklearn.datasets.make_classification).\n",
        "Split it into training and testing sets.\n",
        "Train a CatBoost Classifier without class weighting.\n",
        "Train another CatBoost Classifier with class weighting.\n",
        "Compare performance using F1-score & confusion matrix."
      ],
      "metadata": {
        "id": "Hp7SS9u1eWzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train CatBoost Classifier without class weighting\n",
        "model_no_weights = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, verbose=0, random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "# Train CatBoost Classifier with class weighting\n",
        "class_weights = {0: 1, 1: 9}  # Inverse of class distribution\n",
        "model_with_weights = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, class_weights=class_weights, verbose=0, random_state=42)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "# Evaluate F1-score\n",
        "f1_no_weights = f1_score(y_test, y_pred_no_weights)\n",
        "f1_with_weights = f1_score(y_test, y_pred_with_weights)\n",
        "\n",
        "print(f'F1-Score (Without Class Weighting): {f1_no_weights:.4f}')\n",
        "print(f'F1-Score (With Class Weighting): {f1_with_weights:.4f}')\n",
        "\n",
        "# Plot confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Without class weighting\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_no_weights), annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix (No Class Weighting)\")\n",
        "axes[0].set_xlabel(\"Predicted Label\")\n",
        "axes[0].set_ylabel(\"True Label\")\n",
        "\n",
        "# With class weighting\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_with_weights), annot=True, fmt=\"d\", cmap=\"Oranges\", ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix (With Class Weighting)\")\n",
        "axes[1].set_xlabel(\"Predicted Label\")\n",
        "axes[1].set_ylabel(\"True Label\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c1nUVbZhegvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.Train an AdaBoost Classifier and analyze the effect of different learning rates?\n",
        "ANs.Steps:\n",
        "Load a dataset (e.g., sklearn.datasets.load_breast_cancer).\n",
        "Split it into training and testing sets.\n",
        "Train AdaBoost Classifiers with different learning_rate values.\n",
        "Evaluate performance using accuracy.\n",
        "Plot learning rate vs. accuracy.\n"
      ],
      "metadata": {
        "id": "d49lzlpKemgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define range of learning rates to test\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
        "accuracies = []\n",
        "\n",
        "# Train AdaBoost Classifier with different learning rates\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "                               n_estimators=100, learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f'Learning Rate: {lr}, Accuracy: {acc:.4f}')\n",
        "\n",
        "# Plot learning rate vs. accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(learning_rates, accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')\n",
        "plt.xlabel(\"Learning Rate\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.title(\"Effect of Learning Rate on AdaBoost Classifier Performance\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QkoWFeFYet82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Train an XGBoost Classifier for multi-class classification and evaluate using log-loss?\n",
        "AnsSteps:\n",
        "Load a multi-class dataset (e.g., sklearn.datasets.load_digits).\n",
        "Split it into training and testing sets.\n",
        "Train an XGBoost Classifier.\n",
        "Predict probabilities for the test set.\n",
        "Evaluate the model using log-loss."
      ],
      "metadata": {
        "id": "Jlib7IHme0dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load dataset (Digits dataset with 10 classes)\n",
        "data = load_digits()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define XGBoost Classifier\n",
        "xgb_classifier = xgb.XGBClassifier(objective='multi:softprob', num_class=10, n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_probs = xgb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Compute Log-Loss\n",
        "logloss_score = log_loss(y_test, y_probs)\n",
        "print(f'XGBoost Classifier Log-Loss: {logloss_score:.4f}')\n"
      ],
      "metadata": {
        "id": "Rf9wwyK0e4kf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}