{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qa9WelKHD20"
      },
      "outputs": [],
      "source": [
        "SVM & Naive bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is a Support Vector Machine (SVM)\n",
        "Ans.A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is particularly powerful for binary classification problems.\n",
        "\n",
        "How SVM Works\n",
        "Hyperplane: SVM finds the optimal hyperplane that best separates the data into different classes.\n",
        "Support Vectors: These are the data points closest to the hyperplane, which influence its position and orientation.\n",
        "Margin Maximization: SVM tries to maximize the margin (distance) between the hyperplane and the nearest data points from each class.\n",
        "Kernel Trick: If the data is not linearly separable, SVM can transform it into a higher-dimensional space using kernel functions (e.g., linear, polynomial, radial basis function (RBF)).\n",
        "SVM in Python (Using Scikit-Learn)\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "model = SVC(kernel='linear')  # You can try 'rbf', 'poly', 'sigmoid' as well\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "When to Use SVM\n",
        "When you have a small to medium-sized dataset.\n",
        "When the data is not too noisy.\n",
        "When you need a strong classifier for complex, high-dimensional data.\n",
        "SVM is widely used in text classification, image recognition, and bioinformatics. Let me know if you want more details!"
      ],
      "metadata": {
        "id": "T98N4xNWHKoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "model = SVC(kernel='linear')  # You can try 'rbf', 'poly', 'sigmoid' as well\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "n8H8RF7DHiND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What is the difference between Hard Margin and Soft Margin SVM4\n",
        "Ans.The key difference between Hard Margin SVM and Soft Margin SVM is how they handle misclassified points in the dataset.\n",
        "\n",
        "1. Hard Margin SVM\n",
        "Used when the data is perfectly linearly separable (i.e., there exists a hyperplane that separates the classes without any errors).\n",
        "It strictly enforces that no training data points can be misclassified.\n",
        "Maximizes the margin while ensuring all points are correctly classified.\n",
        "Problem: It is very sensitive to outliers and may not work well for real-world noisy data.\n",
        "Mathematical Formulation\n",
        "The optimization problem for hard margin SVM:\n",
        "\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "Subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector,\n",
        "𝑏\n",
        "b is the bias,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the class label (\n",
        "+\n",
        "1\n",
        "+1 or\n",
        "−\n",
        "1\n",
        "−1),\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is the feature vector.\n",
        "2. Soft Margin SVM\n",
        "Used when the data is not perfectly separable.\n",
        "Introduces a slack variable\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  to allow some misclassifications.\n",
        "Tries to balance maximizing the margin while minimizing classification errors.\n",
        "Less sensitive to outliers compared to Hard Margin SVM.\n",
        "Controlled by the regularization parameter C:\n",
        "High C → Less tolerance for misclassification (tries to fit the data tightly).\n",
        "Low C → More tolerance for misclassification (allows some margin violations).\n",
        "Mathematical Formulation\n",
        "The optimization problem for soft margin SVM:\n",
        "\n",
        "min\n",
        "⁡\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜉\n",
        "𝑖\n",
        "min\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "Subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,∀i\n",
        "where:\n",
        "\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  represents the slack variables allowing misclassification.\n",
        "𝐶\n",
        "C is the regularization parameter controlling the trade-off.\n"
      ],
      "metadata": {
        "id": "2yYbONgEHkxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a simple dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "y = np.where(y == 0, -1, 1)  # Convert to {-1, 1}\n",
        "\n",
        "# Hard Margin SVM (C is very high)\n",
        "hard_margin_svm = SVC(kernel='linear', C=1e5)\n",
        "hard_margin_svm.fit(X, y)\n",
        "\n",
        "# Soft Margin SVM (C is smaller)\n",
        "soft_margin_svm = SVC(kernel='linear', C=1.0)\n",
        "soft_margin_svm.fit(X, y)\n",
        "\n",
        "# Plotting decision boundaries\n",
        "def plot_svm(model, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "    # Create grid\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision bo\n"
      ],
      "metadata": {
        "id": "mrAPBGsZHuo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.What is the mathematical intuition behind SVM4\n",
        "Ans.The mathematical intuition behind Support Vector Machines (SVMs) is rooted in maximizing the margin between different classes while minimizing classification errors. Below is a step-by-step breakdown of its core concepts.\n",
        "\n",
        "1. Finding the Optimal Hyperplane\n",
        "Given a labeled dataset\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "(x\n",
        "i\n",
        "​\n",
        " ,y\n",
        "i\n",
        "​\n",
        " ), where:\n",
        "\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is the feature vector,\n",
        "𝑦\n",
        "𝑖\n",
        "∈\n",
        "{\n",
        "−\n",
        "1\n",
        ",\n",
        "+\n",
        "1\n",
        "}\n",
        "y\n",
        "i\n",
        "​\n",
        " ∈{−1,+1} is the class label.\n",
        "The goal of SVM is to find a hyperplane that best separates the classes:\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "0\n",
        "w⋅x+b=0\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector (normal to the hyperplane),\n",
        "𝑏\n",
        "b is the bias term.\n",
        "For binary classification, we define two decision boundaries:\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "+\n",
        "1\n",
        "(\n",
        "for positive class\n",
        ")\n",
        "w⋅x+b=+1(for positive class)\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "−\n",
        "1\n",
        "(\n",
        "for negative class\n",
        ")\n",
        "w⋅x+b=−1(for negative class)\n",
        "Margin Definition\n",
        "The margin is the distance between these two boundaries:\n",
        "\n",
        "Margin\n",
        "=\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "Margin=\n",
        "∣∣w∣∣\n",
        "2\n",
        "​\n",
        "\n",
        "Our objective is to maximize the margin, ensuring better generalization.\n",
        "\n",
        "2. Optimization Problem (Hard Margin SVM)\n",
        "To maximize the margin while ensuring correct classification, we solve:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "This ensures that all data points lie outside or on their respective margins.\n",
        "\n",
        "Lagrange Multipliers & Dual Form\n",
        "Using Lagrange multipliers\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        " , we transform this into a dual problem:\n",
        "\n",
        "max\n",
        "⁡\n",
        "𝛼\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "−\n",
        "1\n",
        "2\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝛼\n",
        "𝑗\n",
        "𝑦\n",
        "𝑖\n",
        "𝑦\n",
        "𝑗\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "α\n",
        "max\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " −\n",
        "2\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " α\n",
        "j\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " y\n",
        "j\n",
        "​\n",
        " K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )\n",
        "subject to:\n",
        "\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "0\n",
        "≤\n",
        "𝛼\n",
        "𝑖\n",
        "≤\n",
        "𝐶\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " =0,0≤α\n",
        "i\n",
        "​\n",
        " ≤C\n",
        "where\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "  (for linear SVM) or a kernel function for non-linear SVM.\n",
        "\n",
        "The solution gives support vectors, which are the only points with\n",
        "𝛼\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "α\n",
        "i\n",
        "​\n",
        " >0, and they define the decision boundary.\n",
        "\n",
        "3. Soft Margin SVM (Handling Misclassification)\n",
        "For non-linearly separable data, we introduce slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  to allow some violations:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "The new objective function balances margin maximization with misclassification:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝜉\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜉\n",
        "𝑖\n",
        "w,b,ξ\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "where\n",
        "𝐶\n",
        "C controls the trade-off between maximizing the margin and allowing misclassifications.\n",
        "\n",
        "4. Kernel Trick (Handling Non-Linearity)\n",
        "For complex datasets where a linear separator doesn’t work, we use the kernel trick to map data to a higher-dimensional space. The kernel function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ) replaces the dot product:\n",
        "\n",
        "Linear Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "\n",
        "Polynomial Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=(x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " +c)\n",
        "d\n",
        "\n",
        "Radial Basis Function (RBF) Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∣∣x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∣∣\n",
        "2\n",
        " )\n",
        "This allows SVM to learn non-linear decision boundaries efficiently.\n",
        "\n",
        "5. Final Decision Function\n",
        "Once we solve for\n",
        "𝑤\n",
        "w and\n",
        "𝑏\n",
        "b, predictions are made using:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "sign\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "f(x)=sign(w⋅x+b)\n",
        "For the dual form with kernels:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "sign\n",
        "(\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "f(x)=sign(\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " K(x\n",
        "i\n",
        "​\n",
        " ,x)+b)\n"
      ],
      "metadata": {
        "id": "a-SuewAcHwj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What is the role of Lagrange Multipliers in SVM4\n",
        "Ans.Role of Lagrange Multipliers in SVM\n",
        "Lagrange multipliers play a crucial role in Support Vector Machines (SVMs) by transforming the constrained optimization problem into a solvable dual problem. This allows SVM to efficiently find the optimal hyperplane and support vectors.\n",
        "\n",
        "1. The Primal Optimization Problem (Hard Margin SVM)\n",
        "The objective of SVM is to find a hyperplane that maximizes the margin while ensuring all data points are correctly classified. The problem is formulated as:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to the constraints:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector (normal to the hyperplane),\n",
        "𝑏\n",
        "b is the bias term,\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is the feature vector,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the class label (\n",
        "±\n",
        "1\n",
        "±1).\n",
        "2. Introducing Lagrange Multipliers\n",
        "To solve this constrained optimization problem, we introduce Lagrange multipliers\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        "  for each constraint. The Lagrangian function is defined as:\n",
        "\n",
        "𝐿\n",
        "(\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝛼\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "−\n",
        "1\n",
        "]\n",
        "L(w,b,α)=\n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " −\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)−1]\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "α\n",
        "i\n",
        "​\n",
        " ≥0 are the Lagrange multipliers,\n",
        "The term\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "−\n",
        "1\n",
        "]\n",
        "[y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)−1] ensures the constraints are satisfied.\n",
        "Karush-Kuhn-Tucker (KKT) Conditions\n",
        "To find the optimal solution, we differentiate\n",
        "𝐿\n",
        "L with respect to\n",
        "𝑤\n",
        "w and\n",
        "𝑏\n",
        "b, and set the derivatives to zero:\n",
        "\n",
        "Gradient w.r.t\n",
        "𝑤\n",
        "w:\n",
        "\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑤\n",
        "=\n",
        "𝑤\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "∂w\n",
        "∂L\n",
        "​\n",
        " =w−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        " =0\n",
        "𝑤\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "w=\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        "\n",
        "Gradient w.r.t\n",
        "𝑏\n",
        "b:\n",
        "\n",
        "∂\n",
        "𝐿\n",
        "∂\n",
        "𝑏\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "∂b\n",
        "∂L\n",
        "​\n",
        " =\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " =0\n",
        "Complementary Slackness:\n",
        "\n",
        "𝛼\n",
        "𝑖\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "−\n",
        "1\n",
        "]\n",
        "=\n",
        "0\n",
        "α\n",
        "i\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)−1]=0\n",
        "This means\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        "  is nonzero only for support vectors (points that lie on the margin boundary).\n",
        "\n",
        "3. Converting to the Dual Problem\n",
        "By substituting\n",
        "𝑤\n",
        "w into the Lagrangian, we eliminate\n",
        "𝑤\n",
        "w and obtain the dual form:\n",
        "\n",
        "max\n",
        "⁡\n",
        "𝛼\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "−\n",
        "1\n",
        "2\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝛼\n",
        "𝑗\n",
        "𝑦\n",
        "𝑖\n",
        "𝑦\n",
        "𝑗\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "α\n",
        "max\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " −\n",
        "2\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " α\n",
        "j\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " y\n",
        "j\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " )\n",
        "subject to:\n",
        "\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "=\n",
        "0\n",
        ",\n",
        "𝛼\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " =0,α\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "This quadratic optimization problem is easier to solve than the original primal problem.\n",
        "\n",
        "4. Support Vectors & Decision Function\n",
        "Only points with\n",
        "𝛼\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "α\n",
        "i\n",
        "​\n",
        " >0 are support vectors, meaning they define the decision boundary.\n",
        "\n",
        "The final decision function is:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "sign\n",
        "(\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛼\n",
        "𝑖\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        ")\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "f(x)=sign(\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " α\n",
        "i\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        " (x\n",
        "i\n",
        "​\n",
        " ⋅x)+b)\n",
        "For non-linearly separable data, we replace\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "  with a kernel function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " ), allowing us to compute decision boundaries in higher-dimensional spaces.\n",
        "\n",
        "5. Soft Margin SVM (Handling Misclassification)\n",
        "For noisy data, we introduce slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        "  and modify the optimization:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝜉\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜉\n",
        "𝑖\n",
        "w,b,ξ\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "The dual problem now includes an upper bound on\n",
        "𝛼\n",
        "𝑖\n",
        "α\n",
        "i\n",
        "​\n",
        " :\n",
        "\n",
        "0\n",
        "≤\n",
        "𝛼\n",
        "𝑖\n",
        "≤\n",
        "𝐶\n",
        "0≤α\n",
        "i\n",
        "​\n",
        " ≤C\n",
        "where C controls the trade-off between margin maximization and misclassification.\n",
        "\n"
      ],
      "metadata": {
        "id": "m3-o_eZcIER4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What are Support Vectors in SVM4\n",
        "Ans.Support Vectors in SVM\n",
        "Support Vectors are the most important data points in Support Vector Machines (SVMs) because they define the optimal hyperplane that separates different classes.\n",
        "\n",
        "1. Definition of Support Vectors\n",
        "In an SVM model, the goal is to find a decision boundary (hyperplane) that maximizes the margin between two classes. The support vectors are the data points that lie closest to this hyperplane. These points determine:\n",
        "\n",
        "The position of the hyperplane.\n",
        "The width of the margin.\n",
        "The robustness of the model.\n",
        "Mathematically, for a given hyperplane:\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "0\n",
        "w⋅x+b=0\n",
        "The support vectors are the points that satisfy the condition:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "=\n",
        "1\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)=1\n",
        "where:\n",
        "\n",
        "𝑤\n",
        "w is the weight vector,\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is a support vector,\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the class label (\n",
        "+\n",
        "1\n",
        "+1 or\n",
        "−\n",
        "1\n",
        "−1),\n",
        "𝑏\n",
        "b is the bias term.\n",
        "2. Importance of Support Vectors\n",
        "They define the margin of the classifier.\n",
        "Removing any non-support vector has no effect on the decision boundary.\n",
        "Removing a support vector can change the decision boundary significantly.\n",
        "3. Finding Support Vectors in Python\n",
        "Using Scikit-Learn, you can find the support vectors after training an SVM model:\n",
        "\n",
        "python\n"
      ],
      "metadata": {
        "id": "w6333D1WIWWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=50, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "y = np.where(y == 0, -1, 1)  # Convert labels to {-1, 1}\n",
        "\n",
        "# Train SVM model\n",
        "svm = SVC(kernel='linear', C=1.0)\n",
        "svm.fit(X, y)\n",
        "\n",
        "# Get support vectors\n",
        "support_vectors = svm.support_vectors_\n",
        "\n",
        "# Plot decision boundary and support vectors\n",
        "def plot_svm(model, X, y, support_vectors):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], color='gold', s=200, edgecolors='black', label=\"Support Vectors\")\n",
        "\n",
        "    # Create grid\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.legend()\n",
        "    plt.title(\"SVM with Support Vectors\")\n",
        "    plt.show()\n",
        "\n",
        "plot_svm(svm, X, y, support_vectors)\n"
      ],
      "metadata": {
        "id": "mSM_tCV5In_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Support Vectors in Soft Margin SVM\n",
        "When data is not perfectly separable, SVM allows some misclassification using slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        " :\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "Support vectors can either be on the margin or inside the margin.\n",
        "Some support vectors may be misclassified, especially when\n",
        "𝐶\n",
        "C (regularization parameter) is low.\n",
        "You can identify support vectors in a soft-margin SVM using:"
      ],
      "metadata": {
        "id": "1gnHodVwIp57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Support Vector Indices:\", svm.support_)\n",
        "print(\"Number of Support Vectors per Class:\", svm.n_support_)\n"
      ],
      "metadata": {
        "id": "qUDPfhwvIuKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.What is a Support Vector Classifier (SVC)4\n",
        "Ans.Support Vector Classifier (SVC) in SVM\n",
        "A Support Vector Classifier (SVC) is a type of Support Vector Machine (SVM) used for classification tasks. It finds the best decision boundary (hyperplane) that maximizes the margin between different classes.\n",
        "\n",
        "1. What is SVC?\n",
        "SVC is the classification version of SVM.\n",
        "It aims to separate data points belonging to different classes using a hyperplane.\n",
        "If data is not linearly separable, SVC uses a soft margin and kernel trick to handle non-linearity.\n",
        "2. Mathematical Formulation of SVC\n",
        "For a binary classification problem with dataset\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "(x\n",
        "i\n",
        "​\n",
        " ,y\n",
        "i\n",
        "​\n",
        " ), where:\n",
        "\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is the feature vector.\n",
        "𝑦\n",
        "𝑖\n",
        "∈\n",
        "{\n",
        "−\n",
        "1\n",
        ",\n",
        "+\n",
        "1\n",
        "}\n",
        "y\n",
        "i\n",
        "​\n",
        " ∈{−1,+1} is the class label.\n",
        "The decision boundary is given by:\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "0\n",
        "w⋅x+b=0\n",
        "Hard Margin SVC (for linearly separable data)\n",
        "Objective function:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        ",\n",
        "∀\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1,∀i\n",
        "Soft Margin SVC (for non-linearly separable data)\n",
        "We introduce slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "ξ\n",
        "i\n",
        "​\n",
        " :\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "−\n",
        "𝜉\n",
        "𝑖\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "≥\n",
        "0\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1−ξ\n",
        "i\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "​\n",
        " ≥0\n",
        "The objective function balances margin maximization and misclassification:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝜉\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜉\n",
        "𝑖\n",
        "w,b,ξ\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝐶\n",
        "C controls the trade-off between margin width and classification accuracy.\n",
        "3. Kernel Trick in SVC\n",
        "If data is not linearly separable, SVC uses the kernel trick to project data into a higher-dimensional space where it is separable.\n",
        "\n",
        "Common Kernels:\n",
        "\n",
        "Linear Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "\n",
        "Polynomial Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=(x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " +c)\n",
        "d\n",
        "\n",
        "Radial Basis Function (RBF) Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∣∣x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∣∣\n",
        "2\n",
        " )\n"
      ],
      "metadata": {
        "id": "MK76pUf6IwQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
        "y = np.where(y == 0, -1, 1)  # Convert labels to {-1, 1}\n",
        "\n",
        "# Train SVC model with a linear kernel\n",
        "svc = SVC(kernel='linear', C=1.0)\n",
        "svc.fit(X, y)\n",
        "\n",
        "# Get support vectors\n",
        "support_vectors = svc.support_vectors_\n",
        "\n",
        "# Plot decision boundary and support vectors\n",
        "def plot_svc(model, X, y, support_vectors):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], color='gold', s=200, edgecolors='black', label=\"Support Vectors\")\n",
        "\n",
        "    # Create grid\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.legend()\n",
        "    plt.title(\"SVC with Support Vectors\")\n",
        "    plt.show()\n",
        "\n",
        "plot_svc(svc, X, y, support_vectors)\n"
      ],
      "metadata": {
        "id": "KG67SpM1I7MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.What is a Support Vector Regressor (SVR)\n",
        "Ans.Support Vector Regressor (SVR) in SVM\n",
        "A Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. Unlike Support Vector Classifier (SVC), which finds a decision boundary to separate classes, SVR finds a function that best fits the data while maintaining a margin of tolerance.\n",
        "\n",
        "1. What is SVR?\n",
        "SVR is the regression version of SVM.\n",
        "It finds a function\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "f(x) that predicts\n",
        "𝑦\n",
        "y with minimal error.\n",
        "Instead of maximizing the margin like SVC, SVR fits a function within an\n",
        "𝜖\n",
        "ϵ-tube, allowing some flexibility.\n",
        "Mathematical Formulation of SVR\n",
        "Given a dataset\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "(x\n",
        "i\n",
        "​\n",
        " ,y\n",
        "i\n",
        "​\n",
        " ), SVR aims to find a function:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "f(x)=w⋅x+b\n",
        "such that most predictions fall within a margin\n",
        "𝜖\n",
        "ϵ:\n",
        "\n",
        "∣\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "∣\n",
        "≤\n",
        "𝜖\n",
        "∣y\n",
        "i\n",
        "​\n",
        " −(w⋅x\n",
        "i\n",
        "​\n",
        " +b)∣≤ϵ\n",
        "2. Hard vs Soft Margin SVR\n",
        "Hard Margin SVR (Exact Fit)\n",
        "For perfectly predictable data, SVR minimizes:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "w,b\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        "\n",
        "subject to:\n",
        "\n",
        "∣\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "∣\n",
        "≤\n",
        "𝜖\n",
        "∣y\n",
        "i\n",
        "​\n",
        " −(w⋅x\n",
        "i\n",
        "​\n",
        " +b)∣≤ϵ\n",
        "where\n",
        "𝜖\n",
        "ϵ is a tolerance threshold.\n",
        "\n",
        "Soft Margin SVR (Handling Noise)\n",
        "For real-world data with noise, we introduce slack variables\n",
        "𝜉\n",
        "𝑖\n",
        "+\n",
        "ξ\n",
        "i\n",
        "+\n",
        "​\n",
        "  and\n",
        "𝜉\n",
        "𝑖\n",
        "−\n",
        "ξ\n",
        "i\n",
        "−\n",
        "​\n",
        "  to allow some points to be outside the\n",
        "𝜖\n",
        "ϵ-tube:\n",
        "\n",
        "∣\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "∣\n",
        "≤\n",
        "𝜖\n",
        "+\n",
        "𝜉\n",
        "𝑖\n",
        "+\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "+\n",
        ",\n",
        "𝜉\n",
        "𝑖\n",
        "−\n",
        "≥\n",
        "0\n",
        "∣y\n",
        "i\n",
        "​\n",
        " −(w⋅x\n",
        "i\n",
        "​\n",
        " +b)∣≤ϵ+ξ\n",
        "i\n",
        "+\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "+\n",
        "​\n",
        " ,ξ\n",
        "i\n",
        "−\n",
        "​\n",
        " ≥0\n",
        "Objective function:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝜉\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝜉\n",
        "𝑖\n",
        "+\n",
        "+\n",
        "𝜉\n",
        "𝑖\n",
        "−\n",
        ")\n",
        "w,b,ξ\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (ξ\n",
        "i\n",
        "+\n",
        "​\n",
        " +ξ\n",
        "i\n",
        "−\n",
        "​\n",
        " )\n",
        "where:\n",
        "\n",
        "𝐶\n",
        "C controls the trade-off between flatness of the function and margin violations.\n",
        "Higher\n",
        "𝐶\n",
        "C → Lower tolerance for outliers.\n",
        "3. Kernel Trick in SVR\n",
        "Like SVC, SVR can model non-linear relationships using kernels:\n",
        "\n",
        "Linear Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        "\n",
        "Polynomial Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "⋅\n",
        "𝑥\n",
        "𝑗\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=(x\n",
        "i\n",
        "​\n",
        " ⋅x\n",
        "j\n",
        "​\n",
        " +c)\n",
        "d\n",
        "\n",
        "Radial Basis Function (RBF) Kernel:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝑥\n",
        "𝑗\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x\n",
        "i\n",
        "​\n",
        " ,x\n",
        "j\n",
        "​\n",
        " )=exp(−γ∣∣x\n",
        "i\n",
        "​\n",
        " −x\n",
        "j\n",
        "​\n",
        " ∣∣\n",
        "2\n",
        " )"
      ],
      "metadata": {
        "id": "InI0IIouI88J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Generate synthetic regression data\n",
        "np.random.seed(42)\n",
        "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
        "y = np.sin(X).ravel() + 0.1 * np.random.randn(40)  # Add some noise\n",
        "\n",
        "# Train SVR models with different kernels\n",
        "svr_linear = SVR(kernel='linear', C=100, epsilon=0.1)\n",
        "svr_rbf = SVR(kernel='rbf', C=100, epsilon=0.1, gamma=0.5)\n",
        "\n",
        "# Fit models\n",
        "svr_linear.fit(X, y)\n",
        "svr_rbf.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "X_test = np.linspace(0, 5, 100).reshape(-1, 1)\n",
        "y_pred_linear = svr_linear.predict(X_test)\n",
        "y_pred_rbf = svr_rbf.predict(X_test)\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y, color='black', label='Data')\n",
        "plt.plot(X_test, y_pred_linear, color='blue', label='SVR (Linear Kernel)')\n",
        "plt.plot(X_test, y_pred_rbf, color='red', label='SVR (RBF Kernel)')\n",
        "plt.legend()\n",
        "plt.title(\"Support Vector Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q5pocVE9JJ-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is the Kernel Trick in SVM\n",
        "Ans.Kernel Trick in SVM\n",
        "The Kernel Trick is a technique in Support Vector Machines (SVMs) that allows them to handle non-linearly separable data by transforming the input space into a higher-dimensional space where a linear separation is possible.\n",
        "\n",
        "1. Why is the Kernel Trick Needed?\n",
        "In many real-world scenarios, data is not linearly separable in its original feature space. Consider a dataset like this:\n",
        "\n",
        " Class A: (inside a circle)\n",
        " Class B: (outside the circle)\n",
        "\n",
        "A linear SVM cannot separate these classes with a straight line. The Kernel Trick helps by mapping the data to a higher-dimensional space where a hyperplane can be used for separation.\n",
        "\n",
        "2. How Does the Kernel Trick Work?\n",
        "Instead of explicitly computing the transformation into a higher-dimensional space (which may be computationally expensive), we use a Kernel Function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " ) that computes the dot product directly in the higher-dimensional space:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "⋅\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " )=ϕ(x)⋅ϕ(x\n",
        "′\n",
        " )\n",
        "where:\n",
        "\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        "x,x\n",
        "′\n",
        "  are two input feature vectors.\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x) is the mapping function to a higher-dimensional space.\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " ) computes the dot product in that high-dimensional space without explicitly transforming\n",
        "𝑥\n",
        "x.\n",
        "3. Common Kernel Functions\n",
        "Different kernel functions are used in SVMs, depending on the data:\n",
        "\n",
        "(a) Linear Kernel (For Linearly Separable Data)\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "K(x,x\n",
        "′\n",
        " )=x⋅x\n",
        "′\n",
        "\n",
        "Equivalent to a standard SVM without kernel trick.\n",
        "Used when data is already linearly separable."
      ],
      "metadata": {
        "id": "YH2siTJHJMdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_linear = SVC(kernel='linear')\n"
      ],
      "metadata": {
        "id": "JBP8X7wwJX--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Polynomial Kernel (For Polynomial Relationships)\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x,x\n",
        "′\n",
        " )=(x⋅x\n",
        "′\n",
        " +c)\n",
        "d\n",
        "\n",
        "Projects data into a higher-degree polynomial space.\n",
        "Hyperparameters:\n",
        "𝑐\n",
        "c (coefficient) controls bias.\n",
        "𝑑\n",
        "d (degree) controls complexity."
      ],
      "metadata": {
        "id": "X12qIxT9Jatq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(b) Polynomial Kernel (For Polynomial Relationships)\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x,x\n",
        "′\n",
        " )=(x⋅x\n",
        "′\n",
        " +c)\n",
        "d\n",
        "\n",
        "Projects data into a higher-degree polynomial space.\n",
        "Hyperparameters:\n",
        "𝑐\n",
        "c (coefficient) controls bias.\n",
        "𝑑\n",
        "d (degree) controls complexity."
      ],
      "metadata": {
        "id": "7eTt5p55JdmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Radial Basis Function (RBF) Kernel (For Complex Non-Linear Data)\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "−\n",
        "𝑥\n",
        "′\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " )=exp(−γ∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        " )\n",
        "Maps data into an infinite-dimensional space.\n",
        "Hyperparameter:\n",
        "𝛾\n",
        "γ controls how much influence each data point has.\n",
        "🔹 Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', gamma=0.5)\n",
        "(d) Sigmoid Kernel (Similar to Neural Networks)\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "tanh\n",
        "⁡\n",
        "(\n",
        "𝛼\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " )=tanh(αx⋅x\n",
        "′\n",
        " +c)\n",
        "Mimics the behavior of a neural network activation function.\n",
        "🔹 Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "svm_sigmoid = SVC(kernel='sigmoid')\n",
        "4. Visualizing the Effect of Kernels\n",
        "Here’s a Python example demonstrating different kernels:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate non-linearly separable data\n",
        "X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "# Train SVMs with different kernels\n",
        "svm_linear = SVC(kernel='linear').fit(X, y)\n",
        "svm_rbf = SVC(kernel='rbf', gamma=1).fit(X, y)\n",
        "svm_poly = SVC(kernel='poly', degree=3).fit(X, y)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_svm(model, X, y, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    \n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot results\n",
        "plot_svm(svm_linear, X, y, \"SVM with Linear Kernel\")\n",
        "plot_svm(svm_rbf, X, y, \"SVM with RBF Kernel\")\n",
        "plot_svm(svm_poly, X, y, \"SVM with Polynomial Kernel\")\n",
        "Output Explanation\n",
        "Linear Kernel: Tries to draw a straight line (fails for complex shapes).\n",
        "Polynomial Kernel: Fits curves (good for polynomial relationships).\n",
        "RBF Kernel: Best fit for highly non-linear data.\n"
      ],
      "metadata": {
        "id": "yQh4SPupJi-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:\n",
        "Ans.Comparison of Linear Kernel, Polynomial Kernel, and RBF Kernel in SVM\n",
        "Feature\tLinear Kernel\tPolynomial Kernel\tRBF (Radial Basis Function) Kernel\n",
        "Formula\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "K(x,x\n",
        "′\n",
        " )=x⋅x\n",
        "′\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x,x\n",
        "′\n",
        " )=(x⋅x\n",
        "′\n",
        " +c)\n",
        "d\n",
        " \t( K(x, x') = \\exp(-\\gamma\n",
        "Type of Mapping\tNo mapping (remains in original space)\tMaps to a higher-degree polynomial space\tMaps to an infinite-dimensional space\n",
        "When to Use?\tData is linearly separable\tData has polynomial relationships\tData is highly non-linear\n",
        "Hyperparameters\tNone\t-\n",
        "𝑐\n",
        "c (coefficient)\n",
        "-\n",
        "𝑑\n",
        "d (degree)\t-\n",
        "𝛾\n",
        "γ (controls variance)\n",
        "Computational Cost\tLow (fastest)\tMedium (depends on degree\n",
        "𝑑\n",
        "d)\tHigh (most computationally expensive)\n",
        "Flexibility\tRigid (only linear separation)\tMore flexible (can fit polynomial patterns)\tMost flexible (captures complex patterns)\n",
        "Overfitting Risk\tLow\tModerate (depends on degree\n",
        "𝑑\n",
        "d)\tHigh (for large\n",
        "𝛾\n",
        "γ)\n",
        "Best For\tLinearly separable data (e.g., text classification)\tModerately complex data (e.g., images, gene data)\tHighly complex data (e.g., image recognition, speech classification)\n",
        "1. Linear Kernel: Best for Linearly Separable Data\n",
        "Formula:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "K(x,x\n",
        "′\n",
        " )=x⋅x\n",
        "′\n",
        "\n",
        "Pros:\n",
        " Simple and fast.\n",
        " Works well for high-dimensional sparse data (e.g., text classification).\n",
        "Cons:\n",
        " Cannot handle non-linear patterns.\n",
        "Example Use Case: Spam detection, document classification.\n",
        "Python Example:"
      ],
      "metadata": {
        "id": "sTwQc8NyJtqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svm_linear = SVC(kernel='linear')\n"
      ],
      "metadata": {
        "id": "aHYS1ddJJibJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Polynomial Kernel: Best for Polynomial Relationships\n",
        "Formula:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "⋅\n",
        "𝑥\n",
        "′\n",
        "+\n",
        "𝑐\n",
        ")\n",
        "𝑑\n",
        "K(x,x\n",
        "′\n",
        " )=(x⋅x\n",
        "′\n",
        " +c)\n",
        "d\n",
        "\n",
        "Pros:\n",
        " Captures polynomial relationships.\n",
        " More flexible than a linear kernel.\n",
        "Cons:\n",
        " Can be computationally expensive for high-degree polynomials.\n",
        "Risk of overfitting if\n",
        "𝑑\n",
        "d is too high.\n",
        "Example Use Case: Image recognition with moderate complexity.\n",
        " Python Example:"
      ],
      "metadata": {
        "id": "WkS0r0JAKEUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_poly = SVC(kernel='poly', degree=3, coef0=1)\n"
      ],
      "metadata": {
        "id": "hVj6iMrWKJjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. RBF Kernel: Best for Highly Non-Linear Data\n",
        "Formula:\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "−\n",
        "𝑥\n",
        "′\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " )=exp(−γ∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        " )\n",
        "Pros:\n",
        " Extremely powerful for complex patterns.\n",
        "Handles data that cannot be separated linearly or polynomially.\n",
        "Cons:\n",
        " Computationally expensive.\n",
        " Can overfit if\n",
        "𝛾\n",
        "γ is too high.\n",
        "Example Use Case: Image classification, speech recognition.\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf', gamma=0.5)\n",
        "4. Visualization of Different Kernels\n",
        "Let's compare the decision boundaries of Linear, Polynomial, and RBF kernels:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate synthetic non-linearly separable data\n",
        "X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "# Train SVM models with different kernels\n",
        "svm_linear = SVC(kernel='linear').fit(X, y)\n",
        "svm_poly = SVC(kernel='poly', degree=3).fit(X, y)\n",
        "svm_rbf = SVC(kernel='rbf', gamma=1).fit(X, y)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_svm(model, X, y, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot results\n",
        "plot_svm(svm_linear, X, y, \"SVM with Linear Kernel\")\n",
        "plot_svm(svm_poly, X, y, \"SVM with Polynomial Kernel\")\n",
        "plot_svm(svm_rbf, X, y, \"SVM with RBF Kernel\")"
      ],
      "metadata": {
        "id": "rO19o1hNKOQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.What is the effect of the C parameter in SVM\n",
        "Ans.Effect of the C Parameter in SVM\n",
        "The C parameter in Support Vector Machines (SVMs) controls the trade-off between achieving a perfect classification and maximizing the margin. It acts as a regularization parameter that determines how much misclassification is tolerated.\n",
        "\n",
        "1. What is C in SVM?\n",
        "C is a hyperparameter that influences the decision boundary.\n",
        "A higher C means fewer misclassifications but a smaller margin.\n",
        "A lower C allows a larger margin but permits some misclassification.\n",
        "Mathematically, the objective function of SVM includes C as:\n",
        "\n",
        "min\n",
        "⁡\n",
        "𝑤\n",
        ",\n",
        "𝑏\n",
        ",\n",
        "𝜉\n",
        "1\n",
        "2\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "+\n",
        "𝐶\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜉\n",
        "𝑖\n",
        "w,b,ξ\n",
        "min\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∣∣w∣∣\n",
        "2\n",
        " +C\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ξ\n",
        "i\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "∣\n",
        "∣\n",
        "𝑤\n",
        "∣\n",
        "∣\n",
        "2\n",
        "∣∣w∣∣\n",
        "2\n",
        "  ensures a maximum margin.\n",
        "∑\n",
        "𝜉\n",
        "𝑖\n",
        "∑ξ\n",
        "i\n",
        "​\n",
        "  represents misclassified points (soft margin violations).\n",
        "𝐶\n",
        "C balances margin size vs. misclassification.\n",
        "2. Effect of High vs. Low C\n",
        " High C (Strong Regularization, Less Margin)\n",
        "SVM tries to classify all points correctly.\n",
        "Small margin and complex decision boundary.\n",
        "Risk of overfitting (good on training data, bad on new data).\n",
        " Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svm_high_c = SVC(kernel='linear', C=1000)  # High C\n",
        " Visualization:\n",
        "\n",
        "Decision boundary is tight around data points.\n",
        "Little tolerance for misclassified points.\n",
        "May fail on new (test) data.\n",
        " Low C (Weak Regularization, More Margin)\n",
        "Larger margin, but some misclassifications are allowed.\n",
        "SVM prioritizes a simpler model (generalization over accuracy).\n",
        "Risk of underfitting (not capturing patterns well).\n",
        " Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "svm_low_c = SVC(kernel='linear', C=0.01)  # Low C\n",
        " Visualization:\n",
        "\n",
        "Decision boundary is smooth and wide.\n",
        "More misclassification is accepted.\n",
        "Better generalization to new data.\n",
        "3. Visualization of C's Effect\n",
        "Here’s a code snippet to visualize the effect of different C values on an SVM classifier:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic classification data\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
        "\n",
        "# Train SVMs with different C values\n",
        "svm_low_c = SVC(kernel='linear', C=0.1).fit(X, y)\n",
        "svm_high_c = SVC(kernel='linear', C=1000).fit(X, y)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_svm(model, X, y, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot results\n",
        "plot_svm(svm_low_c, X, y, \"SVM with Low C (Large Margin, More Errors)\")\n",
        "plot_svm(svm_high_c, X, y, \"SVM with High C (Small Margin, Fewer Errors)\")\n"
      ],
      "metadata": {
        "id": "q_ScQQYuKVbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.What is the role of the Gamma parameter in RBF Kernel SVM\n",
        "Ans.Role of the Gamma Parameter in RBF Kernel SVM\n",
        "The Gamma (𝛾) parameter in Radial Basis Function (RBF) Kernel SVM controls the influence of individual training points on the decision boundary. It determines how far the impact of a single training example reaches.\n",
        "\n",
        "1. What is Gamma (𝛾) in SVM?\n",
        "In the RBF kernel function:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝛾\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "−\n",
        "𝑥\n",
        "′\n",
        "∣\n",
        "∣\n",
        "2\n",
        ")\n",
        "K(x,x\n",
        "′\n",
        " )=exp(−γ∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        " )\n",
        "where:\n",
        "\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        "′\n",
        "x,x\n",
        "′\n",
        "  are feature vectors.\n",
        "∣\n",
        "∣\n",
        "𝑥\n",
        "−\n",
        "𝑥\n",
        "′\n",
        "∣\n",
        "∣\n",
        "2\n",
        "∣∣x−x\n",
        "′\n",
        " ∣∣\n",
        "2\n",
        "  is the squared Euclidean distance.\n",
        "𝛾\n",
        "γ determines how much influence a single training point has.\n",
        "Effect of Gamma (𝛾)\n",
        "High\n",
        "𝛾\n",
        "γ → Each point has a short-range influence (decision boundary becomes very tight around points).\n",
        "Low\n",
        "𝛾\n",
        "γ → Each point has a long-range influence (decision boundary is smoother).\n",
        "2. Effect of High vs. Low Gamma\n",
        " High Gamma (𝛾 → Large)\n",
        "Each training point has a very local influence.\n",
        "The decision boundary is highly flexible, capturing tiny variations.\n",
        "Leads to overfitting (good on training data but bad on test data).\n",
        " Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svm_high_gamma = SVC(kernel='rbf', gamma=10)  # High Gamma\n",
        " Visualization:\n",
        "\n",
        "The decision boundary wraps tightly around individual data points.\n",
        "Captures too much noise, leading to poor generalization.\n",
        " Low Gamma (𝛾 → Small)\n",
        "Each training point has a wide influence.\n",
        "The decision boundary is smooth and less complex.\n",
        "Leads to underfitting (does not capture complex patterns).\n",
        " Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "svm_low_gamma = SVC(kernel='rbf', gamma=0.01)  # Low Gamma\n",
        " Visualization:\n",
        "\n",
        "The decision boundary is broad and smooth.\n",
        "May miss important patterns in data.\n",
        "3. Visualizing the Effect of Gamma\n",
        "The following Python script demonstrates how different\n",
        "𝛾\n",
        "γ values impact decision boundaries:\n",
        "\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate synthetic non-linearly separable data\n",
        "X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "# Train SVMs with different gamma values\n",
        "svm_low_gamma = SVC(kernel='rbf', gamma=0.1).fit(X, y)\n",
        "svm_high_gamma = SVC(kernel='rbf', gamma=10).fit(X, y)\n",
        "\n",
        "# Function to plot decision boundaries\n",
        "def plot_svm(model, X, y, title):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100), np.linspace(ylim[0], ylim[1], 100))\n",
        "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    ax.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot results\n",
        "plot_svm(svm_low_gamma, X, y, \"SVM with Low Gamma (Smooth Decision Boundary)\")\n",
        "plot_svm(svm_high_gamma, X, y, \"SVM with High Gamma (Tight Decision Boundary)\")\n",
        "4. Summary of Gamma’s Effect\n",
        "Gamma Value\tEffect on Decision Boundary\tRisk\n",
        "Low\n",
        "𝛾\n",
        "γ (e.g., 0.01)\tSmooth and generalized\tUnderfitting\n",
        "High\n",
        "𝛾\n",
        "γ (e.g., 10)\tComplex, wraps tightly around data\tOverfitting\n",
        "5. Choosing the Right Gamma\n",
        " Low Gamma → If you want a smooth, general boundary (good for noisy data).\n",
        " High Gamma → If you want a tight fit around training data (good for clean data).\n",
        "\n",
        "Grid Search for Optimal Gamma\n",
        "To automatically find the best Gamma, use GridSearchCV:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'gamma': [0.01, 0.1, 1, 10, 100]}\n",
        "svm = SVC(kernel='rbf')\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"Best Gamma:\", grid_search.best_params_['gamma'])"
      ],
      "metadata": {
        "id": "2hnAEKxqKyE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What is the Naïve Bayes classifier, and why is it called \"Naïve\"\n",
        "Ans.Naïve Bayes Classifier: Explanation & Why It's \"Naïve\"\n",
        "1. What is the Naïve Bayes Classifier?\n",
        "Naïve Bayes is a probabilistic machine learning algorithm based on Bayes' Theorem. It is mainly used for classification tasks such as spam filtering, sentiment analysis, and document classification.\n",
        "\n",
        "Bayes' Theorem:\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X)=\n",
        "P(X)\n",
        "P(X∣Y)P(Y)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(Y∣X) → Posterior Probability (probability of class Y given features X).\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(X∣Y) → Likelihood (probability of features X given class Y).\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "P(Y) → Prior Probability (probability of class Y occurring).\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X) → Evidence (overall probability of features X).\n",
        "2. Why is it Called \"Naïve\"?\n",
        "The \"Naïve\" part comes from the assumption that all features (X) are independent of each other given the class\n",
        "𝑌\n",
        "Y.\n",
        "\n",
        " Example:\n",
        "In a spam classifier, words like \"free\", \"win\", and \"money\" might appear in spam emails. Naïve Bayes assumes:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "\"free\"\n",
        ",\n",
        "\"win\"\n",
        ",\n",
        "\"money\"\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "\"free\"\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "\"win\"\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "\"money\"\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "P(\"free\",\"win\",\"money\"∣Spam)=P(\"free\"∣Spam)×P(\"win\"∣Spam)×P(\"money\"∣Spam)\n",
        " Real-world fact: Words in a sentence are not truly independent, but Naïve Bayes assumes they are to simplify computation.\n",
        "\n",
        "3. Types of Naïve Bayes Classifiers\n",
        " Gaussian Naïve Bayes (GNB) → Assumes features are normally distributed (good for continuous data).\n",
        " Multinomial Naïve Bayes (MNB) → Used for text classification (counts word frequencies).\n",
        " Bernoulli Naïve Bayes (BNB) → Works with binary features (e.g., word presence in spam detection).\n",
        "\n",
        "4. Python Example: Naïve Bayes for Spam Classification\n",
        "python\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Example data: spam vs. non-spam messages\n",
        "messages = [\"Win a free lottery now\", \"Call me later\", \"Congratulations, you won!\", \"Let's meet for lunch\"]\n",
        "labels = [1, 0, 1, 0]  # 1 = Spam, 0 = Not Spam\n",
        "\n",
        "# Convert text to numerical feature vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(messages)\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X, labels)\n",
        "\n",
        "# Predict a new message\n",
        "new_message = [\"Free money, claim now\"]\n",
        "X_new = vectorizer.transform(new_message)\n",
        "prediction = nb.predict(X_new)\n",
        "\n",
        "print(\"Prediction:\", \"Spam\" if prediction[0] == 1 else \"Not Spam\")\n",
        "Output Example: \"Spam\"\n",
        "\n",
        "5. Advantages & Disadvantages of Naïve Bayes\n",
        "Feature\tPros \tCons\n",
        "Speed\tVery fast to train and predict\tAssumes feature independence (not always true)\n",
        "Works Well on Small Data\tPerforms well with limited training data\tStruggles with correlated features\n",
        "Handles High-Dimensional Data\tUsed in text classification (spam filtering, sentiment analysis)\tRequires proper feature engineering\n",
        "Performs Well on Probabilistic Data\tGreat for medical diagnosis, spam filtering, NLP\tCan overestimate probabilities when assumptions don’t hold\n"
      ],
      "metadata": {
        "id": "k_o3ORpXLXwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What is Bayes’ Theorem\n",
        "AnsBayes’ Theorem: Explanation & Formula\n",
        "Bayes’ Theorem is a fundamental rule in probability that describes how to update our beliefs based on new evidence. It is widely used in machine learning, statistics, and decision-making.\n",
        "\n",
        "1. Bayes’ Theorem Formula\n",
        "The theorem states:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        "∣\n",
        "𝐵\n",
        ")\n",
        "P(A∣B) → Posterior Probability (probability of event A happening given B has occurred).\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        "∣\n",
        "𝐴\n",
        ")\n",
        "P(B∣A) → Likelihood (probability of event B occurring given A is true).\n",
        "𝑃\n",
        "(\n",
        "𝐴\n",
        ")\n",
        "P(A) → Prior Probability (initial belief about event A before evidence).\n",
        "𝑃\n",
        "(\n",
        "𝐵\n",
        ")\n",
        "P(B) → Evidence (total probability of event B across all possible scenarios).\n",
        "2. Intuition Behind Bayes’ Theorem\n",
        " It helps update our beliefs based on new information.\n",
        " Used in classification problems (e.g., Spam Detection, Medical Diagnosis).\n",
        "\n",
        "3. Example: Medical Diagnosis (Cancer Test)\n",
        "Suppose:\n",
        "\n",
        "1% of people have cancer →\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "=\n",
        "0.01\n",
        "P(Cancer)=0.01.\n",
        "If a person has cancer, the test is 90% accurate →\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        "∣\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "=\n",
        "0.9\n",
        "P(Positive∣Cancer)=0.9.\n",
        "If a person doesn’t have cancer, the test falsely gives 5% false positives →\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        "∣\n",
        "𝑁\n",
        "𝑜\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "=\n",
        "0.05\n",
        "P(Positive∣NoCancer)=0.05.\n",
        "What is the probability that a person actually has cancer if they test positive?\n",
        "Using Bayes’ Theorem:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        "∣\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        "∣\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        ")\n",
        "P(Cancer∣Positive)=\n",
        "P(Positive)\n",
        "P(Positive∣Cancer)⋅P(Cancer)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        "∣\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "+\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        "∣\n",
        "𝑁\n",
        "𝑜\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝑁\n",
        "𝑜\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        ")\n",
        "P(Positive)=P(Positive∣Cancer)⋅P(Cancer)+P(Positive∣NoCancer)⋅P(NoCancer)\n",
        "Substituting values:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        ")\n",
        "=\n",
        "(\n",
        "0.9\n",
        "×\n",
        "0.01\n",
        ")\n",
        "+\n",
        "(\n",
        "0.05\n",
        "×\n",
        "0.99\n",
        ")\n",
        "=\n",
        "0.0594\n",
        "P(Positive)=(0.9×0.01)+(0.05×0.99)=0.0594\n",
        "Now:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑟\n",
        "∣\n",
        "𝑃\n",
        "𝑜\n",
        "𝑠\n",
        "𝑖\n",
        "𝑡\n",
        "𝑖\n",
        "𝑣\n",
        "𝑒\n",
        ")\n",
        "=\n",
        "0.9\n",
        "×\n",
        "0.01\n",
        "0.0594\n",
        "=\n",
        "0.009\n",
        "0.0594\n",
        "≈\n",
        "0.1515\n",
        "P(Cancer∣Positive)=\n",
        "0.0594\n",
        "0.9×0.01\n",
        "​\n",
        " =\n",
        "0.0594\n",
        "0.009\n",
        "​\n",
        " ≈0.1515\n",
        "So, even if the test is positive, the probability of actually having cancer is only ~15.15%!\n",
        "\n",
        "4. Python Implementation of Bayes' Theorem\n",
        "python\n",
        "\n",
        "def bayes_theorem(prior_A, likelihood_B_given_A, prior_not_A, likelihood_B_given_not_A):\n",
        "    P_B = (likelihood_B_given_A * prior_A) + (likelihood_B_given_not_A * prior_not_A)\n",
        "    posterior_A_given_B = (likelihood_B_given_A * prior_A) / P_B\n",
        "    return posterior_A_given_B\n",
        "\n",
        "# Given probabilities\n",
        "prior_cancer = 0.01\n",
        "likelihood_positive_given_cancer = 0.9\n",
        "prior_no_cancer = 1 - prior_cancer\n",
        "likelihood_positive_given_no_cancer = 0.05\n",
        "\n",
        "# Compute probability of having cancer given a positive test result\n",
        "posterior_cancer_given_positive = bayes_theorem(prior_cancer, likelihood_positive_given_cancer,\n",
        "                                                 prior_no_cancer, likelihood_positive_given_no_cancer)\n",
        "print(f\"Probability of having cancer given a positive test: {posterior_cancer_given_positive:.4f}\")\n",
        " Output: 0.1515 (~15.15%)\n",
        "\n",
        "5. Applications of Bayes' Theorem\n",
        " Naïve Bayes Classifier (Spam filtering, Sentiment Analysis).\n",
        " Medical Diagnosis (Predicting diseases based on test results).\n",
        "Fraud Detection (Finding probability of fraud given transaction data).\n",
        " Speech & Image Recognition (Updating probabilities based on features).\n",
        "\n"
      ],
      "metadata": {
        "id": "5i6sxMtHL0cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:\n",
        "Ans.Differences Between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes\n",
        "Naïve Bayes classifiers are probabilistic models based on Bayes’ Theorem with an assumption that features are conditionally independent given the class. Different variations exist depending on the type of data.\n",
        "\n",
        "Type\tData Type\tAssumption\tBest Use Cases\n",
        "Gaussian Naïve Bayes (GNB)\tContinuous (real-valued)\tAssumes features follow a normal (Gaussian) distribution\tUsed for numerical features like age, height, salary, temperature\n",
        "Multinomial Naïve Bayes (MNB)\tDiscrete (counts)\tFeatures represent word counts or frequency\tUsed in text classification (spam detection, NLP, sentiment analysis)\n",
        "Bernoulli Naïve Bayes (BNB)\tBinary (0/1)\tFeatures represent presence/absence of words\tUsed for binary features like \"word exists or not\" in spam filtering\n",
        " Gaussian Naïve Bayes (GNB)\n",
        " Used for continuous numerical features.\n",
        " Assumes features follow a Gaussian (Normal) distribution.\n",
        " Example: Height, Weight, Age, Temperature, Income, Exam Scores\n",
        "\n",
        " Probability Formula (Normal Distribution Assumption):\n",
        "For each feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , the probability is computed as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "𝜋\n",
        "𝜎\n",
        "2\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "−\n",
        "𝜇\n",
        ")\n",
        "2\n",
        "2\n",
        "𝜎\n",
        "2\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣Y)=\n",
        "2πσ\n",
        "2\n",
        "\n",
        "​\n",
        "\n",
        "1\n",
        "​\n",
        " e\n",
        "−\n",
        "2σ\n",
        "2\n",
        "\n",
        "(x\n",
        "i\n",
        "​\n",
        " −μ)\n",
        "2\n",
        "\n",
        "​\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "𝜇\n",
        "μ = Mean of feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  in class\n",
        "𝑌\n",
        "Y.\n",
        "𝜎\n",
        "2\n",
        "σ\n",
        "2\n",
        "  = Variance of feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  in class\n",
        "𝑌\n",
        "Y.\n",
        " Python Example:\n",
        "\n",
        "python\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "print(\"GaussianNB Accuracy:\", gnb.score(X_test, y_test))\n",
        " Best for:\n",
        "\n",
        "Continuous numerical data\n",
        "Weather prediction, medical diagnosis (e.g., blood pressure, cholesterol levels)\n",
        " Multinomial Naïve Bayes (MNB)\n",
        " Used for discrete features (word counts, frequencies).\n",
        " Common in Natural Language Processing (NLP).\n",
        " Example: Spam filtering, sentiment analysis, text classification.\n",
        "\n",
        " Probability Formula (Word Counts Assumption):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "𝑐\n",
        "𝑜\n",
        "𝑢\n",
        "𝑛\n",
        "𝑡\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        " in class\n",
        "𝑌\n",
        ")\n",
        "+\n",
        "𝛼\n",
        "∑\n",
        "count(all words in class\n",
        "𝑌\n",
        ")\n",
        "+\n",
        "𝛼\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣Y)=\n",
        "∑count(all words in class Y)+α\n",
        "count(x\n",
        "i\n",
        "​\n",
        "  in class Y)+α\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "α = Smoothing parameter (Laplace Smoothing, avoids zero probabilities).\n",
        " Python Example:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\"Free money now\", \"Call me later\", \"Win a lottery now\", \"Meet me for dinner\"]\n",
        "labels = [1, 0, 1, 0]  # 1 = Spam, 0 = Not Spam\n",
        "\n",
        "# Convert text to word count vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Train Multinomial Naïve Bayes\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X, labels)\n",
        "\n",
        "# Predict new text\n",
        "new_doc = [\"Win a free lottery\"]\n",
        "X_new = vectorizer.transform(new_doc)\n",
        "prediction = mnb.predict(X_new)\n",
        "\n",
        "print(\"Prediction:\", \"Spam\" if prediction[0] == 1 else \"Not Spam\")\n",
        " Best for:\n",
        "\n",
        "Text classification, NLP (spam detection, news categorization)\n",
        "Document classification, word frequency-based tasks\n",
        " Bernoulli Naïve Bayes (BNB)\n",
        " Used for binary (0/1) features.\n",
        " Each feature is treated as a \"Yes/No\" (Present/Absent).\n",
        " Example: Spam filtering (word present or not), sentiment analysis.\n",
        "\n",
        " Probability Formula (Binary Feature Assumption):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "𝑝\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "P(x\n",
        "i\n",
        "​\n",
        " ∣Y)=p\n",
        "i\n",
        "x\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " (1−p\n",
        "i\n",
        "​\n",
        " )\n",
        "(1−x\n",
        "i\n",
        "​\n",
        " )\n",
        "\n",
        "where\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  is 1 if the feature is present, else 0.\n",
        "\n",
        " Python Example:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Binary word presence dataset\n",
        "X = [[1, 0, 1], [0, 1, 0], [1, 1, 0], [0, 0, 1]]\n",
        "y = [1, 0, 1, 0]  # 1 = Spam, 0 = Not Spam\n",
        "\n",
        "# Train Bernoulli Naïve Bayes\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X, y)\n",
        "\n",
        "# Predict\n",
        "new_sample = [[1, 0, 0]]  # Example with \"first word present, others absent\"\n",
        "prediction = bnb.predict(new_sample)\n",
        "\n",
        "print(\"Prediction:\", \"Spam\" if prediction[0] == 1 else \"Not Spam\")"
      ],
      "metadata": {
        "id": "jNPK5E41MU4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.When should you use Gaussian Naïve Bayes over other variants\n",
        "Ans.When to Use Gaussian Naïve Bayes (GNB) Over Other Variants?\n",
        "Gaussian Naïve Bayes (GNB) is best suited for continuous numerical data that follows a normal (Gaussian) distribution. It differs from MultinomialNB and BernoulliNB, which are designed for categorical and text-based data.\n",
        "\n",
        " Use GaussianNB When:\n",
        "1 Features Are Continuous (Real Numbers)\n",
        " If your dataset contains numerical variables like height, weight, age, income, temperature, blood pressure, etc., then GaussianNB is the best choice.\n",
        " It models each feature using a normal distribution, which works well for real-valued inputs.\n",
        "\n",
        " Example: Medical Diagnosis (Diabetes Prediction)\n",
        "\n",
        "Features: Blood pressure, glucose levels, BMI.\n",
        "Target: Diabetic (Yes/No).\n",
        "Why GNB? These values are continuous and assumed to be normally distributed.\n",
        "2 Features Are Normally Distributed (or Close to It)\n",
        " If your feature values follow a bell-shaped curve (Gaussian distribution), GNB will perform well.\n",
        " Even if the distribution isn’t perfectly normal, GNB is still effective in many cases.\n",
        "\n",
        " Example: Iris Flower Classification\n",
        "\n",
        "Features: Sepal length, petal width (real-valued).\n",
        "Why GNB? These features are continuous and approximately Gaussian.\n",
        "python\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict & Evaluate\n",
        "accuracy = gnb.score(X_test, y_test)\n",
        "print(\"GaussianNB Accuracy:\", accuracy)\n",
        " When You Need a Fast & Simple Classifier\n",
        " GNB is extremely fast to train and predict because it computes probabilities analytically without iterative optimization.\n",
        " Works well for small to medium-sized datasets.\n",
        "\n",
        " Example: Real-time Predictions\n",
        "\n",
        "Use Case: Fraud detection in financial transactions.\n",
        "Why GNB? It provides quick predictions on continuous features like transaction amount, time, and frequency.\n",
        " Avoid GaussianNB When:\n",
        " Data Is Categorical or Discrete (Use MultinomialNB)\n",
        " If your features represent word counts, frequency, or categorical values, GaussianNB is not suitable.\n",
        " Instead, use MultinomialNB for text classification tasks.\n",
        "\n",
        " Example: Spam Detection\n",
        "\n",
        "Wrong choice: GaussianNB (doesn’t handle word counts well).\n",
        "Right choice: MultinomialNB (counts word occurrences).\n",
        " Data Has Binary Features (Use BernoulliNB)\n",
        " If features are binary (0 or 1), like \"word present or not\", use BernoulliNB instead.\n",
        " GNB assumes continuous inputs, so binary data won't work well.\n",
        "\n",
        " Example: Spam Filtering Based on Presence of Words\n",
        "\n",
        "Wrong choice: GaussianNB (assumes numerical values).\n",
        "Right choice: BernoulliNB (better for binary presence/absence).\n",
        " Features Are Highly Correlated\n",
        " Naïve Bayes assumes feature independence. If features are strongly correlated, the assumption breaks down.\n",
        " Solution: Consider Logistic Regression or Decision Trees instead.\n",
        "\n",
        " Example: Predicting House Prices\n",
        "\n",
        "Features: Square footage, number of rooms, lot size.\n",
        "These features are strongly correlated, making GNB less effective.\n",
        "Alternative: Use Linear Regression or Decision Trees.\n"
      ],
      "metadata": {
        "id": "n1ml1Z3MNRiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.What are the key assumptions made by Naïve Bayes\n",
        "Ans.Key Assumptions of Naïve Bayes\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes' Theorem. It is called \"naïve\" because it makes strong independence assumptions about the features. Here are the key assumptions:\n",
        "\n",
        "1 Conditional Independence Assumption\n",
        " Assumption: Features are conditionally independent given the class label.\n",
        "\n",
        " Mathematically, for a given class\n",
        "𝑌\n",
        "Y, the probability of input features\n",
        "𝑋\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "X=(x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,...,x\n",
        "n\n",
        "​\n",
        " ) is:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "1\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "2\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "×\n",
        ".\n",
        ".\n",
        ".\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑛\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(X∣Y)=P(x\n",
        "1\n",
        "​\n",
        " ∣Y)×P(x\n",
        "2\n",
        "​\n",
        " ∣Y)×...×P(x\n",
        "n\n",
        "​\n",
        " ∣Y)\n",
        " Why it's useful?\n",
        "\n",
        "Simplifies calculations, making Naïve Bayes fast and efficient.\n",
        "Works well even if the independence assumption is not strictly true.\n",
        " When this assumption fails?\n",
        "\n",
        "If features are highly correlated, Naïve Bayes performs poorly.\n",
        "Example: Predicting house prices using lot size, number of rooms, and total square footage (which are correlated).\n",
        "Alternative: Use Logistic Regression, Decision Trees, or Random Forest instead.\n",
        "2 Feature Probability Distribution Assumption\n",
        " Assumption: Each feature follows a specific probability distribution depending on the variant of Naïve Bayes used.\n",
        "\n",
        "Variants of Naïve Bayes and Their Assumptions:\n",
        "Naïve Bayes Type\tAssumed Feature Distribution\tSuitable for\n",
        "GaussianNB\tFeatures follow a normal (Gaussian) distribution\tContinuous data (e.g., age, height, blood pressure)\n",
        "MultinomialNB\tFeatures represent word counts or frequencies\tText classification (spam filtering, sentiment analysis)\n",
        "BernoulliNB\tFeatures are binary (0/1)\tBinary classification (word presence/absence in spam detection)\n",
        " What happens if the assumption is violated?\n",
        "\n",
        "If data is not normally distributed, GaussianNB may not perform well.\n",
        "If features are not word counts, MultinomialNB will give incorrect probabilities.\n",
        "Solution: Use a different Naïve Bayes variant or try other models (e.g., SVM, Decision Trees).\n",
        "3 Class Conditional Independence\n",
        " Assumption: Each feature contributes independently to the class label.\n",
        " Mathematically,\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "∝\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "1\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "2\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "×\n",
        ".\n",
        ".\n",
        ".\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑥\n",
        "𝑛\n",
        "∣\n",
        "𝑌\n",
        ")\n",
        "P(Y∣X)∝P(Y)×P(x\n",
        "1\n",
        "​\n",
        " ∣Y)×P(x\n",
        "2\n",
        "​\n",
        " ∣Y)×...×P(x\n",
        "n\n",
        "​\n",
        " ∣Y)\n",
        "This means:\n",
        "\n",
        "The effect of one feature does not depend on another.\n",
        "Example: In email classification, the presence of \"free\" and \"lottery\" are treated as independent, even though they often appear together in spam emails.\n",
        " Why it's useful?\n",
        "\n",
        "Makes Naïve Bayes computationally efficient.\n",
        "Works well in text classification, where words often appear independently in documents.\n",
        " When does it fail?\n",
        "\n",
        "If features have strong dependencies (e.g., \"free\" and \"lottery\" always appear together).\n",
        "Solution:\n",
        "Use feature selection techniques (e.g., remove highly correlated words).\n",
        "Use models that handle dependencies (e.g., Random Forest, Neural Networks).\n",
        "4 All Features Are Equally Important\n",
        " Assumption: All features contribute equally to the final classification.\n",
        "\n",
        " Works well when:\n",
        "\n",
        "Features are balanced and have similar levels of importance.\n",
        " Fails when:\n",
        "\n",
        "Some features are more influential than others.\n",
        "Example: In fraud detection, transaction amount may be more important than time of day.\n",
        " Solution:\n",
        "\n",
        "Feature engineering: Scale important features properly.\n",
        "Use different models: Logistic Regression, SVM, or Decision Trees can assign different feature weights.\n"
      ],
      "metadata": {
        "id": "31OuVHMjNziW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. What are the advantages and disadvantages of Naïve Bayes\n",
        "Ans.Advantages and  Disadvantages of Naïve Bayes\n",
        "Naïve Bayes is a powerful, simple, and efficient algorithm, but it has some limitations. Let’s break it down.\n",
        "\n",
        " Advantages of Naïve Bayes\n",
        "1 Fast and Efficient\n",
        "Training and prediction are very fast, even on large datasets.\n",
        "Computationally lightweight, making it ideal for real-time applications.\n",
        "Example: Spam detection in emails—can classify thousands of emails in seconds.\n",
        "\n",
        "2 Works Well with Small Datasets\n",
        "Unlike deep learning or complex models, Naïve Bayes works well with limited data.\n",
        "Can generalize well even with a few training examples.\n",
        "Example: Classifying medical conditions based on a small set of patient records.\n",
        "\n",
        "3 Effective for High-Dimensional Data\n",
        "Performs well with high-dimensional datasets (lots of features).\n",
        "Commonly used in text classification (e.g., sentiment analysis, spam detection).\n",
        " Example: Classifying news articles based on thousands of words in the text.\n",
        "\n",
        "4 Handles Missing Data Well\n",
        "Since it computes probabilities independently for each feature, missing values don’t affect it much.\n",
        " Example: In a dataset where some features (e.g., blood pressure) are missing, it can still predict disease probability.\n",
        "\n",
        "5 Works Well for Categorical and Text Data\n",
        "Multinomial and Bernoulli Naïve Bayes work exceptionally well for text classification and categorical data.\n",
        " Example:\n",
        "\n",
        "Spam detection (word frequency-based).\n",
        "Sentiment analysis (positive/negative review classification).\n",
        "6 Handles Class Imbalance Well\n",
        "Works well even when one class is much more frequent than others.\n",
        "Assigns probabilities based on prior occurrences.\n",
        " Example: Fraud detection, where fraud cases are rare but still need accurate classification.\n",
        "\n",
        " Disadvantages of Naïve Bayes\n",
        "1 Assumes Feature Independence\n",
        "The biggest drawback: It assumes all features are independent (which is rarely true).\n",
        "If features are correlated, Naïve Bayes performs poorly.\n",
        " Example:\n",
        "\n",
        "Predicting house prices using square footage and number of rooms (which are correlated).\n",
        "Solution: Use models like Logistic Regression, SVM, or Random Forest instead.\n",
        "2 Struggles with Continuous Data\n",
        "Gaussian Naïve Bayes assumes continuous data follows a normal distribution, which may not always be true.\n",
        "If the real data distribution is skewed or multimodal, performance drops.\n",
        " Example:\n",
        "\n",
        "Predicting customer age range for an online service. If the distribution is not Gaussian, predictions may be off.\n",
        "Solution: Try Kernel Density Estimation (KDE) or other classifiers like Decision Trees.\n",
        "3 Zero Probability Problem (Smoothing Required)\n",
        "If a word or feature never appeared in training data, it gets a probability of 0, which can cause issues.\n",
        "Solution: Laplace Smoothing (adds a small value to avoid zero probabilities).\n",
        " Example:\n",
        "\n",
        "If a new word appears in an email, Naïve Bayes might classify it incorrectly as non-spam.\n",
        "Using Laplace Smoothing: Adjust probabilities to avoid hard zeros.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Use Laplace smoothing (alpha > 0 to avoid zero probabilities)\n",
        "nb = MultinomialNB(alpha=1.0)\n",
        "4 Not Good for Complex Decision Boundaries\n",
        "Works best for simple decision boundaries but fails for complex, non-linear relationships.\n",
        "Solution: Use SVM, Neural Networks, or Decision Trees.\n",
        " Example:\n",
        "\n",
        "Naïve Bayes struggles in image classification where pixels interact in complex ways.\n",
        "Instead, CNNs (Convolutional Neural Networks) are a better choice.\n",
        "5 Sensitive to Noisy or Redundant Features\n",
        "If there are too many irrelevant or correlated features, performance drops.\n",
        "Solution: Perform feature selection before applying Naïve Bayes.\n",
        " Example:\n",
        "\n",
        "If spam detection includes unrelated features like email font size or background color, Naïve Bayes may get confused."
      ],
      "metadata": {
        "id": "NwGjnu9pOXia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Why is Naïve Bayes a good choice for text classification\n",
        "Ans.Naïve Bayes is one of the best algorithms for text classification, including spam detection, sentiment analysis, topic categorization, and email filtering. Here’s why:\n",
        "\n",
        " 1 Works Well with High-Dimensional Data (Many Features)\n",
        " In text classification, every unique word is a feature. Documents can have thousands of words (features), leading to high-dimensional data.\n",
        " Naïve Bayes handles high-dimensional data efficiently because it treats each word independently and only computes probabilities.\n",
        "\n",
        " Example:\n",
        "\n",
        "A dataset with 10,000 unique words (features) and 1 million emails (examples).\n",
        "Naïve Bayes still trains and predicts efficiently because it only needs word frequency counts.\n",
        "Other models like SVM or deep learning struggle with high-dimensional text without feature reduction.\n",
        "\n",
        " 2 Very Fast to Train and Predict\n",
        " Naïve Bayes uses simple probability calculations, making it one of the fastest classifiers.\n",
        " Training takes seconds or minutes, even for millions of documents.\n",
        "\n",
        " Example:\n",
        "\n",
        "Spam filtering: Gmail processes billions of emails daily.\n",
        "Naïve Bayes can classify emails in milliseconds based on word probabilities.\n",
        " Alternative models (like SVM or deep learning) take longer to train and predict.\n",
        "\n",
        " 3 Handles Missing Words Well (Sparse Data)\n",
        " In text classification, not every word appears in every document.\n",
        " Naïve Bayes is robust to missing features (unseen words) because it calculates probabilities independently for each word.\n",
        "\n",
        " Example:\n",
        "\n",
        "If a word like \"crypto\" appears only in some emails, it doesn't break the model because Naïve Bayes still works with other available words.\n",
        " Other models like Decision Trees or SVM may struggle with missing words.\n",
        "\n",
        " 4 Works Well for Imbalanced Datasets (Spam vs. Non-Spam)\n",
        " Many real-world datasets are imbalanced (e.g., spam vs. non-spam emails, fake vs. real reviews).\n",
        " Naïve Bayes still performs well because it assigns probabilities based on prior frequencies.\n",
        "\n",
        " Example:\n",
        "\n",
        "If 95% of emails are non-spam and 5% are spam, Naïve Bayes automatically adjusts for imbalance using prior probabilities:\n",
        "𝑃\n",
        "(\n",
        "𝑆\n",
        "𝑝\n",
        "𝑎\n",
        "𝑚\n",
        ")\n",
        "=\n",
        "0.05\n",
        ",\n",
        "𝑃\n",
        "(\n",
        "𝑁\n",
        "𝑜\n",
        "𝑡\n",
        "\n",
        "𝑆\n",
        "𝑝\n",
        "𝑎\n",
        "𝑚\n",
        ")\n",
        "=\n",
        "0.95\n",
        "P(Spam)=0.05,P(Not Spam)=0.95\n",
        " Other models (like Logistic Regression) require explicit rebalancing techniques.\n",
        "\n",
        "5 Simple and Interpretable\n",
        " Naïve Bayes provides clear probability scores, making it easy to understand.\n",
        " You can see which words contribute most to a classification.\n",
        "\n",
        " Example:\n",
        "\n",
        "If \"lottery\" and \"win\" appear frequently in spam emails, Naïve Bayes assigns high probability to spam.\n",
        "You can interpret why an email is classified as spam.\n",
        " Deep learning models are complex and hard to interpret.\n",
        "\n",
        " 6 Different Variants for Different Text Data\n",
        "Naïve Bayes has different versions that handle different types of text data:\n",
        "\n",
        "Variant\tAssumed Feature Type\tBest Use Case\n",
        "MultinomialNB\tWord counts or frequencies\tSpam filtering, topic classification\n",
        "BernoulliNB\tWord presence (0/1)\tShort text, binary features (e.g., \"word exists or not\")\n",
        "GaussianNB\tContinuous values\tUncommon for text, more for numerical features\n",
        " Example:\n",
        "\n",
        "MultinomialNB works best for email classification (spam filtering).\n",
        "BernoulliNB is useful when only word presence matters (e.g., document categorization).\n",
        " Other models don’t have specific versions optimized for different text types.\n",
        " Example: Using Naïve Bayes for Spam Classification\n",
        " Step 1: Import and Load Data\n",
        "python\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset (spam vs. non-spam messages)\n",
        "emails = [\n",
        "    (\"Win a free iPhone now!\", \"spam\"),\n",
        "    (\"Meeting at 10 AM tomorrow\", \"ham\"),\n",
        "    (\"Congratulations! You won $1000\", \"spam\"),\n",
        "    (\"Reminder: Doctor appointment at 4 PM\", \"ham\"),\n",
        "]\n",
        "\n",
        "# Split data into texts and labels\n",
        "X_texts, y_labels = zip(*emails)\n",
        "\n",
        "# Convert text into word frequency features\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X_texts)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes Model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on new messages\n",
        "new_email = [\"Win a free vacation trip now!\"]\n",
        "X_new = vectorizer.transform(new_email)\n",
        "prediction = nb.predict(X_new)\n",
        "\n",
        "print(\"Prediction:\", prediction[0])  # Expected output: spam"
      ],
      "metadata": {
        "id": "MUVy6AevPFLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Compare SVM and Naïve Bayes for classification tasks\n",
        "Ans.SVM (Support Vector Machine) and Naïve Bayes (NB) are both popular classifiers, but they work differently and are suited for different types of problems. Let's compare them in detail.\n",
        "\n",
        " Key Differences Between SVM and Naïve Bayes\n",
        "Feature\tSVM (Support Vector Machine)\tNaïve Bayes (NB)\n",
        "Approach\tDiscriminative Model (Finds a decision boundary)\tGenerative Model (Estimates probabilities)\n",
        "Algorithm Type\tMaximizes margin between classes (Optimization-based)\tUses Bayes’ Theorem for probability calculation\n",
        "Feature Independence Assumption\tNo assumption (Considers feature relationships)\tAssumes all features are independent\n",
        "Performance on High-Dimensional Data\tWorks well, but requires kernel tricks for non-linear data\tWorks exceptionally well, especially for text classification\n",
        "Computational Complexity\tSlower, especially on large datasets\tExtremely fast and scalable\n",
        "Robustness to Outliers\tMore sensitive to outliers\tLess sensitive to outliers\n",
        "Interpretability\tHarder to interpret (complex decision boundary)\tSimple and interpretable (probabilities assigned)\n",
        "Best for\tComplex classification problems, image recognition, non-linear datasets\tText classification, spam filtering, sentiment analysis\n",
        " When to Use Naïve Bayes?\n",
        " Best suited for:\n",
        "\n",
        "Text classification tasks (spam filtering, sentiment analysis, topic categorization).\n",
        "Real-time applications (fast training and prediction).\n",
        "High-dimensional data (thousands of features, like words in text).\n",
        "Small datasets (performs well even with limited data).\n",
        " Not ideal when:\n",
        "\n",
        "Features are correlated (NB assumes independence, which may not be true).\n",
        "You need a complex decision boundary (e.g., image classification).\n",
        " Example:\n",
        "Naïve Bayes is the best choice for email spam detection because words in emails are mostly independent, and the model only needs word frequency counts.\n",
        "\n",
        " When to Use SVM?\n",
        " Best suited for:\n",
        "\n",
        "Binary classification tasks (e.g., fraud detection, medical diagnosis).\n",
        "Complex decision boundaries (SVM can learn non-linear relationships using kernels).\n",
        "Small to medium datasets (handles non-linearly separable data well).\n",
        "Structured data with feature relationships (since it doesn’t assume independence).\n",
        " Not ideal when:\n",
        "\n",
        "The dataset is very large (SVM is computationally expensive).\n",
        "You need probabilistic outputs (SVM doesn’t provide direct probabilities like NB).\n",
        " Example:\n",
        "SVM is a better choice for image classification (e.g., handwritten digit recognition) because the relationships between pixels are complex and not independent.\n",
        "\n",
        " Example: Comparing SVM and Naïve Bayes for Text Classification\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Sample dataset (Spam vs. Ham)\n",
        "emails = [\n",
        "    (\"Win a free iPhone now!\", \"spam\"),\n",
        "    (\"Meeting at 10 AM tomorrow\", \"ham\"),\n",
        "    (\"Congratulations! You won $1000\", \"spam\"),\n",
        "    (\"Reminder: Doctor appointment at 4 PM\", \"ham\"),\n",
        "]\n",
        "\n",
        "# Prepare data\n",
        "X_texts, y_labels = zip(*emails)\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X_texts)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes Model\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "nb_preds = nb_model.predict(X_test)\n",
        "\n",
        "# Train SVM Model\n",
        "svm_model = SVC(kernel=\"linear\")  # Using linear kernel for text data\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "\n",
        "# Compare Accuracy\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy_score(y_test, nb_preds))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_preds))"
      ],
      "metadata": {
        "id": "XLYOSsDWprF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.How does Laplace Smoothing help in Naïve Bayes?\n",
        "Ans.The Problem: Zero Probability Issue\n",
        "Naïve Bayes calculates the probability of a class given certain features using Bayes' Theorem:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        "∣\n",
        "𝐹\n",
        "𝑒\n",
        "𝑎\n",
        "𝑡\n",
        "𝑢\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        ")\n",
        "∝\n",
        "𝑃\n",
        "(\n",
        "𝐹\n",
        "𝑒\n",
        "𝑎\n",
        "𝑡\n",
        "𝑢\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "P(Class∣Features)∝P(Features∣Class)⋅P(Class)\n",
        "When calculating\n",
        "𝑃\n",
        "(\n",
        "𝐹\n",
        "𝑒\n",
        "𝑎\n",
        "𝑡\n",
        "𝑢\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "P(Features∣Class), we multiply the probabilities of individual features (e.g., words in text classification):\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "1\n",
        ",\n",
        "𝑤\n",
        "2\n",
        ",\n",
        "𝑤\n",
        "3\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "1\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "2\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "3\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "×\n",
        "…\n",
        "P(w\n",
        "1\n",
        "​\n",
        " ,w\n",
        "2\n",
        "​\n",
        " ,w\n",
        "3\n",
        "​\n",
        " ∣Class)=P(w\n",
        "1\n",
        "​\n",
        " ∣Class)×P(w\n",
        "2\n",
        "​\n",
        " ∣Class)×P(w\n",
        "3\n",
        "​\n",
        " ∣Class)×…\n",
        "However, if a word never appears in the training data for a certain class, its probability becomes zero:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "=\n",
        "0\n",
        "P(w\n",
        "i\n",
        "​\n",
        " ∣Class)=0\n",
        "Since probabilities are multiplied, the entire prediction collapses to zero, making classification impossible.\n",
        "\n",
        " The Solution: Laplace Smoothing (Additive Smoothing)\n",
        "To fix the zero probability issue, we apply Laplace Smoothing, which adds a small constant\n",
        "𝑘\n",
        "k to all word counts before calculating probabilities.\n",
        "\n",
        "The smoothed probability formula is:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑤\n",
        "𝑖\n",
        "∣\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "=\n",
        "Count\n",
        "(\n",
        "𝑤\n",
        "𝑖\n",
        ",\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        ")\n",
        "+\n",
        "𝑘\n",
        "Total Words in Class\n",
        "+\n",
        "𝑘\n",
        "×\n",
        "Vocabulary Size\n",
        "P(w\n",
        "i\n",
        "​\n",
        " ∣Class)=\n",
        "Total Words in Class+k×Vocabulary Size\n",
        "Count(w\n",
        "i\n",
        "​\n",
        " ,Class)+k\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑘\n",
        "k (Laplace constant) is typically 1.\n",
        "Count(\n",
        "𝑤\n",
        "𝑖\n",
        ",\n",
        "𝐶\n",
        "𝑙\n",
        "𝑎\n",
        "𝑠\n",
        "𝑠\n",
        "w\n",
        "i\n",
        "​\n",
        " ,Class) = Number of times word\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        "  appears in documents of a given class.\n",
        "Total Words in Class = Sum of all word counts in documents of that class.\n",
        "Vocabulary Size = Number of unique words in the dataset.\n",
        " Example: Without vs. With Laplace Smoothing\n",
        " Without Laplace Smoothing:\n",
        "Consider spam detection with two classes: Spam and Not Spam.\n",
        "Suppose we have this training data:\n",
        "\n",
        "Email\tContent\tClass\n",
        "1\t\"Win a free iPhone now\"\tSpam\n",
        "2\t\"Meeting at 10 AM\"\tNot Spam\n",
        "3\t\"Congratulations! You won\"\tSpam\n",
        "Now, let's predict whether \"You won a free trip\" is spam.\n",
        "\n",
        "Word counts in spam emails:\n",
        "\"win\" = 1, \"free\" = 1, \"iPhone\" = 1, \"congratulations\" = 1, \"won\" = 1\n",
        "\"trip\" never appeared in training data.\n",
        "Probability of \"trip\" in Spam:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "\"\n",
        "𝑡\n",
        "𝑟\n",
        "𝑖\n",
        "𝑝\n",
        "\"\n",
        "∣\n",
        "𝑆\n",
        "𝑝\n",
        "𝑎\n",
        "𝑚\n",
        ")\n",
        "=\n",
        "0\n",
        "Total Words in Spam\n",
        "P(\"trip\"∣Spam)=\n",
        "Total Words in Spam\n",
        "0\n",
        "​\n",
        "\n",
        "Since P(\"trip\" | Spam) = 0, the entire probability of the email being spam collapses to zero.\n",
        "\n",
        " With Laplace Smoothing (k=1):\n",
        "Using Laplace Smoothing:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "\"\n",
        "𝑡\n",
        "𝑟\n",
        "𝑖\n",
        "𝑝\n",
        "\"\n",
        "∣\n",
        "𝑆\n",
        "𝑝\n",
        "𝑎\n",
        "𝑚\n",
        ")\n",
        "=\n",
        "0\n",
        "+\n",
        "1\n",
        "Total Words in Spam\n",
        "+\n",
        "Vocabulary Size\n",
        "P(\"trip\"∣Spam)=\n",
        "Total Words in Spam+Vocabulary Size\n",
        "0+1\n",
        "​\n",
        "\n",
        "Even though \"trip\" never appeared, it now gets a nonzero probability, allowing the classifier to still make a meaningful prediction.\n",
        "\n",
        " Key Benefits of Laplace Smoothing\n",
        " Prevents Zero Probabilities – Ensures unseen words don’t make the entire classification fail.\n",
        " Improves Generalization – Helps the model handle new words in test data.\n",
        " Works Well for Text Classification – Commonly used in Naïve Bayes for spam filtering, sentiment analysis, and document categorization.\n",
        "\n",
        " Python Example: Naïve Bayes with Laplace Smoothing\n",
        "By default, Scikit-learn’s Naïve Bayes (MultinomialNB) uses Laplace Smoothing with\n",
        "𝛼\n",
        "=\n",
        "1\n",
        "α=1 (equivalent to\n",
        "𝑘\n",
        "k).\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample training dataset\n",
        "emails = [\"Win a free iPhone\", \"Meeting at 10 AM\", \"Congratulations! You won\"]\n",
        "labels = [\"spam\", \"ham\", \"spam\"]\n",
        "\n",
        "# Convert text to word count vectors\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "# Train Naïve Bayes with Laplace Smoothing (alpha=1)\n",
        "nb = MultinomialNB(alpha=1)\n",
        "nb.fit(X, labels)\n",
        "\n",
        "# Predict new email\n",
        "new_email = [\"You won a free trip\"]\n",
        "X_new = vectorizer.transform(new_email)\n",
        "prediction = nb.predict(X_new)\n",
        "\n",
        "print(\"Prediction:\", prediction[0])  # Expected output: spam"
      ],
      "metadata": {
        "id": "zxXkIdbrrt0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:\n",
        "Ans."
      ],
      "metadata": {
        "id": "Day-ot20sydq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with an RBF kernel\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "IqAzTfM7s8-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "compare their accuracies\n",
        "Ans."
      ],
      "metadata": {
        "id": "G-ehpUpes91X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Compare Accuracies\n",
        "print(f\"Linear Kernel Accuracy: {accuracy_linear:.2f}\")\n",
        "print(f\"RBF Kernel Accuracy: {accuracy_rbf:.2f}\")\n"
      ],
      "metadata": {
        "id": "sWvQqcygtGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "Squared Error (MSE)\n",
        "Ans."
      ],
      "metadata": {
        "id": "Le3oosVHtHdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = datasets.load_diabetes()  # Using Diabetes dataset since Boston Housing is deprecated\n",
        "X, y = boston.data, boston.target  # Features and target\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVR model with an RBF kernel\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
        "svr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = svr_model.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "zpVX9VjRtOoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "boundary\n",
        "Ans."
      ],
      "metadata": {
        "id": "jEwn9w7YtPiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Standardize features for better SVM performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train SVM with a Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=plt.cm.coolwarm)\n",
        "    plt.title(\"SVM with Polynomial Kernel (degree=3)\")\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize decision boundary\n",
        "plot_decision_boundary(svm_poly, X, y)\n"
      ],
      "metadata": {
        "id": "HSCMaXSAtWhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "evaluate accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "WDmY7wgXtYVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes Classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "94y6-IWItixc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26.Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "Newsgroups dataset.\n",
        "Ans."
      ],
      "metadata": {
        "id": "fSTJSivMtkSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset (only some categories for speed)\n",
        "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics', 'talk.politics.mideast']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X, y = newsgroups.data, newsgroups.target  # Text data and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline for text preprocessing and classification\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),          # Convert text to token counts\n",
        "    ('tfidf', TfidfTransformer()),        # Apply TF-IDF transformation\n",
        "    ('clf', MultinomialNB())              # Tra\n"
      ],
      "metadata": {
        "id": "0gRUAacatxk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27.Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "boundaries visually\n",
        "Ans."
      ],
      "metadata": {
        "id": "7g-sLB5otybT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic dataset (non-linearly separable)\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset for better SVM performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Different C values to compare\n",
        "C_values = [0.01, 1, 100]\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(model, X, y, title):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
        "                         np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=plt.cm.coolwarm)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Feature 1\")\n",
        "    plt.ylabel(\"Feature 2\")\n",
        "\n",
        "# Train SVM classifiers with different C values and plot their decision boundaries\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    svm_model = SVC(kernel='rbf', C=C, gamma='scale')\n",
        "    svm_model.fit(X, y)\n",
        "\n",
        "    plt.subplot(1, len(C_values), i + 1)\n",
        "    plot_decision_boundary(svm_model, X, y, f\"SVM with C={C}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yAg5bSaMt7c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "binary features\n",
        "Ans."
      ],
      "metadata": {
        "id": "Iel4mTn3t94S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic dataset with binary features\n",
        "X, y = make_classification(n_samples=500, n_features=10, n_informative=5,\n",
        "                           n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Convert features into binary (0 or 1) using a threshold\n",
        "X = (X > 0).astype(int)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bernoulli Naïve Bayes Classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "WhHUHeVAuEl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "unscaled data\n",
        "Ans."
      ],
      "metadata": {
        "id": "2kPs2DXRuHTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM model WITHOUT feature scaling\n",
        "svm_unscaled = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feat\n"
      ],
      "metadata": {
        "id": "6LKv9wYjuUsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "after Laplace Smoothing\n",
        "ANS."
      ],
      "metadata": {
        "id": "82l0WgWFuVqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes WITHOUT Laplace Smoothing (default)\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)  # No Laplace Smoothing\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Train Gaussian Naïve Bayes WITH Laplace Smoothing (default smoothing)\n",
        "gnb_smoothing = GaussianNB(var_smoothing=1e-9)  # Default small smoothing\n",
        "gnb_smoothing.fit(X_train, y_train)\n",
        "y_pred_smoothing = gnb_smoothing.predict(X_test)\n",
        "accuracy_smoothing = accuracy_score(y_test, y_pred_smoothing)\n",
        "\n",
        "# Compare results\n",
        "print(f\"Accuracy WITHOUT Laplace Smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy WITH Laplace Smoothing: {accuracy_smoothing:.4f}\")\n",
        "\n",
        "# Compare predictions\n",
        "print(\"\\nPredictions WITHOUT Smoothing:\", y_pred_no_smoothing[:10])\n",
        "print(\"Predictions WITH Smoothing:\", y_pred_smoothing[:10])\n"
      ],
      "metadata": {
        "id": "7HGaawjuudxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31.Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "gamma, kernel)\n",
        "Ans."
      ],
      "metadata": {
        "id": "KNiDE25yuew0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],             # Regularization parameter\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient (for RBF, poly, sigmoid)\n",
        "    'kernel': ['linear', 'rbf', 'poly']  # Types of kernel\n",
        "}\n",
        "\n",
        "# Initialize SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Best Model Accuracy on Test Data: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "qmg8D87GuoL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q32.Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "check it improve accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "madESZ7zupAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
        "                           weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train an SVM model WITHOUT class weighting\n",
        "svm_no_weight = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "# Train an SVM model WITH class weighting\n",
        "svm_with_weight = SVC(kernel='rbf', C=1.0, gamma='scale', class_weight='balanced', random_state=42)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "# Evaluate both models\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "\n",
        "print(f\"Accuracy WITHOUT class weighting: {accuracy_no_weight:.4f}\")\n",
        "print(f\"Accuracy WITH class weighting: {accuracy_with_weight:.4f}\")\n",
        "\n",
        "# Print classification reports for better insight\n",
        "print(\"\\nClassification Report WITHOUT class weighting:\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"\\nClassification Report WITH class weighting:\")\n",
        "print(classification_report(y_test, y_pred_with_weight))\n"
      ],
      "metadata": {
        "id": "JWcZ44b1uyh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q33.Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
        "Ans."
      ],
      "metadata": {
        "id": "HupcYhoTuzza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the SMS Spam Collection dataset (download if needed)\n",
        "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms-spam-collection.csv\"\n",
        "df = pd.read_csv(url, encoding='latin-1')\n",
        "\n",
        "# Rename columns for clarity\n",
        "df.columns = [\"label\", \"message\"]\n",
        "\n",
        "# Convert labels to binary values (ham = 0, spam = 1)\n",
        "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"message\"], df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text messages into numerical feature vectors using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Limit features for efficiency\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "MtabpLLSu7lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q34.Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "compare their accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "WKgykNzJu_5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Train a Naïve Bayes Classifier (GaussianNB)\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy of both models\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of SVM Classifier: {accuracy_svm:.4f}\")\n",
        "print(f\"Accuracy of Naïve Bayes Classifier: {accuracy_nb:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_svm > accuracy_nb:\n",
        "    print(\"SVM performs better than Naïve Bayes on this dataset.\")\n",
        "elif accuracy_nb > accuracy_svm:\n",
        "    print(\"Naïve Bayes performs better than SVM on this dataset.\")\n",
        "else:\n",
        "    print(\"Both classifiers have similar accuracy.\")\n"
      ],
      "metadata": {
        "id": "RSB3_ZmOvIGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q35.Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "results\n",
        "Ans."
      ],
      "metadata": {
        "id": "zBZ8GQGlvI-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naïve Bayes classifier WITHOUT feature selection\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_no_fs = nb_model.predict(X_test)\n",
        "accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "\n",
        "# Perform feature selection (SelectKBest with Chi-Square Test)\n",
        "k = 10  # Select top 10 features\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train a Naïve Bayes classifier WITH feature selection\n",
        "nb_model_fs = GaussianNB()\n",
        "nb_model_fs.fit(X_train_selected, y_train)\n",
        "y_pred_fs = nb_model_fs.predict(X_test_selected)\n",
        "accuracy_fs = accuracy_score(y_test, y_pred_fs)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy WITHOUT Feature Selection: {accuracy_no_fs:.4f}\")\n",
        "print(f\"Accuracy WITH Feature Selection (Top {k} Features): {accuracy_fs:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_fs > accuracy_no_fs:\n",
        "    print(f\"Feature selection improved accuracy by {accuracy_fs - accuracy_no_fs:.4f}\")\n",
        "else:\n",
        "    print(f\"Feature selection did not improve accuracy. Consider tuning the number of features.\")\n"
      ],
      "metadata": {
        "id": "QjWvqLmNvUpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q36.Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "strategies on the Wine dataset and compare their accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "9TVRD3AUvVhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM using One-vs-Rest (OvR)\n",
        "svm_ovr = OneVsRestClassifier(SVC(kernel='linear', C=1.0, random_state=42))\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# Train SVM using One-vs-One (OvO)\n",
        "svm_ovo = OneVsOneClassifier(SVC(kernel='linear', C=1.0, random_state=42))\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test,\n"
      ],
      "metadata": {
        "id": "wA-Sto-2vc2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q37.Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "Cancer dataset and compare their accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "zUTjxh6_veAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, gamma='scale', random_state=42)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy using Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy using Polynomial Kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy using RBF Kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "best_kernel = max(zip([\"Linear\", \"Polynomial\", \"RBF\"], [accuracy_linear, accuracy_poly, accuracy_rbf]), key=lambda x: x[1])\n",
        "print(f\"\\nBest performing kernel: {best_kernel[0]} with accuracy {best_kernel[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "Dqb0ssmUvl1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q38.Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "average accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "tLMPuw07vm2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Define Stratified K-Fold Cross-Validation\n",
        "k = 5  # Number of folds\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# Train SVM Classifier using Stratified K-Fold Cross-Validation\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "\n",
        "# Perform cross-validation and compute accuracy for each fold\n",
        "accuracies = cross_val_score(svm_model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracies for each fold: {accuracies}\")\n",
        "print(f\"Average Accurac\n"
      ],
      "metadata": {
        "id": "4NrT4_FFvuZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q39.Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "performance\n",
        "Ans."
      ],
      "metadata": {
        "id": "0Jd69HUsvvSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes with default prior probabilities\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Define custom prior probabilities (e.g., assuming class imbalance)\n",
        "custom_priors = [0.3, 0.7]  # Adjust based on domain knowledge\n",
        "\n",
        "# Train Naïve Bayes with custom priors\n",
        "nb_custom = GaussianNB(priors=custom_priors)\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with Default Priors: {accuracy_default:.4f}\")\n",
        "print(f\"Accuracy with Custom Priors {custom_priors}: {accuracy_custom:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_custom > accuracy_default:\n",
        "    print(\"Custom prior probabilities improved accuracy.\")\n",
        "elif accuracy_custom < accuracy_default:\n",
        "    print(\"Default prior probabilities performed better.\")\n",
        "else:\n",
        "    print(\"Both models have the same accuracy.\")\n"
      ],
      "metadata": {
        "id": "owx9FXI0v4IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q40.Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "compare accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "WH8JpEuTv5xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM without feature selection\n",
        "svm_full = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_full.fit(X_train, y_train)\n",
        "y_pred_full = svm_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Perform Recursive Feature Elimination (RFE) with SVM\n",
        "n_features_to_select = 10  # Choose number of features to keep\n",
        "rfe = RFE(estimator=SVC(kernel='linear', C=1.0), n_features_to_select=n_features_to_select)\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Train SVM with selected features\n",
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "svm_rfe = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with all features: {accuracy_full:.4f}\")\n",
        "print(f\"Accuracy after RFE (Top {\n"
      ],
      "metadata": {
        "id": "1o5gxpbEwBaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q41.Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "F1-Score instead of accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "obYYlDbUwC2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Print full classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "xDn7ibhiwNSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q42.Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "(Cross-Entropy Loss)\n",
        "Ans."
      ],
      "metadata": {
        "id": "lwnwyqv-wRQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for log loss calculation\n",
        "y_prob = nb_model.predict_proba(X_test)  # Get probability estimates for both classes\n",
        "\n",
        "# Compute Log Loss (Cross-Entropy Loss)\n",
        "logloss = log_loss(y_test, y_prob)\n",
        "\n",
        "# Print results\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {logloss:.4f}\")\n"
      ],
      "metadata": {
        "id": "mxRLyrgYwY-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q43.Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "Ans."
      ],
      "metadata": {
        "id": "wOfji7JKwaMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Compute the Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the Confusion Matrix using Seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Malignant\", \"Benign\"], yticklabels=[\"Malignant\", \"Benign\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - SVM Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hWDqaPSzwk3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q44.Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "Error (MAE) instead of MSE\n",
        "Ans."
      ],
      "metadata": {
        "id": "vdTLyYvLwpG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # Features and target variable\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply Feature Scaling (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# Reshape y for scaling (SVR requires y as a 2D array)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Train an SVM Regressor (SVR)\n",
        "svr_model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
        "svr_model.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_scaled = svr_model.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Compute Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "oClPuOnLwtUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q45.Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "score\n",
        "Ans."
      ],
      "metadata": {
        "id": "Qx6QZ9DewwC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target  # Features and labels\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naïve Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC computation\n",
        "y_prob = nb_model.predict_proba(X_test)[:, 1]  # Probability of class 1 (Benign)\n",
        "\n",
        "# Compute the ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Print results\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "tzEHFcbzw5pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q46.Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "Ans."
      ],
      "metadata": {
        "id": "RhcJzoXrw7rX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F29oy4pdxBGm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}