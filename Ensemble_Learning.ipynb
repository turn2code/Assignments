{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PI78w1Rs6gE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Can we use Bagging for regression problems?\n",
        "Ans.Yes, Bagging (Bootstrap Aggregating) can be used for regression problems in Python. Bagging is not limited to classification; it also works well for regression tasks by reducing variance and improving model stability.\n",
        "\n",
        "How Bagging Works for Regression:\n",
        "It creates multiple bootstrap samples from the training data.\n",
        "A base regression model (e.g., Decision Tree Regressor) is trained on each sample.\n",
        "The final prediction is obtained by averaging the predictions from all models."
      ],
      "metadata": {
        "id": "pWnyGft6s7jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate sample regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base estimator (e.g., Decision Tree Regressor)\n",
        "base_model = DecisionTreeRegressor()\n",
        "\n",
        "# Create the Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n"
      ],
      "metadata": {
        "id": "lGfQoMTYtDIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the difference between multiple model training and single model training?\n",
        "Ans.Difference Between Single Model Training and Multiple Model Training\n",
        "1. Definition\n",
        "Single Model Training: Train one model on the entire dataset.\n",
        "Multiple Model Training: Train multiple models independently or as an ensemble.\n",
        "2. Complexity\n",
        "Single Model: Easier to implement and requires less coordination.\n",
        "Multiple Models: More complex, requiring additional computation and resources.\n",
        "3. Training Time\n",
        "Single Model: Faster as only one model is trained.\n",
        "Multiple Models: Slower due to training multiple models and aggregating results.\n",
        "4. Performance & Generalization\n",
        "Single Model: Can suffer from overfitting (high variance) or underfitting (high bias).\n",
        "Multiple Models: Typically improves generalization and reduces bias/variance.\n",
        "5. Robustness to Noise & Outliers\n",
        "Single Model: More sensitive to noise, may struggle with unstable predictions.\n",
        "Multiple Models: More stable, as different models compensate for each other's weaknesses.\n",
        "6. Common Techniques\n",
        "Single Model: Logistic Regression, Decision Trees, Neural Networks.\n",
        "Multiple Models:\n",
        "Bagging (e.g., Random Forest) → Reduces variance.\n",
        "Boosting (e.g., XGBoost, AdaBoost) → Reduces bias.\n",
        "Stacking & Voting → Combines multiple models for better performance."
      ],
      "metadata": {
        "id": "sY_6FLHjtFJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.Explain the concept of feature randomness in Random Forest?\n",
        "Ans.Feature Randomness in Random Forest\n",
        "Feature randomness is a key concept in Random Forest, which helps improve generalization and reduce overfitting. It ensures that different trees in the forest use different subsets of features, making the ensemble more diverse and robust.\n",
        "\n",
        "How Feature Randomness Works in Random Forest?\n",
        "Random Subset of Features for Each Tree:\n",
        "\n",
        "When training each decision tree in the Random Forest, only a random subset of features (instead of all features) is considered at each split.\n",
        "This prevents trees from always picking the most dominant feature, leading to more diverse trees in the ensemble.\n",
        "Controlled by max_features Parameter:\n",
        "\n",
        "In sklearn.ensemble.RandomForestClassifier and RandomForestRegressor, the max_features parameter controls feature randomness:\n",
        "For Classification (RandomForestClassifier):\n",
        "Default: sqrt(n_features) (square root of total features).\n",
        "For Regression (RandomForestRegressor):\n",
        "Default: n_features / 3 (one-third of total features).\n",
        "Effect on Model Performance:\n",
        "\n",
        "Less Feature Randomness (max_features = total_features)\n",
        "Each tree sees almost the same data → Less diversity, more correlation among trees.\n",
        "Can lead to overfitting (especially if features are highly correlated).\n",
        "More Feature Randomness (max_features is small)\n",
        "Trees become more diverse → Reduces overfitting.\n",
        "However, if max_features is too low, individual trees may become too weak.\n"
      ],
      "metadata": {
        "id": "1exloIMjtZkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What is OOB (Out-of-Bag) Score?\n",
        "Ans.OOB (Out-of-Bag) Score in Random Forest\n",
        "The Out-of-Bag (OOB) Score is a performance metric used in Random Forest to estimate model accuracy without using a separate validation set. It helps in evaluating how well the model generalizes to unseen data.\n",
        "\n",
        "How OOB Score Works?\n",
        "Bootstrap Sampling:\n",
        "\n",
        "Random Forest uses Bootstrap Aggregation (Bagging), where each decision tree is trained on a random subset of the dataset (with replacement).\n",
        "Some data points are not included in this training sample—these are called Out-of-Bag (OOB) samples.\n",
        "Prediction on OOB Samples:\n",
        "\n",
        "Since each tree in the forest does not see certain data points, those OOB samples can be used as a validation set for that tree.\n",
        "The final OOB prediction for a sample is obtained by averaging (regression) or majority voting (classification) over all trees where the sample was OOB.\n",
        "Computing the OOB Score:\n",
        "\n",
        "The OOB score is simply the accuracy (classification) or R² score (regression) calculated from OOB predictions."
      ],
      "metadata": {
        "id": "PmDS7qILtj4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train Random Forest with OOB score enabled\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "rf.fit(X, y)  # No need for a separate train-test split\n",
        "\n",
        "# Print OOB Score\n",
        "print(\"OOB Score:\", rf.oob_score_)\n"
      ],
      "metadata": {
        "id": "yhGM1WKLtuGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.How can you measure the importance of features in a Random Forest model?\n",
        "Ans.Measuring Feature Importance in a Random Forest Model\n",
        "Feature importance in Random Forest helps us understand which features have the most impact on predictions. It allows for better interpretability, feature selection, and model optimization.\n",
        "\n",
        "Methods to Measure Feature Importance in Random Forest\n",
        "Mean Decrease in Impurity (MDI) – Gini Importance\n",
        "Permutation Importance (Mean Decrease in Accuracy)\n",
        "SHAP (SHapley Additive exPlanations) Values\n",
        "1. Mean Decrease in Impurity (MDI) – Gini Importance\n",
        "Each decision tree in the Random Forest splits nodes based on the feature that reduces impurity (e.g., Gini index or entropy) the most.\n",
        "The higher the impurity reduction, the more important the feature.\n",
        "The Random Forest model averages these impurity reductions across all trees to assign importance scores."
      ],
      "metadata": {
        "id": "MYT6r-_ztv_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "feature_names = load_iris().feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get Feature Importance\n",
        "importance = rf.feature_importances_\n",
        "\n",
        "# Display in DataFrame\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "Oci8xGxmuO3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Permutation Importance (Mean Decrease in Accuracy)\n",
        "Shuffles each feature randomly and checks how much the model performance (accuracy or RMSE) drops.\n",
        "If a feature is important, shuffling it will significantly lower accuracy.\n",
        "More robust than MDI, works even for correlated features."
      ],
      "metadata": {
        "id": "YSKfAe0AuPrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Compute Permutation Importance\n",
        "perm_importance = permutation_importance(rf, X, y, n_repeats=10, random_state=42)\n",
        "\n",
        "# Display in DataFrame\n",
        "perm_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': perm_importance.importances_mean})\n",
        "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(perm_importance_df)\n"
      ],
      "metadata": {
        "id": "ikmAEIvquVK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Explain the working principle of a Bagging Classifier?\n",
        "Ans.Working Principle of a Bagging Classifier\n",
        "A Bagging Classifier (Bootstrap Aggregating) is an ensemble learning method that improves model stability and accuracy by reducing variance. It works by training multiple weak models on different subsets of data and combining their predictions.\n",
        "\n",
        "Steps in the Working of a Bagging Classifier:\n",
        "Bootstrap Sampling (Random Sampling with Replacement):\n",
        "\n",
        "The dataset is randomly sampled with replacement to create multiple bootstrap samples.\n",
        "Each bootstrap sample has the same size as the original dataset but may contain duplicate samples.\n",
        "Some original data points may not be included in a bootstrap sample.\n",
        "Train Multiple Base Models (Weak Learners):\n",
        "\n",
        "A base model (e.g., Decision Tree, SVM, etc.) is trained on each bootstrap sample independently.\n",
        "Each model learns slightly different patterns due to data variation.\n",
        "Aggregation of Predictions (Majority Voting or Averaging):\n",
        "\n",
        "For Classification:\n",
        "Each model makes a prediction.\n",
        "The majority vote (most frequent class label) is chosen as the final prediction.\n",
        "For Regression:\n",
        "The final prediction is obtained by averaging all model predictions.\n"
      ],
      "metadata": {
        "id": "FnB0O4tPuZEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base model (weak learner)\n",
        "base_model = DecisionTreeClassifier()\n",
        "\n",
        "# Create a Bagging Classifier with multiple decision trees\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "HQiX6Fo3u7Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.How do you evaluate a Bagging Classifier’s performance?\n",
        "Ans.Evaluating a Bagging Classifier’s Performance\n",
        "To assess the effectiveness of a Bagging Classifier, we use various performance metrics and validation techniques.\n",
        "\n",
        "1. Accuracy (for Classification Problems)\n",
        "Measures the percentage of correctly classified instances.\n",
        "Works best for balanced datasets."
      ],
      "metadata": {
        "id": "a8C6_fCKu83R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Compute Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "5GMG4Uc8vGpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Precision, Recall, and F1-Score (for Imbalanced Data)\n",
        "Precision: How many of the predicted positives are actually positive?\n",
        "Recall: How many actual positives were correctly predicted?\n",
        "F1-Score: Harmonic mean of Precision and Recall (useful for imbalanced datasets)."
      ],
      "metadata": {
        "id": "i5yIGgncvItp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Compute Classification Report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Grs959j9vLGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ROC-AUC Score (for Binary Classification)\n",
        "Receiver Operating Characteristic (ROC) Curve evaluates model performance at different thresholds.\n",
        "AUC (Area Under the Curve): Measures how well the model separates classes."
      ],
      "metadata": {
        "id": "VZkHqGJ1vNgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Compute AUC Score (Only for Binary Classification)\n",
        "auc_score = roc_auc_score(y_test, bagging_clf.predict_proba(X_test)[:, 1])\n",
        "print(f\"AUC Score: {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "eqXd0TMzvQjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Cross-Validation (for More Reliable Evaluation)\n",
        "Splits data into multiple train-test folds to evaluate stability.\n",
        "Reduces bias due to random train-test splits."
      ],
      "metadata": {
        "id": "gX6PsdGLvSeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-Fold Cross-Validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "Nl_fiUdnvUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.How does a Bagging Regressor work?\n",
        "Ans.Working of a Bagging Regressor\n",
        "A Bagging Regressor is an ensemble learning technique that combines multiple regression models (weak learners) to improve prediction accuracy and reduce overfitting. It follows the Bootstrap Aggregation (Bagging) principle to create an ensemble of models.\n",
        "\n",
        "Steps in the Working of a Bagging Regressor\n",
        "Bootstrap Sampling (Random Sampling with Replacement)\n",
        "\n",
        "The dataset is randomly sampled with replacement to create multiple bootstrap samples.\n",
        "Each sample has the same size as the original dataset but may contain duplicate values.\n",
        "Training Multiple Base Regressors\n",
        "\n",
        "A base regressor (e.g., Decision Tree Regressor, Linear Regression, etc.) is trained on each bootstrap sample.\n",
        "Each regressor learns a slightly different relationship from the data due to variation in training samples.\n",
        "Aggregating Predictions (Averaging Output)\n",
        "\n",
        "The final regression prediction is obtained by averaging the predictions from all base models.\n",
        "This helps in reducing variance and improving generalization.\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑓\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =\n",
        "n\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " f\n",
        "i\n",
        "​\n",
        " (X)\n",
        "where\n",
        "𝑓\n",
        "𝑖\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "f\n",
        "i\n",
        "​\n",
        " (X) is the prediction from the\n",
        "𝑖\n",
        "𝑡\n",
        "ℎ\n",
        "i\n",
        "th\n",
        "  base regressor."
      ],
      "metadata": {
        "id": "yZBy5LhWvWgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=1000, n_features=5, noise=0.2, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a base regressor (weak learner)\n",
        "base_regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Create a Bagging Regressor\n",
        "bagging_regressor = BaggingRegressor(base_estimator=base_regressor, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "xX-KCIiLvgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is the main advantage of ensemble techniques?\n",
        "Ans.Main Advantages of Ensemble Techniques\n",
        "Ensemble techniques combine multiple models to improve performance, making predictions more accurate, stable, and generalizable. Here are the key advantages:\n",
        "\n",
        "1. Higher Accuracy\n",
        "By aggregating predictions from multiple models, ensembles reduce individual model errors.\n",
        "Example: Random Forest (an ensemble of decision trees) performs better than a single decision tree."
      ],
      "metadata": {
        "id": "Fuu2hosmvkCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Train Random Forest (Ensemble)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Compare Accuracy\n",
        "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
        "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n"
      ],
      "metadata": {
        "id": "wx0avACnvwBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Reduces Overfitting\n",
        "Ensembles prevent overfitting by averaging multiple models.\n",
        "Bagging methods (like Random Forest) reduce variance by training on different subsets of data.\n",
        "3. Works Well with Noisy Data\n",
        "Boosting methods (like AdaBoost, Gradient Boosting) focus on hard-to-predict samples, making the model more robust to noise.\n",
        "4. Improves Generalization (Better Performance on Unseen Data)\n",
        "Since ensemble models learn diverse patterns, they generalize better than single models.\n",
        "This is useful for complex real-world datasets where a single model may fail.\n",
        "5. Handles Bias-Variance Tradeoff Efficiently\n",
        "Bagging reduces variance (useful for high-variance models like Decision Trees).\n",
        "Boosting reduces bias (useful for underfitting models like Logistic Regression)."
      ],
      "metadata": {
        "id": "SQHlK1L-vypW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.What is the main challenge of ensemble methods?\n",
        "Ans.Main Challenges of Ensemble Methods\n",
        "While ensemble methods improve accuracy and generalization, they also come with several challenges:\n",
        "\n",
        "1. Increased Computational Cost\n",
        "Training multiple models requires more time and computational power.\n",
        "Example: A Random Forest with 100 trees takes longer to train than a single Decision Tree.\n",
        "Solution: Use parallel processing and optimized libraries like scikit-learn, XGBoost for faster training.\n",
        "\n",
        "2. Complexity in Interpretation\n",
        "Individual models like Decision Trees are easy to interpret, but ensembles (e.g., Random Forest, Gradient Boosting) act as black boxes.\n",
        "It is harder to explain why a model made a specific prediction.\n",
        "Solution: Use feature importance (feature_importances_ in Random Forest) or tools like SHAP (SHapley Additive Explanations) for interpretability."
      ],
      "metadata": {
        "id": "NTJivrvOv3D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "importances = rf.feature_importances_\n",
        "plt.bar(range(len(importances)), importances)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.title(\"Feature Importance in Random Forest\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SGOs8OfgwHjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Risk of Overfitting in Boosting\n",
        "Boosting methods (AdaBoost, Gradient Boosting, XGBoost) can overfit if trained with too many estimators.\n",
        "Unlike Bagging (which reduces variance), Boosting focuses on hard-to-learn samples, sometimes learning noise instead of patterns.\n",
        "Solution: Use early stopping to prevent overfitting."
      ],
      "metadata": {
        "id": "ycbQpkj6wLWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost with early stopping\n",
        "xgb = XGBClassifier(n_estimators=1000, early_stopping_rounds=50, eval_metric=\"logloss\", eval_set=[(X_test, y_test)], verbose=False)\n",
        "xgb.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "LMRcC58vwNsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Requires More Data for Best Performance\n",
        "Ensemble methods work best with large datasets.\n",
        "Small datasets may not benefit much, and simple models like Logistic Regression might perform just as well.\n",
        "Solution: If data is limited, use cross-validation or simpler models like Bagging instead of Boosting.\n",
        "\n",
        "5. Harder to Tune Hyperparameters\n",
        "Ensembles have more hyperparameters than single models.\n",
        "Example: Random Forest requires tuning n_estimators, max_depth, min_samples_split, etc.\n",
        "Example: Gradient Boosting has learning_rate, n_estimators, max_depth, etc.\n",
        "Solution: Use Grid Search or Random Search for tuning."
      ],
      "metadata": {
        "id": "Rd7xhihAwOa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n"
      ],
      "metadata": {
        "id": "J2XbBVlTwSLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.Explain the key idea behind ensemble techniques?\n",
        "Ans.Key Idea Behind Ensemble Techniques\n",
        "Ensemble techniques combine multiple models to improve overall performance. Instead of relying on a single model, ensemble methods aggregate predictions from multiple models to reduce errors, increase accuracy, and improve generalization.\n",
        "\n",
        "Key Principles of Ensemble Learning\n",
        "Diversity of Models\n",
        "\n",
        "Different models make different types of errors. Combining diverse models ensures that errors cancel out, improving overall accuracy.\n",
        "Example: Using Decision Trees, Support Vector Machines (SVMs), and Neural Networks together in an ensemble.\n",
        "Reducing Variance (Bagging)\n",
        "\n",
        "Bagging (Bootstrap Aggregating) reduces variance by training multiple models on different random subsets of data and averaging their predictions.\n",
        "Example: Random Forest is a bagging technique that combines multiple Decision Trees.\n",
        "Reducing Bias (Boosting)\n",
        "\n",
        "Boosting reduces bias by training models sequentially, where each new model focuses on correcting the errors made by the previous model.\n",
        "Example: Gradient Boosting, AdaBoost, and XGBoost.\n",
        "Combining Weak Learners to Form a Strong Learner\n",
        "\n",
        "A weak learner is a model that performs slightly better than random guessing.\n",
        "Combining multiple weak learners results in a highly accurate model.\n",
        "Example: Decision Stumps (one-level Decision Trees) in AdaBoost.\n",
        "Aggregating Predictions for Stability\n",
        "\n",
        "Different ensemble methods aggregate predictions differently:\n",
        "Averaging (Regression Models) → Bagging, Random Forest.\n",
        "Majority Voting (Classification Models) → Voting Classifier.\n",
        "Weighted Sum (Boosting Models) → AdaBoost, Gradient Boosting.\n"
      ],
      "metadata": {
        "id": "aK2Me9M2wU1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest (Bagging)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "vYpdXTOkweB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Base model (weak learner)\n",
        "base_model = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Train an AdaBoost model\n",
        "ada = AdaBoostClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ada.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "H13Pvgd0whQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What is a Random Forest Classifier?\n",
        "Ans.Random Forest Classifier\n",
        "A Random Forest Classifier is an ensemble learning method that combines multiple Decision Trees to improve accuracy, reduce overfitting, and enhance generalization. It is a bagging-based technique, meaning it trains multiple models on different subsets of the data and aggregates their predictions.\n",
        "\n",
        "Key Features of Random Forest Classifier\n",
        "Ensemble of Decision Trees\n",
        "\n",
        "Random Forest consists of multiple Decision Trees, each trained on a different subset of data.\n",
        "The final prediction is made using majority voting (for classification) or averaging (for regression).\n",
        "Feature Randomness (Random Subspace Method)\n",
        "\n",
        "Each tree is trained on a random subset of features, preventing trees from being too similar.\n",
        "This increases diversity among models and improves performance.\n",
        "Handles Overfitting\n",
        "\n",
        "Unlike a single Decision Tree, which can overfit, Random Forest reduces overfitting by averaging multiple trees.\n",
        "Works Well with Missing Data\n",
        "\n",
        "Can handle missing values effectively by averaging predictions from multiple trees.\n",
        "Can Handle Large Datasets\n",
        "\n",
        "Suitable for high-dimensional data with many features and samples.\n",
        "How Random Forest Works\n",
        "Bootstrap Sampling (Bagging)\n",
        "\n",
        "Randomly select multiple subsets of the training data with replacement.\n",
        "Each subset is used to train a separate Decision Tree.\n",
        "Random Feature Selection\n",
        "\n",
        "Each tree is trained on a random subset of features instead of using all features.\n",
        "Helps create diverse trees and reduces correlation among them.\n",
        "Decision Tree Training\n",
        "\n",
        "Each tree is trained independently on its subset of data.\n",
        "The trees are not pruned, meaning they grow fully.\n",
        "Majority Voting (Classification) or Averaging (Regression)\n",
        "\n",
        "For classification, each tree votes for a class, and the most common class is chosen.\n",
        "For regression, the final prediction is the average of all trees."
      ],
      "metadata": {
        "id": "l_W4lt6ownAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "IWkia6KRxE7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What are the main types of ensemble techniques?\n",
        "Ans.Main Types of Ensemble Techniques\n",
        "Ensemble techniques combine multiple models to improve accuracy, reduce variance, and enhance generalization. The main types of ensemble methods are:\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "Key Idea:\n",
        "\n",
        "Reduces variance by training multiple models on different random subsets of the dataset.\n",
        "Each model is trained independently in parallel, and predictions are combined (averaging for regression, majority voting for classification).\n",
        "Example Algorithms:\n",
        "\n",
        "Random Forest (combines multiple Decision Trees).\n",
        "Bagging Classifier (wrapper for any base model)."
      ],
      "metadata": {
        "id": "EAb1SB9UxNoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model (Bagging)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "IZzXwOkTxZH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Boosting\n",
        "Key Idea:\n",
        "\n",
        "Reduces bias by sequentially training weak models, where each model corrects errors of the previous one.\n",
        "Models are trained sequentially, unlike bagging, which trains them in parallel.\n",
        "Example Algorithms:\n",
        "\n",
        "AdaBoost (Adaptive Boosting)\n",
        "Gradient Boosting (GBM, XGBoost, LightGBM, CatBoost)"
      ],
      "metadata": {
        "id": "2lapZ2IDxcu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Base model (weak learner)\n",
        "base_model = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Train an AdaBoost model\n",
        "ada = AdaBoostClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ada.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "fC9kGUy3xhl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stacking (Stacked Generalization)\n",
        "Key Idea:\n",
        "\n",
        "Combines predictions from multiple base models using a meta-model (final model) to make the final prediction.\n",
        "Each base model makes predictions, and these predictions are used as input features for the meta-model.\n",
        "Example Algorithms:\n",
        "\n",
        "Stacking Classifier (combining SVM, Decision Trees, Neural Networks, etc.)\n",
        "Stacking Regressor (for regression tasks)"
      ],
      "metadata": {
        "id": "RhzsUpwAxpmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Define base models\n",
        "base_models = [\n",
        "    ('svm', SVC(probability=True)),\n",
        "    ('tree', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "# Meta-model\n",
        "meta_model = LogisticRegression()\n",
        "\n",
        "# Stacking Classifier\n",
        "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
        "\n",
        "# Train and evaluate\n",
        "stack.fit(X_train, y_train)\n",
        "y_pred = stack.predict(X_test)\n",
        "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "S-BzKVxixuPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.What is ensemble learning in machine learning?\n",
        "Ans.Ensemble Learning in Machine Learning\n",
        "Definition\n",
        "Ensemble learning is a technique in machine learning where multiple models (weak learners) are combined to produce a stronger and more accurate predictive model. The goal is to improve performance by reducing errors, increasing accuracy, and enhancing generalization.\n",
        "\n",
        "Why Use Ensemble Learning?\n",
        "Reduces Overfitting (Variance Reduction) – Combining multiple models prevents over-reliance on a single model, making the final prediction more stable.\n",
        "Increases Accuracy – Multiple weak models working together perform better than a single strong model.\n",
        "Improves Generalization – Works well on unseen data by reducing bias and variance.\n",
        "Handles Noisy Data – Since multiple models contribute, the impact of noise is minimized.\n",
        "Types of Ensemble Learning Techniques\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "Reduces variance by training multiple models on random subsets of data (with replacement).\n",
        "Models are trained independently in parallel, and final predictions are combined (averaging for regression, majority voting for classification).\n",
        "Example Algorithm: Random Forest\n",
        "2. Boosting\n",
        "Reduces bias by training models sequentially, where each new model focuses on correcting the errors of the previous one.\n",
        "Example Algorithms: AdaBoost, Gradient Boosting, XGBoost\n",
        "3. Stacking (Stacked Generalization)\n",
        "Uses multiple base models and a meta-model to make final predictions.\n",
        "Unlike bagging and boosting, stacking learns how to combine models optimally.\n",
        "Example Algorithm: Stacking Classifier\n",
        "4. Voting Ensemble\n",
        "Combines multiple models and selects the final output using:\n",
        "Hard Voting – Chooses the most common class label.\n",
        "Soft Voting – Uses probability-weighted predictions.\n",
        "Example Algorithm: Voting Classifier"
      ],
      "metadata": {
        "id": "Vab7LZXLyCZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest (Bagging)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "5etXEjw-yKa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Base model (weak learner)\n",
        "base_model = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# Train AdaBoost model\n",
        "ada = AdaBoostClassifier(base_estimator=base_model, n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ada.predict(X_test)\n",
        "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "rsh2mwcXyPww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.When should we avoid using ensemble methods?\n",
        "Ans.When to Avoid Using Ensemble Methods\n",
        "While ensemble methods improve accuracy and generalization, they are not always the best choice. Here are situations where you should avoid using them:\n",
        "\n",
        "1. When a Single Model is Sufficient\n",
        "If a simple model (like Logistic Regression or a single Decision Tree) provides good accuracy, using an ensemble is unnecessary.\n",
        "Example: If a dataset is linearly separable, Logistic Regression or SVM may perform well without needing ensembles.\n",
        "2. When Interpretability is Important\n",
        "Ensemble models like Random Forest, XGBoost, and Stacking are complex and hard to interpret.\n",
        "Example: In healthcare or finance, where model decisions impact real lives, decision trees or linear models may be preferred for transparency.\n",
        "3. When Computational Resources are Limited\n",
        "Ensemble models (especially Boosting and Stacking) require high memory and CPU/GPU power.\n",
        "Example: If running on edge devices (like IoT sensors or mobile phones), simpler models are more efficient.\n",
        "4. When the Training Data is Small\n",
        "If the dataset is too small, ensemble methods can overfit instead of improving performance.\n",
        "Example: Training a Random Forest with 100 trees on a dataset with only 100 samples may lead to overfitting.\n",
        "5. When Prediction Speed is Crucial\n",
        "Some ensemble methods (like Boosting and Stacking) have slow inference times because they need to evaluate multiple models.\n",
        "Example: Real-time applications like autonomous driving or fraud detection require fast predictions, making ensembles less suitable.\n",
        "6. When the Problem is Not Complex\n",
        "If a dataset is well-structured with clear patterns, a single model like SVM, Logistic Regression, or Decision Tree may work just as well.\n",
        "Example: A spam detection system with simple word frequency counts may perform well with Naïve Bayes instead of an ensemble.\n",
        "7. When Ensemble Performance Gains Are Minimal\n",
        "If ensemble learning improves accuracy by only 1-2%, but increases training time significantly, it may not be worth using.\n",
        "Example: If a Decision Tree gives 94% accuracy and Random Forest gives 95%, the improvement may not justify the extra complexity."
      ],
      "metadata": {
        "id": "-sDm17TYyV9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.How does Bagging help in reducing overfitting?\n",
        "Ans.Bagging (Bootstrap Aggregating) is an ensemble learning technique that helps reduce overfitting by decreasing model variance. It works by training multiple instances of the same model on different random subsets of the dataset and then aggregating their predictions.\n",
        "\n",
        "How Bagging Reduces Overfitting?\n",
        "1. Introduces Data Variability (Bootstrap Sampling)\n",
        "Bagging creates multiple training datasets by randomly sampling data with replacement.\n",
        "Each model trains on a slightly different dataset, reducing dependence on any particular sample.\n",
        "This prevents overfitting to the noise in the original data.\n",
        "2. Reduces Model Variance\n",
        "Overfitting happens when a model is too sensitive to minor variations in data.\n",
        "Since bagging averages predictions from multiple models, it smooths out extreme variations and reduces overfitting.\n",
        "3. Reduces the Impact of Outliers\n",
        "If a dataset has outliers, a single model (like a Decision Tree) may get misled by them.\n",
        "In Bagging, since each model sees only a subset of the data, outliers affect only a few models, preventing overfitting.\n",
        "4. Creates Independent Decision Boundaries\n",
        "Overfitting occurs when a model memorizes training data instead of generalizing.\n",
        "Bagging ensures that each model learns slightly different decision boundaries, leading to a more generalized ensemble model.\n",
        "5. Aggregates Predictions to Improve Stability\n",
        "Bagging averages predictions (for regression) or uses majority voting (for classification).\n",
        "This aggregation ensures that individual models’ errors do not dominate, leading to a more stable and robust model.\n"
      ],
      "metadata": {
        "id": "VtOV6bGxyd9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model (Bagging technique)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "udBQNQYayzRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Why is Random Forest better than a single Decision Tree?\n",
        "Ans.Why is Random Forest Better Than a Single Decision Tree?\n",
        "Random Forest is an ensemble method that combines multiple Decision Trees to improve accuracy, generalization, and robustness. Here’s why Random Forest outperforms a single Decision Tree:\n",
        "\n",
        "1. Reduces Overfitting (Lower Variance)\n",
        "A single Decision Tree tends to memorize the training data, leading to overfitting and poor generalization on unseen data.\n",
        "Random Forest reduces overfitting by training multiple trees on different random subsets of data (Bagging) and averaging their predictions, leading to a more generalized model.\n",
        "2. More Stable and Robust\n",
        "A single Decision Tree is highly sensitive to small changes in the dataset. If you change the training data slightly, the tree structure may change completely.\n",
        "Random Forest, by combining multiple trees, is much more stable and resistant to data variations.\n",
        "3. Handles Noisy Data Better\n",
        "A single Decision Tree can be easily misled by noise in the training data, making incorrect splits.\n",
        "Random Forest, by aggregating multiple trees, reduces the impact of noise, making predictions more reliable.\n",
        "4. Works Well with High-Dimensional Data\n",
        "In datasets with many features, a single Decision Tree may struggle to find the best splits and may overfit.\n",
        "Random Forest uses Feature Randomness, selecting a random subset of features for each tree, ensuring better feature utilization and improved performance.\n",
        "5. Handles Missing Values Automatically\n",
        "Decision Trees may struggle with missing values, leading to biased splits.\n",
        "Random Forest can handle missing values by using feature selection across multiple trees.\n",
        "6. Reduces Correlation Between Trees\n",
        "A single Decision Tree relies entirely on the structure of one model.\n",
        "Random Forest ensures trees are decorrelated by training each one on a random subset of data and features, making it more independent and diverse.\n",
        "7. More Accurate Predictions\n",
        "Random Forest aggregates predictions from multiple trees, using:\n",
        "Majority voting (for classification)\n",
        "Averaging (for regression)\n",
        "This ensemble approach leads to higher accuracy compared to a single Decision Tree."
      ],
      "metadata": {
        "id": "qX4gEmNmy1sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "f9zA5M-qzE7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.What is the role of bootstrap sampling in Bagging?\n",
        "Ans.1. Reduces Overfitting (Variance Reduction)\n",
        "A single model (e.g., Decision Tree) tends to memorize data, leading to overfitting.\n",
        "By training multiple models on different sampled datasets, Bagging prevents overfitting and improves generalization.\n",
        "2. Increases Model Stability\n",
        "Since each model is trained on a different dataset, they make different errors.\n",
        "Combining predictions reduces the impact of outliers and noise.\n",
        "3. Introduces Diversity in Models\n",
        "The randomness introduced by bootstrap sampling ensures that no two models are identical.\n",
        "This decorrelation between models makes the final ensemble more effective.\n",
        "4. Works Well with High Variance Models\n",
        "Bagging is particularly effective with high-variance models like Decision Trees, as it smooths their predictions.\n",
        "Example: Random Forest uses Bagging to train multiple Decision Trees, leading to better performance."
      ],
      "metadata": {
        "id": "EhOlX6QzzNLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"
      ],
      "metadata": {
        "id": "BtpdDG26zmBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.What are some real-world applications of ensemble techniques?\n",
        "Ans.eal-World Applications of Ensemble Techniques\n",
        "Ensemble techniques like Bagging, Boosting, and Stacking are widely used in various industries to improve predictive accuracy, reduce overfitting, and enhance model stability. Here are some key real-world applications:\n",
        "\n",
        "1. Finance and Banking\n",
        "✔ Fraud Detection:\n",
        "\n",
        "Problem: Detecting fraudulent transactions in real-time.\n",
        "Solution: Random Forest & XGBoost analyze transaction patterns to distinguish fraud from legitimate transactions.\n",
        "✔ Credit Risk Assessment:\n",
        "\n",
        "Problem: Predicting whether a loan applicant will default.\n",
        "Solution: Gradient Boosting Models (GBM) & AdaBoost improve classification accuracy in predicting high-risk customers.\n",
        "2. Healthcare and Medical Diagnosis\n",
        "✔ Disease Prediction and Diagnosis:\n",
        "\n",
        "Problem: Predicting diseases like diabetes, cancer, and heart disease.\n",
        "Solution: Bagging and Boosting combine models (e.g., Decision Trees, SVM) for better diagnostic accuracy.\n",
        "✔ Medical Image Analysis:\n",
        "\n",
        "Problem: Detecting tumors or anomalies in MRI/X-ray images.\n",
        "Solution: Convolutional Neural Networks (CNNs) + Ensemble Learning improve image classification performance.\n",
        "3. E-commerce and Retail\n",
        "✔ Recommendation Systems:\n",
        "\n",
        "Problem: Predicting user preferences for personalized recommendations.\n",
        "Solution: Stacking & Random Forest combine multiple models (e.g., collaborative filtering, content-based models) to improve recommendations.\n",
        "✔ Customer Churn Prediction:\n",
        "\n",
        "Problem: Identifying customers likely to stop using a service.\n",
        "Solution: XGBoost & LightGBM analyze customer behavior to predict churn and improve retention strategies.\n",
        "4. Autonomous Vehicles and Transportation\n",
        "✔ Self-Driving Cars:\n",
        "\n",
        "Problem: Identifying objects, pedestrians, and traffic signals in real-time.\n",
        "Solution: Bagging & Boosting with Deep Learning Models improve accuracy in object detection.\n",
        "✔ Traffic Flow Prediction:\n",
        "\n",
        "Problem: Predicting congestion and suggesting optimal routes.\n",
        "Solution: Gradient Boosting & Random Forest analyze traffic patterns for better predictions.\n",
        "5. Cybersecurity\n",
        "✔ Intrusion Detection Systems (IDS):\n",
        "\n",
        "Problem: Detecting malware and network intrusions.\n",
        "Solution: Ensemble techniques like Random Forest & XGBoost analyze system logs and network patterns for anomaly detection.\n",
        "✔ Spam Email Filtering:\n",
        "\n",
        "Problem: Identifying and filtering spam emails.\n",
        "Solution: Bagging (Random Forest) & Boosting (AdaBoost) improve email classification accuracy.\n",
        "6. Social Media and Sentiment Analysis\n",
        "✔ Fake News Detection:\n",
        "\n",
        "Problem: Identifying misinformation in news articles.\n",
        "Solution: Boosting algorithms (XGBoost, CatBoost) improve text classification accuracy.\n",
        "✔ Sentiment Analysis:\n",
        "\n",
        "Problem: Understanding public opinion on social media.\n",
        "Solution: Ensemble learning improves the classification of positive, negative, or neutral sentiments.\n",
        "7. Weather Forecasting and Climate Modeling\n",
        "✔ Predicting Natural Disasters:\n",
        "\n",
        "Problem: Forecasting hurricanes, earthquakes, or floods.\n",
        "Solution: Bagging with Random Forest & Boosting models process historical climate data to predict weather patterns.\n",
        "✔ Air Quality Monitoring:\n",
        "\n",
        "Problem: Predicting pollution levels in different cities.\n",
        "Solution: Random Forest & Gradient Boosting analyze pollutant levels and weather conditions for forecasting."
      ],
      "metadata": {
        "id": "Fz4RZw3uztQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.What is the difference between Bagging and Boosting?\n",
        "Ans.Difference Between Bagging and Boosting\n",
        "Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they work in different ways to improve model performance. Here’s a detailed comparison:\n",
        "\n",
        "1. Definition\n",
        "Bagging: Trains multiple models in parallel on different random subsets of data (Bootstrap Sampling) and aggregates their predictions (e.g., majority voting for classification, averaging for regression).\n",
        "Boosting: Trains multiple models sequentially, where each model corrects the errors of the previous one, focusing more on misclassified samples.\n",
        "2. Working Principle\n",
        "Bagging (Parallel Training & Aggregation)\n",
        "✔ Uses random sampling with replacement (bootstrap sampling) to create multiple training datasets.\n",
        "✔ Trains independent base models (e.g., Decision Trees) in parallel.\n",
        "✔ Aggregates predictions using majority voting (classification) or averaging (regression).\n",
        "✔ Reduces variance and prevents overfitting.\n",
        "\n",
        "Boosting (Sequential Training & Weighted Learning)\n",
        "✔ Models are trained sequentially, where each new model focuses on correcting the mistakes of the previous one.\n",
        "✔ Assigns higher weights to misclassified instances, making the model learn from hard-to-classify cases.\n",
        "✔ The final prediction is made by weighted averaging or boosted voting.\n",
        "✔ Reduces bias and improves accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "FfV8ZYDW0VLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier\n",
        "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred_bag))\n"
      ],
      "metadata": {
        "id": "DRRySqfQ0sBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?\n",
        "Ans.Here is a Python implementation to train a Bagging Classifier using Decision Trees on a sample dataset and print the model accuracy.\n",
        "\n",
        "Step 1: Load the Dataset\n",
        "We'll use the Iris dataset, a commonly used dataset for classification problems.\n",
        "\n",
        "Step 2: Train the Bagging Classifier\n",
        "We'll use Decision Trees as the base estimator in the Bagging ensemble.\n",
        "\n",
        "Step 3: Evaluate the Model\n",
        "We'll calculate and print the accuracy score of the trained model on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "qcfAqsxM2U7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Decision Trees as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=100,  # Number of trees\n",
        "                                bootstrap=True,    # Enable bootstrap sampling\n",
        "                                random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "OVqE-4YJ2nMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)?\n",
        "Ans.Here’s a Python implementation to train a Bagging Regressor using Decision Trees and evaluate it using Mean Squared Error (MSE).\n",
        "\n",
        "Steps to Implement\n",
        "Load Dataset: We'll use the Boston Housing dataset (or any regression dataset).\n",
        "Train the Bagging Regressor: Using Decision Trees as base estimators.\n",
        "Evaluate the Model: Using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "YaNan9ef2pxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a sample regression dataset (California Housing)\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                               n_estimators=100,  # Number of trees\n",
        "                               bootstrap=True,    # Enable bootstrap sampling\n",
        "                               random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "syYtrxoQ27N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores?\n",
        "Ans.Here’s a Python implementation to train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "\n",
        "Steps to Implement\n",
        "Load Dataset: Use the Breast Cancer dataset from sklearn.datasets.\n",
        "Train the Random Forest Classifier.\n",
        "Evaluate Feature Importance: Print feature importance scores."
      ],
      "metadata": {
        "id": "IfWBD8eV29bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Get feature names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)  # Sort by importance\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"\\nFeature Importance Scores:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "nhu2_baY3Y2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24Train a Random Forest Regressor and compare its performance with a single Decision Tree?\n",
        "Ans.Here’s a Python implementation to train a Random Forest Regressor and compare its performance with a single Decision Tree Regressor using Mean Squared Error (MSE).\n",
        "\n",
        "Steps to Implement\n",
        "Load Dataset: Use the California Housing dataset (or any regression dataset).\n",
        "Train Models: Train both Decision Tree Regressor and Random Forest Regressor.\n",
        "Evaluate Performance: Compare MSE of both models.\n"
      ],
      "metadata": {
        "id": "pXfj57nI3e_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for both models\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print MSE values\n",
        "print(f\"Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "UEbxEaoc3vkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier?\n",
        "Ans.Computing the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "The Out-of-Bag (OOB) score is an internal validation score used in Random Forest models. Since bootstrap sampling (random sampling with replacement) is used for training, some data points remain out-of-bag and can be used to estimate model accuracy without needing a separate validation set.\n",
        "\n",
        "Steps to Implement\n",
        "Load Dataset: Use the Breast Cancer dataset from sklearn.datasets.\n",
        "Train a Random Forest Classifier with oob_score=True.\n",
        "Print the OOB Score to estimate model performance."
      ],
      "metadata": {
        "id": "NFK0J8p43yUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier with OOB Score enabled\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42, bootstrap=True)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB Score\n",
        "print(f\"Out-of-Bag (OOB) Score: {rf_clf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "L8B7GJ0q3-ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26. Train a Bagging Classifier using SVM as a base estimator and print accuracy?\n",
        "Ans."
      ],
      "metadata": {
        "id": "TCSZiuBU4D0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=SVC(),\n",
        "                                n_estimators=50,\n",
        "                                bootstrap=True,\n",
        "                                random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier (SVM) Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "oCSUvGWp5Gh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27. Train a Random Forest Classifier with different numbers of trees and compare accuracy?\n",
        "Ans.Steps to Implement\n",
        "Load Dataset: Use the Breast Cancer dataset.\n",
        "Train Multiple Random Forest Models with different n_estimators (number of trees).\n",
        "Compare Accuracy for different numbers of trees.\n"
      ],
      "metadata": {
        "id": "2hDcYDGc5Hel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different numbers of trees to test\n",
        "tree_counts = [1, 5, 10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "# Train and evaluate models with different numbers of trees\n",
        "for n in tree_counts:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Random Forest with {n} trees - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(tree_counts, accuracies, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Trees\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Random Forest Accuracy vs. Number of Trees\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u9SwQxQb5ZMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score?\n",
        "Ans.Steps to Implement\n",
        "Load Dataset: Use the Breast Cancer dataset.\n",
        "Train a Bagging Classifier with Logistic Regression as the base estimator.\n",
        "Evaluate Performance using AUC score."
      ],
      "metadata": {
        "id": "OtUVkpgd5gqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with Logistic Regression as the base estimator\n",
        "bagging_lr = BaggingClassifier(base_estimator=LogisticRegression(max_iter=5000),\n",
        "                               n_estimators=50,\n",
        "                               bootstrap=True,\n",
        "                               random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on test data\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]  # Get probability of class 1\n",
        "\n",
        "# Calculate and print AUC Score\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Bagging Classifier (Logistic Regression) AUC Score: {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "4Mqu6WqL5wFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.Train a Random Forest Regressor and analyze feature importance scores?\n",
        "Ans.Steps to Implement\n",
        "Load Dataset: Use the California Housing dataset for regression.\n",
        "Train a Random Forest Regressor.\n",
        "Extract Feature Importance Scores and visualize them.\n"
      ],
      "metadata": {
        "id": "Nz-0n7wq51Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Get feature names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Random Forest Regressor MSE: {mse:.4f}\")\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = rf_reg.feature_importances_\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print feature importance scores\n",
        "print(\"\\nFeature Importance Scores:\")\n",
        "print(importance_df)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance in Random Forest Regressor\")\n",
        "plt.gca().invert_yaxis()  # Highest importance on top\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dRMrrtLl5_jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "Ans.Steps to Implement\n",
        "Load Dataset: Use the Breast Cancer dataset for classification.\n",
        "Train Two Ensemble Models:\n",
        "Bagging Classifier with Decision Trees.\n",
        "Random Forest Classifier.\n",
        "Evaluate Accuracy on the test set.\n",
        "Compare Performance of both models."
      ],
      "metadata": {
        "id": "W1nNOqpq6B0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50,\n",
        "                                bootstrap=True,\n",
        "                                random_state=42)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "zpcMD_tR6PoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV?\n",
        "Ans."
      ],
      "metadata": {
        "id": "dmqilR2U6STF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a base Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees\n",
        "    'max_depth': [None, 10, 20],  # Depth of trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
        "    'criterion': ['gini', 'entropy']  # Splitting criterion\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Optimized Random Forest Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "jl30VCHN6mOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q32.Train a Bagging Regressor with different numbers of base estimators and compare performance?\n",
        "Ans.Steps to Implement\n",
        "Load Dataset: Use the California Housing dataset for regression.\n",
        "Train Bagging Regressors with different numbers of base estimators (n_estimators).\n",
        "Compare Performance using Mean Squared Error (MSE).\n",
        "Visualize the results."
      ],
      "metadata": {
        "id": "8RLc-lkx6o3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different numbers of base estimators to test\n",
        "n_estimators_list = [1, 5, 10, 50, 100, 200]\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate Bagging Regressors with different numbers of base estimators\n",
        "for n in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                   n_estimators=n,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "    # Calculate MSE\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "    print(f\"Bagging Regressor with {n} estimators - MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot MSE vs. Number of Base Estimators\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(n_estimators_list, mse_scores, marker='o', linestyle='-')\n",
        "plt.xlabel(\"Number of Base Estimators\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Bagging Regressor Performance vs. Number of Estimators\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sxltJ7KW64lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q33.Train a Random Forest Classifier and analyze misclassified samples?\n",
        "Ans."
      ],
      "metadata": {
        "id": "bvt7NhoB6_Yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Feature names\n",
        "target_names = data.target_names  # Class labels\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "print(f\"\\nNumber of Misclassified Samples: {len(misclassified_indices)}\")\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_df = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "misclassified_df['Actual Label'] = [target_names[label] for label in y_test[misclassified_indices]]\n",
        "misclassified_df['Predicted Label'] = [target_names[label] for label in y_pred[misclassified_indices]]\n",
        "\n",
        "print(\"\\nMisclassified Samples Analysis:\")\n",
        "print(misclassified_df)\n",
        "\n",
        "# Display Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "6jWR1OQ_7Jiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier?\n",
        "Ans."
      ],
      "metadata": {
        "id": "IMkQ4NMc7KaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Single Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Print accuracy scores\n",
        "print(f\"Decision Tree Classifier Accuracy: {accuracy_dt:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n"
      ],
      "metadata": {
        "id": "orJVraMG7YQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q35.Train a Random Forest Classifier and visualize the confusion matrix\n",
        "Ans."
      ],
      "metadata": {
        "id": "RCIHr82o7ZZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"Actual Label\")\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jpRW7EVH7jyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy?\n",
        "Ans."
      ],
      "metadata": {
        "id": "cBoH-kAL7kuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base classifiers\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)  # Enable probability estimates for stacking\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "\n",
        "# Define stacking classifier with logistic regression as the meta-learner\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('dt', decision_tree), ('svm', svm_clf), ('lr', log_reg)],\n",
        "    final_estimator=LogisticRegression(),\n",
        "    passthrough=False  # Use base model predictions for final model\n",
        ")\n",
        "\n",
        "# Train individual models and Stacking Classifier\n",
        "classifiers = {'Decision Tree': decision_tree, 'SVM': svm_clf, 'Logistic Regression': log_reg, 'Stacking Classifier': stacking_clf}\n",
        "accuracy_scores = {}\n",
        "\n",
        "for name, model in classifiers.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores[name] = accuracy\n",
        "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compare accuracy of different models\n",
        "best_model = max(accuracy_scores, key=accuracy_scores.get)\n",
        "print(f\"\\nBest Performing Model: {best_model} with Accuracy: {accuracy_scores[best_model]:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZJS5vppV7wV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q37.Train a Random Forest Classifier and print the top 5 most important features\n",
        "Ans."
      ],
      "metadata": {
        "id": "kd_fabw17xu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names  # Feature names\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Extract feature importance scores\n",
        "feature_importance = rf_clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features in Random Forest Classifier:\")\n",
        "print(importance_df.head(5))\n"
      ],
      "metadata": {
        "id": "gZQ1nOYW79Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "Ans."
      ],
      "metadata": {
        "id": "38LrhXl57-N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Bagging Classifier Performance:\")\n",
        "print(f\"Accuracy  : {accuracy:.4f}\")\n",
        "print(f\"Precision : {precision:.4f}\")\n",
        "print(f\"Recall    : {recall:.4f}\")\n",
        "print(f\"F1-score  : {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "wnKUfsl98EjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "jQLJ7SXI8JVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define max_depth values to test\n",
        "max_depth_values = range(1, 21)\n",
        "accuracy_scores = []\n",
        "\n",
        "# Train and evaluate Random Forest with different max_depth values\n",
        "for depth in max_depth_values:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plot max_depth vs. accuracy\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(max_depth_values, accuracy_scores, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel(\"Max Depth of Trees\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of max_depth on Random Forest Accuracy\")\n",
        "plt.grid\n"
      ],
      "metadata": {
        "id": "ral1ziF18QBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance\n",
        "Ans."
      ],
      "metadata": {
        "id": "SubKTl8Q8Q0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing dataset\n",
        "data = load_boston()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define base estimators\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "knn_regressor = KNeighborsRegressor()\n",
        "\n",
        "# Train Bagging Regressors with different base estimators\n",
        "bagging_dt = BaggingRegressor(base_estimator=dt_regressor, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "bagging_knn = BaggingRegressor(base_estimator=knn_regressor, n_estimators=50, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Fit models\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error (MSE)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "# Print MSE scores\n",
        "print(f\"Bagging Regressor (Decision Tree) - MSE: {mse_dt:.4f}\")\n",
        "print(f\"Bagging Regressor (K-Neighbors) - MSE: {mse_knn:.4f}\")\n",
        "\n",
        "# Plot comparison\n",
        "models = ['Bagging (Decision Tree)', 'Bagging (K-Neighbors)']\n",
        "mse_scores = [mse_dt, mse_knn]\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(models, mse_scores, color=['blue', 'green'])\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.title(\"Comparison of Bagging Regressors with Different Base Estimators\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vjidL5YU8blj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score?\n",
        "Ans."
      ],
      "metadata": {
        "id": "qH0T_oED7xqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Print ROC-AUC score\n",
        "print(f\"Random Forest Classifier ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')  # Random classifier line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R6FofFzA8nCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q42.Train a Bagging Classifier and evaluate its performance using cross-validatio.\n",
        "Ans."
      ],
      "metadata": {
        "id": "bYOnuRU08n8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define Bagging Classifier with Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
        "                                n_estimators=50,\n",
        "                                random_state=42,\n",
        "                                n_jobs=-1)\n",
        "\n",
        "# Perform cross-validation with Stratified K-Fold (5 folds)\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "# Print Cross-Validation Results\n",
        "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
        "print(f\"Mean Accuracy: {np.mean(cv_scores):.4f}\")\n",
        "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "c29D_Kc58yig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q43.Train a Random Forest Classifier and plot the Precision-Recall curv?\n",
        "Ans."
      ],
      "metadata": {
        "id": "s-74ZwnH8zcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = rf_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Compute PR AUC Score\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Print PR AUC Score\n",
        "print(f\"Random Forest Classifier PR AUC Score: {pr_auc:.4f}\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(recall, precision, color='blue', label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve for Random Forest Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zRysjsIp891R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy?\n",
        "Ans."
      ],
      "metadata": {
        "id": "-Z5kLDAT9B1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Base Learners\n",
        "base_learners = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=500))\n",
        "]\n",
        "\n",
        "# Define Stacking Classifier with Logistic Regression as Meta-Learner\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression(), n_jobs=-1)\n",
        "\n",
        "# Train Stacking Classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred_stacking = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, y_pred_stacking)\n",
        "\n",
        "# Train Individual Models for Comparison\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_accuracy = accuracy_score(y_test, rf_clf.predict(X_test))\n",
        "\n",
        "lr_clf = LogisticRegression(max_iter=500)\n",
        "lr_clf.fit(X_train, y_train)\n",
        "lr_accuracy = accuracy_score(y_test, lr_clf.predict(X_test))\n",
        "\n",
        "# Print Accuracy Scores\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "print(f\"Stacking Classifier Accuracy: {stacking_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "TOIlwcGW9HaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q45.= Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "Ans."
      ],
      "metadata": {
        "id": "lk_wrDeJ9Iad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different bootstrap sample levels\n",
        "bootstrap_samples = [0.5, 0.7, 1.0]\n",
        "mse_scores = {}\n",
        "\n",
        "for sample in bootstrap_samples:\n",
        "    # Train Bagging Regressor with different bootstrap sample sizes\n",
        "    bagging_reg = BaggingRegressor(base_estimator=DecisionTreeRegressor(),\n",
        "                                   n_estimators=50,\n",
        "                                   max_samples=sample,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate MSE\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    mse_scores[sample] = mse\n",
        "    print(f\"Bagging Regressor (max_samples={sample}) - MSE: {mse:.4f}\")\n",
        "\n",
        "# Compare MSE values\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "for sample, mse in mse_scores.items():\n",
        "    print(f\"Bootstrap Sample {sample*100}%: MSE = {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "B9vIgzxv9P9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}