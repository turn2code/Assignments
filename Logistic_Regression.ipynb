{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "Ans.Logistic Regression vs. Linear Regression in Python\n",
        "1. Logistic Regression\n",
        "Logistic Regression is a classification algorithm used to predict categorical outcomes (e.g., binary classification: 0 or 1, spam or not spam).\n",
        "It applies the sigmoid function (or logistic function) to map predictions to probabilities between 0 and 1.\n",
        "The decision boundary is determined using a threshold (e.g., 0.5 for binary classification).\n",
        "It minimizes the log loss (cross-entropy loss) instead of mean squared error (MSE).\n",
        "2. Linear Regression\n",
        "Linear Regression is a regression algorithm used for predicting continuous values (e.g., house prices, temperature).\n",
        "It assumes a linear relationship between independent and dependent variables.\n",
        "It minimizes the Mean Squared Error (MSE).\n",
        "The output can be any real number, unlike Logistic Regression, which outputs probabilities.\n"
      ],
      "metadata": {
        "id": "8yCe261Jl9hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset (only two classes for binary classification)\n",
        "iris = load_iris()\n",
        "X, y = iris.data[:100], iris.target[:100]  # Selecting two classes (0 and 1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = log_reg.predict(X_test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "sVRbVt_NmWYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What is the mathematical equation of Logistic Regression.\n",
        "Ans.Mathematical Equation of Logistic Regression\n",
        "Logistic Regression is based on the logistic (sigmoid) function, which maps any real number to a value between 0 and 1.\n",
        "\n",
        "1. Hypothesis Function\n",
        "The hypothesis function for Logistic Regression is:\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝜃\n",
        "0\n",
        "+\n",
        "𝜃\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝜃\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝜃\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=\n",
        "1+e\n",
        "−(θ\n",
        "0\n",
        "​\n",
        " +θ\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +θ\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +...+θ\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "or simply,\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑋\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=\n",
        "1+e\n",
        "−θ\n",
        "T\n",
        " X\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x) is the predicted probability (output between 0 and 1).\n",
        "𝜃\n",
        "θ (theta) represents the model parameters (weights).\n",
        "𝑋\n",
        "X is the input feature vector.\n",
        "𝑒\n",
        "e is Euler’s number (~2.718).\n",
        "2. Decision Rule\n",
        "To classify an input, we set a threshold (typically 0.5) for the probability:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "{\n",
        "1\n",
        ",\n",
        "if\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "0\n",
        ",\n",
        "if\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "<\n",
        "0.5\n",
        "y={\n",
        "1,\n",
        "0,\n",
        "​\n",
        "  \n",
        "if h\n",
        "θ\n",
        "​\n",
        " (x)≥0.5\n",
        "if h\n",
        "θ\n",
        "​\n",
        " (x)<0.5\n",
        "​\n",
        "\n",
        "3. Cost Function (Log Loss)\n",
        "Instead of using Mean Squared Error (like in Linear Regression), Logistic Regression minimizes the log loss (cross-entropy loss):\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "where:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "J(θ) is the cost function.\n",
        "𝑚\n",
        "m is the number of training examples.\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  is the actual class label (0 or 1).\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ) is the predicted probability.\n",
        "This function penalizes wrong predictions heavily, ensuring the model improves.\n",
        "\n",
        "4. Optimization (Gradient Descent Update Rule)\n",
        "To minimize the cost function, we update parameters using Gradient Descent:\n",
        "\n",
        "𝜃\n",
        "𝑗\n",
        ":\n",
        "=\n",
        "𝜃\n",
        "𝑗\n",
        "−\n",
        "𝛼\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "𝑥\n",
        "𝑗\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "θ\n",
        "j\n",
        "​\n",
        " :=θ\n",
        "j\n",
        "​\n",
        " −α\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " (h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " )x\n",
        "j\n",
        "(i)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "α is the learning rate.\n",
        "The term\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " ) is the error."
      ],
      "metadata": {
        "id": "kgamgCwZmO-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression(X, y, lr=0.01, epochs=1000):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)  # Initialize weights\n",
        "\n",
        "    for _ in range(epochs):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        gradient = (1/m) * np.dot(X.T, (h - y))\n",
        "        theta -= lr * gradient  # Gradient descent update\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])  # Features\n",
        "y = np.array([0, 0, 1, 1])  # Labels\n",
        "\n",
        "theta = logistic_regression(X, y)\n",
        "print(\"Optimized Parameters:\", theta)\n"
      ],
      "metadata": {
        "id": "rth4LRVomkWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we use the Sigmoid function in Logistic Regression.\n",
        "Ans.Why Do We Use the Sigmoid Function in Logistic Regression?\n",
        "The sigmoid function is used in Logistic Regression because it converts any real-valued input into a probability between 0 and 1, making it ideal for classification tasks.\n",
        "\n",
        "1. Sigmoid Function Definition\n",
        "The sigmoid function is mathematically defined as:\n",
        "\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝜃\n",
        "𝑇\n",
        "𝑋\n",
        "z=θ\n",
        "T\n",
        " X (linear combination of weights and features).\n",
        "𝑒\n",
        "e is Euler’s number (~2.718).\n",
        "The output is always between\n",
        "0\n",
        "0 and\n",
        "1\n",
        "1, which can be interpreted as a probability.\n",
        "Reasons for Using the Sigmoid Function\n",
        "1. Converts Any Input into a Probability\n",
        "Unlike Linear Regression, which outputs any real number, the sigmoid function bounds the output between 0 and 1.\n",
        "This makes it perfect for binary classification (e.g., spam detection, disease prediction).\n",
        "2. Maps Predictions to Class Labels\n",
        "We can use a threshold (typically 0.5) to classify outputs:\n",
        "𝑦\n",
        "=\n",
        "{\n",
        "1\n",
        ",\n",
        "if\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≥\n",
        "0.5\n",
        "0\n",
        ",\n",
        "if\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "<\n",
        "0.5\n",
        "y={\n",
        "1,\n",
        "0,\n",
        "​\n",
        "  \n",
        "if σ(z)≥0.5\n",
        "if σ(z)<0.5\n",
        "​\n",
        "\n",
        "This ensures the model outputs discrete class labels rather than continuous values.\n",
        "3. Smooth and Differentiable Function\n",
        "The sigmoid function is smooth and has a well-defined derivative:\n",
        "𝑑\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "𝑑\n",
        "𝑧\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "(\n",
        "1\n",
        "−\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        ")\n",
        "dz\n",
        "dσ(z)\n",
        "​\n",
        " =σ(z)(1−σ(z))\n",
        "This allows us to efficiently optimize the model using gradient descent.\n",
        "4. Helps in Logistic Regression’s Cost Function\n",
        "The log loss (cross-entropy loss) function in Logistic Regression is based on the sigmoid function.\n",
        "The loss function is convex, making it easier to optimize."
      ],
      "metadata": {
        "id": "4krSJq0lmmMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Generate values for visualization\n",
        "z = np.linspace(-10, 10, 100)\n",
        "sigmoid_values = sigmoid(z)\n",
        "\n",
        "# Plot the sigmoid function\n",
        "plt.plot(z, sigmoid_values, label=\"Sigmoid Function\")\n",
        "plt.xlabel(\"z\")\n",
        "plt.ylabel(\"σ(z)\")\n",
        "plt.title(\"Sigmoid Function\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0Eq6hYBYmyHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the cost function of Logistic Regression.\n",
        "Ans.Cost Function of Logistic Regression\n",
        "In Logistic Regression, we use the Log Loss (Cross-Entropy Loss) as the cost function instead of Mean Squared Error (MSE). This is because MSE is non-convex for logistic regression, making optimization difficult.\n",
        "\n",
        "Mathematical Formulation\n",
        "For a single training example\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ",\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(x\n",
        "(i)\n",
        " ,y\n",
        "(i)\n",
        " ), the hypothesis function is:\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=\n",
        "1+e\n",
        "−θ\n",
        "T\n",
        " x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Since Logistic Regression predicts probabilities, we need a cost function that penalizes incorrect predictions while being convex for optimization.\n",
        "\n",
        "The log loss function is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "where:\n",
        "\n",
        "𝑚\n",
        "m = number of training examples.\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  = actual class label (0 or 1).\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ) = predicted probability (output of the sigmoid function).\n",
        "Intuition Behind the Cost Function\n",
        "The cost function is designed to:\n",
        "\n",
        "Punish wrong predictions heavily:\n",
        "If the actual class\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 but the model predicts a small probability, the term\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "logh\n",
        "θ\n",
        "​\n",
        " (x) gives a large negative value.\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 but the model predicts close to 1, the term\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "log(1−h\n",
        "θ\n",
        "​\n",
        " (x)) also gives a large negative value.\n",
        "Encourage correct predictions:\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 and\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "1\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)≈1, then\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "0\n",
        "logh\n",
        "θ\n",
        "​\n",
        " (x)≈0, leading to a small loss.\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 and\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "0\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)≈0, then\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "≈\n",
        "0\n",
        "log(1−h\n",
        "θ\n",
        "​\n",
        " (x))≈0, again resulting in a small loss.\n",
        "Gradient Descent Optimization\n",
        "To minimize the cost function, we use gradient descent with the following update rule:\n",
        "\n",
        "𝜃\n",
        "𝑗\n",
        ":\n",
        "=\n",
        "𝜃\n",
        "𝑗\n",
        "−\n",
        "𝛼\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "𝑥\n",
        "𝑗\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "θ\n",
        "j\n",
        "​\n",
        " :=θ\n",
        "j\n",
        "​\n",
        " −α\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " (h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " )x\n",
        "j\n",
        "(i)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "α is the learning rate.\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " ) represents the error.\n",
        "Cost Function of Logistic Regression\n",
        "In Logistic Regression, we use the Log Loss (Cross-Entropy Loss) as the cost function instead of Mean Squared Error (MSE). This is because MSE is non-convex for logistic regression, making optimization difficult.\n",
        "\n",
        "Mathematical Formulation\n",
        "For a single training example\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ",\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(x\n",
        "(i)\n",
        " ,y\n",
        "(i)\n",
        " ), the hypothesis function is:\n",
        "\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)=\n",
        "1+e\n",
        "−θ\n",
        "T\n",
        " x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Since Logistic Regression predicts probabilities, we need a cost function that penalizes incorrect predictions while being convex for optimization.\n",
        "\n",
        "The log loss function is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]\n",
        "where:\n",
        "\n",
        "𝑚\n",
        "m = number of training examples.\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        "  = actual class label (0 or 1).\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ) = predicted probability (output of the sigmoid function).\n",
        "Intuition Behind the Cost Function\n",
        "The cost function is designed to:\n",
        "\n",
        "Punish wrong predictions heavily:\n",
        "If the actual class\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 but the model predicts a small probability, the term\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "logh\n",
        "θ\n",
        "​\n",
        " (x) gives a large negative value.\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 but the model predicts close to 1, the term\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "log(1−h\n",
        "θ\n",
        "​\n",
        " (x)) also gives a large negative value.\n",
        "Encourage correct predictions:\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 and\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "1\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)≈1, then\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "0\n",
        "logh\n",
        "θ\n",
        "​\n",
        " (x)≈0, leading to a small loss.\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 and\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "≈\n",
        "0\n",
        "h\n",
        "θ\n",
        "​\n",
        " (x)≈0, then\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "≈\n",
        "0\n",
        "log(1−h\n",
        "θ\n",
        "​\n",
        " (x))≈0, again resulting in a small loss.\n",
        "Gradient Descent Optimization\n",
        "To minimize the cost function, we use gradient descent with the following update rule:\n",
        "\n",
        "𝜃\n",
        "𝑗\n",
        ":\n",
        "=\n",
        "𝜃\n",
        "𝑗\n",
        "−\n",
        "𝛼\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "𝑥\n",
        "𝑗\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "θ\n",
        "j\n",
        "​\n",
        " :=θ\n",
        "j\n",
        "​\n",
        " −α\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " (h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " )x\n",
        "j\n",
        "(i)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝛼\n",
        "α is the learning rate.\n",
        "(\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "(h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )−y\n",
        "(i)\n",
        " ) represents the error.\n"
      ],
      "metadata": {
        "id": "n5x9Dy-Sm0G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Compute logistic regression cost function\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    h = sigmoid(X @ theta)  # Compute predictions\n",
        "    cost = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))  # Log loss\n",
        "    return cost\n",
        "\n",
        "# Example usage\n",
        "X = np.array([[1, 2], [2, 3], [3, 4]])  # Feature matrix (without bias term)\n",
        "y = np.array([0, 1, 1])  # Labels\n",
        "theta = np.zeros(X.shape[1])  # Initialize parameters\n",
        "\n",
        "cost = compute_cost(X, y, theta)\n",
        "print(\"Initial Cost:\", cost)\n"
      ],
      "metadata": {
        "id": "wVqfnSQjptWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Regularization in Logistic Regression? Why is it needed.\n",
        "Ans.Regularization in Logistic Regression: Why Is It Needed?\n",
        "1. What Is Regularization?\n",
        "Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting by adding a penalty term to the cost function.\n",
        "\n",
        "In Logistic Regression, two common types of regularization are:\n",
        "\n",
        "L1 Regularization (Lasso Regression)\n",
        "L2 Regularization (Ridge Regression)\n",
        "These methods add a penalty to the model’s parameters (weights/coefficients) to prevent excessively large values, which can lead to overfitting.\n",
        "\n",
        "2. Why Is Regularization Needed?\n",
        "👉 Overfitting Problem:\n",
        "\n",
        "Without regularization, Logistic Regression may learn a model that fits the training data too well but performs poorly on new data.\n",
        "This happens when the model assigns very large weights to some features, making it highly sensitive to noise in the data.\n",
        "👉 Regularization Solves This By:\n",
        "\n",
        "Reducing the impact of less important features.\n",
        "Keeping model weights small and stable.\n",
        "Improving the generalization of the model on unseen data.\n",
        "3. Types of Regularization in Logistic Regression\n",
        "(i) L2 Regularization (Ridge, Default in Scikit-learn)\n",
        "Adds the sum of squared weights to the cost function.\n",
        "Prevents weights from becoming too large by penalizing them.\n",
        "Formula:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝜆\n",
        "λ (regularization parameter) controls the penalty strength.\n",
        "∑\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "∑θ\n",
        "j\n",
        "2\n",
        "​\n",
        "  is the sum of squared weights.\n",
        "🔹 Effect: Shrinks coefficients but does not eliminate them completely.\n",
        "\n",
        "(ii) L1 Regularization (Lasso)\n",
        "Adds the sum of absolute weights to the cost function.\n",
        "Can force some weights to be exactly zero, leading to feature selection.\n",
        "Formula:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]+\n",
        "m\n",
        "λ\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "🔹 Effect: Can remove less important features (useful for feature selection).\n",
        "\n",
        "4. Regularization in Python (Scikit-learn Example)\n",
        "By default, LogisticRegression in scikit-learn applies L2 regularization."
      ],
      "metadata": {
        "id": "PaCrhFCbpssO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, (iris.target == 2).astype(int)  # Convert to binary classification\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression with L2 Regularization (default)\n",
        "log_reg_l2 = LogisticRegression(C=1.0, penalty='l2', solver='liblinear')\n",
        "log_reg_l2.fit(X_train, y_train)\n",
        "\n",
        "# Logistic Regression with L1 Regularization\n",
        "log_reg_l1 = LogisticRegression(C=1.0, penalty='l1', solver='liblinear')\n",
        "log_reg_l1.fit(X_train, y_train)\n",
        "\n",
        "print(\"L2 Regularization Coefficients:\", log_reg_l2.coef_)\n",
        "print(\"L1 Regularization Coefficients:\", log_reg_l1.coef_)\n"
      ],
      "metadata": {
        "id": "er_1ZUpFp9mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "Ans.Difference Between Lasso, Ridge, and Elastic Net Regression\n",
        "Regularization techniques help prevent overfitting in regression models by adding a penalty term to the loss function. The three main types of regularization are Lasso (L1), Ridge (L2), and Elastic Net (L1 + L2).\n",
        "\n",
        "1. Ridge Regression (L2 Regularization)\n",
        "🔹 Adds the sum of squared weights as a penalty\n",
        "🔹 Shrinks coefficients but does not set them to zero\n",
        "🔹 Works well when all features are useful\n",
        "\n",
        "Cost Function:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "2\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " −h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )]\n",
        "2\n",
        " +λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝜆\n",
        "λ is the regularization parameter (higher\n",
        "𝜆\n",
        "λ means stronger penalty).\n",
        "The second term\n",
        "∑\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "∑θ\n",
        "j\n",
        "2\n",
        "​\n",
        "  penalizes large weights.\n",
        "✔ Keeps all features but shrinks them\n",
        "❌ Does not perform feature selection"
      ],
      "metadata": {
        "id": "FzjSaEbTqFv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=1.0)  # alpha = λ (regularization strength)\n",
        "ridge.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "M8Wi-vngqM5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Lasso Regression (L1 Regularization)\n",
        "🔹 Adds the sum of absolute weights as a penalty\n",
        "🔹 Shrinks some coefficients to exactly zero (performs feature selection)\n",
        "🔹 Useful when some features are irrelevant\n",
        "\n",
        "Cost Function:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "2\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "J(θ)=\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " −h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )]\n",
        "2\n",
        " +λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣\n",
        "✔ Feature selection: Eliminates irrelevant features\n",
        "❌ May struggle when features are highly correlated"
      ],
      "metadata": {
        "id": "byRfeRsvqO_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "lasso = Lasso(alpha=1.0)\n",
        "lasso.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "AVIwG1t4qRf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Elastic Net Regression (L1 + L2 Regularization)\n",
        "🔹 Combines Ridge (L2) and Lasso (L1) penalties\n",
        "🔹 Shrinks coefficients and performs feature selection\n",
        "🔹 Works well when features are highly correlated\n",
        "\n",
        "Cost Function:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "2\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝜃\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " −h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )]\n",
        "2\n",
        " +λ\n",
        "1\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣θ\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "✔ Balances feature selection and coefficient shrinkage\n",
        "✔ Handles correlated features better than Lasso\n",
        "❌ More complex to tune (requires two parameters:\n",
        "𝜆\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝜆\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        " )"
      ],
      "metadata": {
        "id": "kXCSvz6FqTXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio controls balance between L1 & L2\n",
        "elastic_net.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "LZJ-24BMqXh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When should we use Elastic Net instead of Lasso or Ridge.\n",
        "Ans.When Should We Use Elastic Net Instead of Lasso or Ridge?\n",
        "Elastic Net is a combination of Ridge (L2) and Lasso (L1) regularization. It is most useful when features are highly correlated and we need both shrinkage and feature selection.\n",
        "\n",
        "Scenarios Where Elastic Net Is Preferred\n",
        "✅ 1. When Features Are Highly Correlated\n",
        "\n",
        "Lasso struggles with highly correlated features and arbitrarily selects one while ignoring the others.\n",
        "Ridge keeps all features but does not eliminate any.\n",
        "Elastic Net ensures all correlated features contribute by balancing between L1 and L2 regularization.\n",
        "✅ 2. When You Need Both Feature Selection and Shrinkage\n",
        "\n",
        "Lasso can set some coefficients to exactly zero (feature selection), but it may be unstable when features are correlated.\n",
        "Ridge shrinks coefficients but never eliminates features.\n",
        "Elastic Net combines both:\n",
        "Selects important features (like Lasso).\n",
        "Shrinks the rest (like Ridge).\n",
        "✅ 3. When You Have More Features Than Samples (High-Dimensional Data)\n",
        "\n",
        "In cases like genomics, text classification, or finance, where number of features\n",
        "𝑝\n",
        "p > number of samples\n",
        "𝑛\n",
        "n, Lasso may select too few features, and Ridge may include too many.\n",
        "Elastic Net balances both effects, leading to better generalization.\n",
        "✅ 4. When Lasso Selects Too Few Features\n",
        "\n",
        "Lasso tends to pick only one feature from a group of correlated features, which may not always be ideal.\n",
        "Elastic Net allows for more distributed feature selection, ensuring important correlated variables are included."
      ],
      "metadata": {
        "id": "M4v0e_0Tqa9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=100, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Elastic Net with 50% L1 and 50% L2 regularization\n",
        "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio=0.5 balances Ridge & Lasso\n",
        "elastic_net.fit(X_train, y_train)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Elastic Net Coefficients:\", elastic_net.coef_)\n"
      ],
      "metadata": {
        "id": "dV7WFIq2qpU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "Ans.Impact of the Regularization Parameter (λ) in Logistic Regression\n",
        "The regularization parameter (λ) in Logistic Regression controls the strength of the penalty applied to the model’s coefficients. It determines the balance between fitting the training data well and preventing overfitting.\n",
        "\n",
        "1. Effect of λ on Model Performance\n",
        "Small λ (Weak Regularization) → Overfitting\n",
        "\n",
        "The model gives high importance to all features.\n",
        "Can lead to overfitting, meaning it performs well on training data but poorly on unseen data.\n",
        "Coefficients\n",
        "𝜃\n",
        "θ can take large values.\n",
        "Large λ (Strong Regularization) → Underfitting\n",
        "\n",
        "The penalty becomes stronger, forcing weights to be small or zero.\n",
        "Can lead to underfitting, where the model is too simple and fails to capture important patterns.\n",
        "The model may assign too little importance to important features, reducing accuracy.\n",
        "Optimal λ (Balanced Regularization)\n",
        "\n",
        "Finds a trade-off between variance and bias.\n",
        "Prevents overfitting while keeping model performance high.\n",
        "2. Regularization Types & Effect of λ\n",
        "In Logistic Regression, we typically use:\n",
        "\n",
        "L1 Regularization (Lasso): Larger λ forces more coefficients to exactly zero (feature selection).\n",
        "L2 Regularization (Ridge): Larger λ shrinks coefficients toward zero but doesn’t eliminate them.\n",
        "3. Mathematical Formulation\n",
        "The regularized cost function in Logistic Regression is:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "ℎ\n",
        "𝜃\n",
        "(\n",
        "𝑥\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝜃\n",
        "𝑗\n",
        "2\n",
        "J(θ)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " logh\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−h\n",
        "θ\n",
        "​\n",
        " (x\n",
        "(i)\n",
        " ))]+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "  \n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " θ\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "The first term is the standard log loss function.\n",
        "The second term adds a penalty to large coefficients.\n",
        "The strength of this penalty is controlled by λ.\n",
        "4. Visualizing the Impact of λ\n",
        "Effect of Different λ Values\n",
        "Regularization Strength (λ)\tEffect on Model\tEffect on Coefficients (θ)\n",
        "λ = 0 (No Regularization)\tOverfits, learns noise\tLarge, unconstrained coefficients\n",
        "Small λ\tSlight regularization, still flexible\tSome shrinkage, but features remain\n",
        "Optimal λ\tBest balance, prevents overfitting\tCoefficients are meaningful\n",
        "Large λ\tUnderfits, too simple\tVery small or zero coefficients\n",
        "5. Python Example: Tuning λ in Logistic Regression\n",
        "We use the C parameter in sklearn, where\n",
        "𝐶\n",
        "=\n",
        "1\n",
        "𝜆\n",
        "C=\n",
        "λ\n",
        "1\n",
        "​\n",
        " .\n",
        "\n",
        "Higher C → Less regularization\n",
        "Lower C → Stronger regularization"
      ],
      "metadata": {
        "id": "t36y_lxGqwY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Try different values of λ (in sklearn, we adjust C = 1/λ)\n",
        "for C in [100, 1, 0.01]:  # Larger C = Smaller λ, Smaller C = Larger λ\n",
        "    log_reg = LogisticRegression(C=C, penalty='l2', solver='liblinear')\n",
        "    log_reg.fit(X_train, y_train)\n",
        "    print(f\"C={C}, Accuracy: {log_reg.score(X_test, y_test):.4f}\")\n"
      ],
      "metadata": {
        "id": "2S7snyT_q5Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the key assumptions of Logistic Regression.\n",
        "Ans.Key Assumptions of Logistic Regression\n",
        "Although Logistic Regression is a powerful and widely used classification algorithm, it relies on several key assumptions for optimal performance. Here are the main assumptions:\n",
        "\n",
        "1. The Dependent Variable Must Be Binary or Categorical\n",
        "Logistic Regression is designed for classification problems, typically binary classification (e.g., Spam vs. Not Spam).\n",
        "It can be extended to multiclass classification using one-vs-rest (OvR) or softmax regression, but the standard logistic regression assumes a binary output:\n",
        "𝑦\n",
        "∈\n",
        "{\n",
        "0\n",
        ",\n",
        "1\n",
        "}\n",
        "y∈{0,1}\n",
        "2. Independence of Observations\n",
        "Each data point should be independent of the others.\n",
        "Logistic Regression does not work well on correlated observations (e.g., time-series data) unless modifications like time-series splitting or feature engineering are applied.\n",
        "🔹 Example Violation:\n",
        "\n",
        "If you predict whether a patient has a disease, but some patients appear multiple times in the dataset, their observations are correlated, violating this assumption.\n",
        "3. No Multicollinearity Between Independent Variables\n",
        "Multicollinearity occurs when two or more features are highly correlated, meaning they provide redundant information.\n",
        "Logistic Regression assumes that independent variables (features) are not too strongly correlated with each other.\n",
        "🔹 How to Detect Multicollinearity?\n",
        "\n",
        "Variance Inflation Factor (VIF): If VIF > 10, the feature is highly collinear.\n",
        "Correlation Matrix: A high correlation (>0.8) between two features suggests multicollinearity.\n",
        "🔹 Solution:\n",
        "\n",
        "Remove highly correlated features.\n",
        "Use Principal Component Analysis (PCA) or L1 Regularization (Lasso) to reduce multicollinearity.\n",
        "4. The Relationship Between Independent Variables and Log-Odds is Linear\n",
        "Logistic Regression assumes a linear relationship between the independent variables (X) and the log-odds of the dependent variable.\n",
        "\n",
        "This means that the logistic function (sigmoid) transforms a linear combination of features into a probability:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−p\n",
        "p\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "🔹 How to Check This Assumption?\n",
        "\n",
        "Use logarithm transformation or interaction terms if relationships appear non-linear.\n",
        "Box-Tidwell test can check for non-linearity in log-odds.\n",
        "5. Large Sample Size for Reliable Estimates\n",
        "Logistic Regression performs best with a large dataset, especially when features are highly dimensional.\n",
        "If the dataset is too small, the model may not converge properly or may overfit.\n",
        "🔹 Solution:\n",
        "\n",
        "Use more training samples if available.\n",
        "Apply regularization (L1/L2) to avoid overfitting.\n",
        "6. No Extreme Outliers in Features\n",
        "Logistic Regression is sensitive to extreme outliers, which can distort the model’s coefficients.\n",
        "Since logistic regression uses Maximum Likelihood Estimation (MLE) instead of Least Squares, extreme values in independent variables can impact predictions.\n",
        "🔹 Solution:\n",
        "\n",
        "Identify and remove outliers using:\n",
        "Boxplots or Z-score (|Z| > 3).\n",
        "Winsorization (capping extreme values).\n",
        "Log transformations to reduce the effect of outliers.\n",
        "7. The Classes Should Be Well-Separated and Balanced\n",
        "If the classes overlap significantly, Logistic Regression may struggle to find a clear decision boundary.\n",
        "If the dataset is imbalanced (e.g., 95% of cases are one class and 5% are the other), the model will bias toward the majority class.\n",
        "🔹 Solution:\n",
        "\n",
        "For class imbalance:\n",
        "Use class weights (class_weight='balanced' in sklearn).\n",
        "Use oversampling (SMOTE) or undersampling techniques.\n",
        "For non-linearly separable data:\n",
        "Use polynomial features or switch to a non-linear model like SVM or Neural Networks.\n",
        "8. Errors Should Be Independent (No Autocorrelation)\n",
        "In time-series or sequential data, errors may be correlated (autocorrelation).\n",
        "Logistic Regression assumes no pattern in the errors (i.e., residuals should be randomly distributed).\n",
        "🔹 Solution:\n",
        "\n",
        "If using time-series data, check residuals for patterns using Durbin-Watson test.\n",
        "Consider models like Recurrent Neural Networks (RNNs) for time-dependent predictions.\n",
        "Summary of Assumptions\n",
        "Assumption\tDescription\tPossible Fix if Violated\n",
        "Binary or Categorical Target\tOutcome must be 0 or 1\tUse Softmax for multi-class\n",
        "Independence of Observations\tNo repeated/related observations\tRestructure data, remove duplicates\n",
        "No Multicollinearity\tFeatures should not be highly correlated\tRemove redundant features, PCA\n",
        "Linear Relationship Between Log-Odds & Features\tIndependent variables must have a linear relationship with log-odds\tUse transformations (log, polynomials)\n",
        "Sufficient Sample Size\tSmall datasets lead to unreliable coefficients\tCollect more data, use regularization\n",
        "No Extreme Outliers\tOutliers distort predictions\tRemove/cap outliers using winsorization\n",
        "Balanced Classes\tImbalanced data biases predictions\tUse class_weight='balanced', SMOTE\n",
        "No Autocorrelation in Errors\tResiduals should not be correlated\tUse time-series models if needed\n"
      ],
      "metadata": {
        "id": "7yc8X1y9rAC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are some alternatives to Logistic Regression for classification tasks.\n",
        "Ans.Alternatives to Logistic Regression for Classification Tasks\n",
        "Logistic Regression is a simple and effective classification model, but in cases where its assumptions are violated or better performance is needed, other models can be used. Below are some powerful alternatives:\n",
        "\n",
        "1. Decision Tree Classifier 🌳\n",
        "How It Works:\n",
        "\n",
        "Splits data into nodes based on feature values using if-else conditions.\n",
        "Continues splitting until a stopping criterion (e.g., max depth) is met.\n",
        "Pros:\n",
        "✅ Handles non-linear relationships.\n",
        "✅ Works with categorical and numerical data.\n",
        "✅ Easy to interpret (like a flowchart).\n",
        "\n",
        "Cons:\n",
        "❌ Prone to overfitting without pruning.\n",
        "❌ Unstable to small changes in data.\n",
        "\n",
        "Python Example:"
      ],
      "metadata": {
        "id": "umaE7qKQrPpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(max_depth=5)\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "XhXrRmdaqxHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Random Forest Classifier 🌲🌲\n",
        "How It Works:\n",
        "\n",
        "An ensemble of multiple Decision Trees.\n",
        "Uses bagging (random sampling + averaging) to reduce variance.\n",
        "Final prediction is based on majority voting.\n",
        "Pros:\n",
        "✅ More accurate than a single Decision Tree.\n",
        "✅ Handles missing values and outliers well.\n",
        "✅ Works with high-dimensional data.\n",
        "\n",
        "Cons:\n",
        "❌ Slower than Logistic Regression for large datasets.\n",
        "❌ Harder to interpret compared to single Decision Trees.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "3. Support Vector Machine (SVM) ⚡\n",
        "How It Works:\n",
        "\n",
        "Finds the optimal hyperplane that best separates classes.\n",
        "Can handle non-linear classification using the kernel trick.\n",
        "Pros:\n",
        "✅ Effective for high-dimensional data.\n",
        "✅ Works well with small datasets.\n",
        "✅ Robust to outliers (with appropriate kernel).\n",
        "\n",
        "Cons:\n",
        "❌ Computationally expensive for large datasets.\n",
        "❌ Hard to tune hyperparameters (kernel, C, gamma, etc.).\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(kernel='rbf', C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "4. k-Nearest Neighbors (KNN) 🔍\n",
        "How It Works:\n",
        "\n",
        "Stores the entire dataset and classifies a new point by looking at the k nearest neighbors.\n",
        "Uses majority voting to assign a class.\n",
        "Pros:\n",
        "✅ Simple and intuitive.\n",
        "✅ No training required (lazy learning).\n",
        "✅ Works well with small datasets.\n",
        "\n",
        "Cons:\n",
        "❌ Slow on large datasets (because it stores all training points).\n",
        "❌ Sensitive to irrelevant features and imbalanced data.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model = KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(X_train, y_train)\n",
        "5. Naïve Bayes 🤖\n",
        "How It Works:\n",
        "\n",
        "Uses Bayes' Theorem to calculate the probability of each class given the features.\n",
        "Assumes independent features (hence \"naïve\").\n",
        "Pros:\n",
        "✅ Fast and efficient for large datasets.\n",
        "✅ Works well for text classification (e.g., spam detection).\n",
        "✅ Works even with small datasets.\n",
        "\n",
        "Cons:\n",
        "❌ Assumes feature independence, which is unrealistic in many cases.\n",
        "❌ Less flexible compared to other models.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "6. Gradient Boosting Models 🚀\n",
        "Boosting algorithms are powerful alternatives to Logistic Regression that combine multiple weak models to create a strong one.\n",
        "\n",
        "(a) XGBoost (Extreme Gradient Boosting)\n",
        "Pros:\n",
        "✅ Very high accuracy, widely used in Kaggle competitions.\n",
        "✅ Handles missing values well.\n",
        "✅ Works with structured/tabular data.\n",
        "\n",
        "Cons:\n",
        "❌ Requires careful hyperparameter tuning.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from xgboost import XGBClassifier\n",
        "model = XGBClassifier(n_estimators=100, learning_rate=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "(b) LightGBM (Light Gradient Boosting Machine)\n",
        "Pros:\n",
        "✅ Faster than XGBoost for large datasets.\n",
        "✅ Handles categorical features natively.\n",
        "\n",
        "Cons:\n",
        "❌ Needs careful tuning.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from lightgbm import LGBMClassifier\n",
        "model = LGBMClassifier(n_estimators=100)\n",
        "model.fit(X_train, y_train)\n",
        "(c) CatBoost\n",
        "Pros:\n",
        "✅ Great for categorical data (no need for encoding).\n",
        "✅ Fast training with GPU acceleration.\n",
        "\n",
        "Cons:\n",
        "❌ Tuning can be tricky.\n",
        "\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from catboost import CatBoostClassifier\n",
        "model = CatBoostClassifier(iterations=100)\n",
        "model.fit(X_train, y_train)\n",
        "7. Neural Networks (Deep Learning) 🧠\n",
        "How It Works:\n",
        "\n",
        "Uses multiple layers of neurons to learn complex patterns.\n",
        "Works well with large datasets and non-linear data.\n",
        "Pros:\n",
        "✅ Best for highly complex data (e.g., images, speech, NLP).\n",
        "✅ Can automatically learn feature representations.\n",
        "\n",
        "Cons:\n",
        "❌ Requires large data and high computational power.\n",
        "❌ Harder to interpret than traditional models.\n",
        "\n",
        "Python Example (Using TensorFlow/Keras):\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "Summary Table of Logistic Regression Alternatives\n",
        "Model\tStrengths\tWeaknesses\n",
        "Decision Tree\tSimple, interpretable\tOverfits without pruning\n",
        "Random Forest\tRobust, handles missing data\tSlower for large datasets\n",
        "SVM\tGood for small, high-dimensional data\tHard to tune for large datasets\n",
        "KNN\tNo training needed, simple\tSlow, sensitive to irrelevant features\n",
        "Naïve Bayes\tFast, good for text\tAssumes feature independence\n",
        "XGBoost / LightGBM / CatBoost\tHigh accuracy, good for structured data\tRequires tuning\n",
        "Neural Networks\tBest for complex patterns (NLP, vision)\tNeeds large data and GPUs\n"
      ],
      "metadata": {
        "id": "wTMnBBP4tzgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.What are Classification Evaluation Metrics.\n",
        "Ans.Classification Evaluation Metrics in Python 📊\n",
        "When training a classification model (e.g., Logistic Regression, SVM, Random Forest, etc.), it’s essential to evaluate how well it performs. Below are key classification evaluation metrics along with Python examples.\n",
        "\n",
        "1. Accuracy 🏆\n",
        "Definition:\n",
        "\n",
        "Measures the percentage of correct predictions out of total predictions.\n",
        "Formula:\n",
        "𝐴\n",
        "𝑐\n",
        "𝑐\n",
        "𝑢\n",
        "𝑟\n",
        "𝑎\n",
        "𝑐\n",
        "𝑦\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "​\n",
        "\n",
        "Best for:\n",
        "\n",
        "Balanced datasets where classes have equal distribution."
      ],
      "metadata": {
        "id": "tLnebVtruoZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h82foR-rl6An"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Precision 🎯\n",
        "Definition:\n",
        "\n",
        "Measures how many predicted positive values are actually positive.\n",
        "Formula:\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        "\n",
        "Best for:\n",
        "\n",
        "When False Positives (FP) are costly, e.g., spam detection, where classifying a normal email as spam is bad.\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import precision_score\n",
        "precision = precision_score(y_true, y_pred)\n",
        "print(\"Precision:\", precision)\n",
        "3. Recall (Sensitivity) 🔍\n",
        "Definition:\n",
        "\n",
        "Measures how many actual positive values were correctly predicted.\n",
        "Formula:\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "=\n",
        "𝑇\n",
        "𝑃\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        "\n",
        "Best for:\n",
        "\n",
        "When False Negatives (FN) are costly, e.g., medical diagnoses, where missing a disease is dangerous.\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import recall_score\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(\"Recall:\", recall)\n",
        "4. F1-Score ⚖️\n",
        "Definition:\n",
        "\n",
        "Harmonic mean of Precision & Recall (balances both).\n",
        "Formula:\n",
        "𝐹\n",
        "1\n",
        "=\n",
        "2\n",
        "×\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "×\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "𝑃\n",
        "𝑟\n",
        "𝑒\n",
        "𝑐\n",
        "𝑖\n",
        "𝑠\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "+\n",
        "𝑅\n",
        "𝑒\n",
        "𝑐\n",
        "𝑎\n",
        "𝑙\n",
        "𝑙\n",
        "F1=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "Best for:\n",
        "\n",
        "Imbalanced datasets, where accuracy alone is misleading.\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import f1_score\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"F1-Score:\", f1)\n",
        "5. Confusion Matrix 🟦🟥\n",
        "Definition:\n",
        "\n",
        "A matrix that shows True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN).\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "💡 Use case: Helps understand which type of errors the model is making.\n",
        "\n",
        "6. ROC Curve & AUC Score 🚀\n",
        "Definition:\n",
        "\n",
        "ROC Curve (Receiver Operating Characteristic): Plots True Positive Rate (Recall) vs. False Positive Rate at different thresholds.\n",
        "AUC (Area Under Curve): Measures overall performance (higher is better).\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true, y_prob)  # y_prob = model's probability predictions\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "💡 Use case:\n",
        "\n",
        "AUC = 1.0 → Perfect classifier\n",
        "AUC = 0.5 → Random guessing\n",
        "7. Log Loss (Cross-Entropy Loss) 🔥\n",
        "Definition:\n",
        "\n",
        "Measures how confident a model is in its predictions.\n",
        "Formula:\n",
        "𝐿\n",
        "𝑜\n",
        "𝑔\n",
        "𝐿\n",
        "𝑜\n",
        "𝑠\n",
        "𝑠\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "[\n",
        "𝑦\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑝\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑝\n",
        ")\n",
        "]\n",
        "LogLoss=−\n",
        "N\n",
        "1\n",
        "​\n",
        " ∑[ylog(p)+(1−y)log(1−p)]\n",
        "Best for:\n",
        "\n",
        "Probabilistic classifiers like Logistic Regression.\n",
        "Python Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import log_loss\n",
        "loss = log_loss(y_true, y_prob)\n",
        "print(\"Log Loss:\", loss)\n",
        "Summary Table\n",
        "Metric\tBest When\tFormula\n",
        "Accuracy\tClasses are balanced\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        ")\n",
        "/\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝑇\n",
        "𝑁\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "(TP+TN)/(TP+TN+FP+FN)\n",
        "Precision\tFalse Positives are costly\n",
        "𝑇\n",
        "𝑃\n",
        "/\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑃\n",
        ")\n",
        "TP/(TP+FP)\n",
        "Recall (Sensitivity)\tFalse Negatives are costly\n",
        "𝑇\n",
        "𝑃\n",
        "/\n",
        "(\n",
        "𝑇\n",
        "𝑃\n",
        "+\n",
        "𝐹\n",
        "𝑁\n",
        ")\n",
        "TP/(TP+FN)\n",
        "F1-Score\tImbalanced data (combines precision & recall)\n",
        "2\n",
        "×\n",
        "(\n",
        "𝑃\n",
        "×\n",
        "𝑅\n",
        ")\n",
        "/\n",
        "(\n",
        "𝑃\n",
        "+\n",
        "𝑅\n",
        ")\n",
        "2×(P×R)/(P+R)\n",
        "Confusion Matrix\tUnderstands types of errors\tTable of TP, FP, TN, FN\n",
        "ROC-AUC Score\tEvaluates probability-based models\tPlot of TPR vs. FPR\n",
        "Log Loss\tMeasures confidence in probability predictions\tCross-entropy formula\n"
      ],
      "metadata": {
        "id": "lxdcu6G_u6pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.How does class imbalance affect Logistic Regression.\n",
        "Ans.Problems Caused by Class Imbalance in Logistic Regression\n",
        "1 Biased Model Towards the Majority Class\n",
        "Logistic Regression minimizes overall error (by default).\n",
        "If 90% of data belongs to Class A and 10% to Class B, the model can achieve 90% accuracy just by always predicting A.\n",
        "However, it completely fails to recognize Class B.\n",
        " Example:\n",
        "\n",
        "A fraud detection system where 99% of transactions are normal and only 1% are fraudulent.\n",
        "The model predicts \"Normal\" for everything, giving 99% accuracy but zero fraud detection.\n",
        "2 Misleading Accuracy\n",
        "Accuracy is misleading when classes are imbalanced.\n",
        "Suppose a dataset has:\n",
        "950 \"No Fraud\" (Class 0)\n",
        "50 \"Fraud\" (Class 1)\n",
        "If the model predicts \"No Fraud\" for everything, it achieves 95% accuracy, but detects 0% of fraud cases.\n",
        " Better metrics:\n",
        "Use Precision, Recall, F1-score, and ROC-AUC instead of Accuracy.\n",
        "\n",
        "3 Poor Decision Boundary\n",
        "Logistic Regression learns a decision boundary based on the available data.\n",
        "With imbalanced data, the boundary gets shifted towards the minority class, making it harder to classify correctly.\n",
        " Example:\n",
        "\n",
        "If 99% of patients are healthy and 1% have a disease, the model is biased toward healthy patients and may miss diagnoses.\n",
        " How to Handle Class Imbalance in Logistic Regression\n",
        "1 Use Different Evaluation Metrics\n",
        "Instead of Accuracy, use:\n",
        "Precision (if False Positives matter)\n",
        "Recall (if False Negatives matter, e.g., in medical diagnosis)\n",
        "F1-score (when balancing both is important)\n",
        "ROC-AUC Score (for probabilistic models)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "2 Use Class Weights in Logistic Regression\n",
        "Set class_weight='balanced' in LogisticRegression().\n",
        "This adjusts weights inversely proportional to class frequencies.\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        " Why?\n",
        "This makes the model pay more attention to the minority class.\n",
        "\n",
        "3 Resampling Techniques (Oversampling & Undersampling)\n",
        " Oversampling the Minority Class (SMOTE)\n",
        "Generates synthetic examples of the minority class using SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "python\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        " Best for: When data is small and we need more samples.\n",
        "\n",
        " Undersampling the Majority Class\n",
        "Removes samples from the majority class to balance the dataset.\n",
        "python\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "rus = RandomUnderSampler()\n",
        "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        " Best for: When we have lots of data and can afford to drop some.\n",
        "\n",
        "4 Use Ensemble Methods\n",
        "Random Forest or XGBoost often perform better on imbalanced data than Logistic Regression.\n",
        "Both support balanced class weighting.\n",
        "python\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(scale_pos_weight=ratio_of_majority_to_minority)\n",
        "model.fit(X_train, y_train)\n",
        " Why?\n",
        "These methods focus more on misclassified samples.\n",
        "\n",
        "5 Adjust Decision Threshold\n",
        "By default, Logistic Regression predicts \"Positive\" if probability > 0.5.\n",
        "Lowering this threshold can improve recall for the minority class.\n",
        "python\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities\n",
        "new_threshold = 0.3  # Lower the decision threshold\n",
        "y_pred_adjusted = (y_prob > new_threshold).astype(int)\n",
        " Best for: When False Negatives are costly (e.g., missing a fraud transaction).\n",
        "\n"
      ],
      "metadata": {
        "id": "v4imCB_RETdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.What is Hyperparameter Tuning in Logistic Regression.\n",
        "Ans.Hyperparameter Tuning in Logistic Regression 🔍\n",
        " What is Hyperparameter Tuning?\n",
        "Hyperparameters are model parameters that cannot be learned from the data but must be set before training.\n",
        "Tuning these hyperparameters helps improve the model’s accuracy, generalization, and performance.\n",
        " Key Hyperparameters in Logistic Regression\n",
        "Hyperparameter\tDescription\tDefault\n",
        "C\tInverse of regularization strength (smaller = stronger regularization)\t1.0\n",
        "penalty\tType of regularization (l1, l2, elasticnet, none)\t'l2'\n",
        "solver\tAlgorithm to optimize the loss function (liblinear, lbfgs, saga, etc.)\t'lbfgs'\n",
        "max_iter\tMaximum number of iterations for optimization\t100\n",
        "class_weight\tHandles imbalanced data ('balanced' or None)\tNone\n",
        " 1. Grid Search for Hyperparameter Tuning\n",
        "Grid Search exhaustively tries all possible combinations of hyperparameters and selects the best one.\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Regularization type\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 and L2\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "log_reg = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        " Best for: Small datasets where we can afford to test all combinations.\n",
        "\n",
        " 2. Randomized Search for Faster Tuning\n",
        "Instead of testing all combinations, Randomized Search tests a random subset for efficiency.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "param_dist = {\n",
        "    'C': np.logspace(-3, 3, 10),  # 10 values between 0.001 and 1000\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=500),\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        " Best for: Large datasets where Grid Search is too slow.\n",
        "\n",
        " 3. Bayesian Optimization (Advanced)\n",
        "Bayesian Optimization intelligently selects hyperparameter values based on past evaluations, making it faster than Grid Search.\n",
        "\n",
        "python\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical\n",
        "\n",
        "bayes_search = BayesSearchCV(\n",
        "    LogisticRegression(max_iter=500),\n",
        "    {\n",
        "        'C': Real(0.0001, 100, prior='log-uniform'),\n",
        "        'penalty': Categorical(['l1', 'l2']),\n",
        "        'solver': Categorical(['liblinear', 'saga'])\n",
        "    },\n",
        "    n_iter=25, cv=5, scoring='accuracy', random_state=42\n",
        ")\n",
        "\n",
        "bayes_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", bayes_search.best_params_)\n",
        " Best for: Large datasets with expensive training times."
      ],
      "metadata": {
        "id": "vyrJo-vGE7gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "Ans.Solvers in Logistic Regression\n",
        "Logistic Regression in scikit-learn provides different solvers (optimization algorithms) to minimize the cost function. The choice of solver depends on dataset size, regularization type, and performance needs.\n",
        "Which Solver Should You Use?\n",
        "Scenario\tRecommended Solver\n",
        "Small dataset (<10,000 samples)\tliblinear\n",
        "Large dataset (>50,000 samples)\tsag or saga\n",
        "Multi-class classification (multi_class='multinomial')\tlbfgs, newton-cg, or saga\n",
        "L1 regularization (Lasso)\tliblinear or saga\n",
        "L2 regularization (Ridge)\tlbfgs, newton-cg, sag, saga\n",
        "Elastic Net regularization\tsaga"
      ],
      "metadata": {
        "id": "Hax7c2WTF8Or"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.How is Logistic Regression extended for multiclass classification.\n",
        "Ans.Extending Logistic Regression for Multiclass Classification\n",
        "By default, Logistic Regression is designed for binary classification (two classes, e.g., 0 and 1). However, it can be extended for multiclass classification using two main approaches:\n",
        "\n",
        " 1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        " How it works?\n",
        "\n",
        "The model trains one classifier per class.\n",
        "Each classifier treats one class as \"positive\" and the rest as \"negative.\"\n",
        "The class with the highest probability is selected.\n",
        " Example (3 Classes: A, B, C)\n",
        "\n",
        "Train Classifier 1: A vs (B, C)\n",
        "Train Classifier 2: B vs (A, C)\n",
        "Train Classifier 3: C vs (A, B)\n",
        "For a new sample, all classifiers predict probabilities, and the class with the highest probability is chosen.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# OvR is the default for multiclass Logistic Regression\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        " Best for:\n",
        "\n",
        "Small datasets\n",
        "Fast training\n",
        "Interpretable models\n",
        " Limitations:\n",
        "\n",
        "Can be less accurate than other methods.\n",
        "Separate classifiers may not work well for overlapping classes.\n",
        " 2. Softmax (Multinomial) Regression\n",
        " How it works?\n",
        "\n",
        "Uses a single model with a Softmax function to predict class probabilities.\n",
        "Instead of binary decision boundaries, it finds a single probability distribution over all classes.\n",
        " Softmax Formula:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "θ\n",
        "j\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "e\n",
        "θ\n",
        "k\n",
        "T\n",
        "​\n",
        " x\n",
        "\n",
        "​\n",
        "\n",
        "where K is the number of classes.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Multinomial Logistic Regression with softmax activation\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        " Best for:\n",
        "\n",
        "Large datasets\n",
        "Better accuracy than OvR\n",
        "Well-suited for datasets where classes have strong relationships\n",
        " Limitations:\n",
        "\n",
        "More computationally expensive than OvR.\n",
        " OvR vs. Softmax: Which One to Use?\n",
        "Feature\tOne-vs-Rest (OvR)\tSoftmax (Multinomial)\n",
        "Number of classifiers\tK (one for each class)\t1\n",
        "Training complexity\tFaster\tSlower\n",
        "Best for\tSmall datasets\tLarge datasets\n",
        "Accuracy\tLower\tHigher\n",
        "Interpretability\tEasier\tHarder\n"
      ],
      "metadata": {
        "id": "m-U0KPECGY82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.What are the advantages and disadvantages of Logistic Regression.\n",
        "Ans.Advantages of Logistic Regression\n",
        "Advantage\tExplanation\n",
        "1 Simple & Easy to Interpret\tThe model outputs probabilities, making results easy to understand.\n",
        "2 Works Well for Linearly Separable Data\tIf data is linearly separable, Logistic Regression performs well.\n",
        "3Fast & Computationally Efficient\tCompared to complex models (e.g., SVM, Neural Networks), it trains quickly.\n",
        "4 Handles Large Datasets Well\tEfficient for large datasets when using optimizations like saga solver.\n",
        "5 Provides Probabilistic Outputs\tUnlike Decision Trees, it gives class probabilities instead of just labels.\n",
        "6 Can Handle Class Imbalance (With Regularization)\tCan be improved using class_weight='balanced' or oversampling techniques like SMOTE.\n",
        "7 Works with Regularization (L1, L2, Elastic Net)\tPrevents overfitting with penalty='l1' (Lasso), penalty='l2' (Ridge), or penalty='elasticnet'.\n",
        "8 Extends to Multiclass Problems (Softmax / OvR)\tSupports multi_class='multinomial' for Softmax Regression.\n",
        " Disadvantages of Logistic Regression\n",
        "Disadvantage\tExplanation\n",
        "1 Assumes Linearity\tStruggles when data is non-linearly separable.\n",
        "2 Poor Performance on Large Feature Sets\tIf there are too many independent variables, it may overfit.\n",
        "3 Sensitive to Outliers\tLogistic Regression can be heavily impacted by outliers, affecting decision boundaries.\n",
        "4 Doesn't Work Well with Highly Correlated Features\tIf independent variables are highly correlated, it can reduce model performance (use PCA or VIF to handle multicollinearity).\n",
        "5 Can Struggle with Class Imbalance\tWithout adjustments (class_weight='balanced' or SMOTE), it may favor the majority class.\n",
        "6 Cannot Model Complex Relationships\tUnlike Decision Trees or Neural Networks, it can't capture complex, nonlinear decision boundaries.\n",
        "7 Requires Feature Engineering\tWorks best when features are well-preprocessed (e.g., normalized, transformed).\n",
        " When to Use Logistic Regression?\n",
        " When data is linearly separable\n",
        " When interpretability is important (e.g., medical diagnosis, fraud detection) When you need a quick and efficient model\n",
        "\n",
        "Avoid Logistic Regression when:\n",
        "\n",
        "The relationship between input and output is nonlinear (use Decision Trees, SVM, or Neural Networks).\n",
        "You have many correlated features (use PCA or feature selection first).\n",
        "You have a large number of categorical variables (use embeddings or tree-based models)."
      ],
      "metadata": {
        "id": "2tLzKkP8G-Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.What are some use cases of Logistic Regression.\n",
        "Ans.Use Cases of Logistic Regression\n",
        "Logistic Regression is widely used in binary and multiclass classification problems across various domains. Here are some of its key applications:\n",
        "\n",
        "1. Medical Diagnosis\n",
        " Example: Predicting whether a patient has a disease (Yes/No).\n",
        "\n",
        "Use case: Diabetes detection, cancer diagnosis, heart disease prediction.\n",
        "Features: Age, blood pressure, cholesterol levels, glucose levels, etc.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Example: Predicting diabetes\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)  # X: patient data, y: diabetes (0 = No, 1 = Yes)\n",
        " 2. Spam Email Detection\n",
        "  Example: Classifying emails as Spam or Not Spam.\n",
        "\n",
        "Use case: Email filtering systems like Gmail’s spam filter.\n",
        "Features: Presence of certain keywords, email length, sender reputation.\n",
        "python\n",
        "\n",
        "# Example: Detecting spam emails\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(email_features, labels)  # labels: 0 = Not Spam, 1 = Spam\n",
        " 3. Customer Churn Prediction\n",
        " Example: Predicting if a customer will cancel a subscription.\n",
        "\n",
        "Use case: Telecom, banking, and SaaS companies use this to reduce customer attrition.\n",
        "Features: Monthly bill, customer complaints, contract type, usage patterns.\n",
        "python\n",
        "\n",
        "# Example: Predicting customer churn\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(customer_data, churn_labels)  # churn_labels: 0 = Stay, 1 = Leave\n",
        " 4. Credit Risk Assessment\n",
        " Example: Predicting whether a loan applicant will default.\n",
        "\n",
        "Use case: Banks use this to decide loan approvals.\n",
        "Features: Credit score, income, loan amount, previous defaults.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# Example: Predicting loan default\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(loan_data, default_labels)  # 0 = No Default, 1 = Default\n",
        "5. Fraud Detection\n",
        " Example: Identifying fraudulent transactions.\n",
        "\n",
        "Use case: Banks and payment processors (e.g., PayPal, Visa).\n",
        "Features: Transaction amount, frequency, location, device used.\n",
        "python\n",
        "\n",
        "# Example: Fraud detection\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(transaction_data, fraud_labels)  # 0 = Genuine, 1 = Fraud\n",
        " 6. Sentiment Analysis\n",
        " Example: Predicting whether a review is positive or negative.\n",
        "\n",
        "Use case: Amazon, Yelp, and social media analysis.\n",
        "Features: Word frequencies, sentiment scores, length of review.\n",
        "python\n",
        "\n",
        "# Example: Sentiment analysis on product reviews\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(review_data, sentiment_labels)  # 0 = Negative, 1 = Positive\n",
        " 7. Employee Attrition Prediction\n",
        " Example: Predicting whether an employee will quit a job.\n",
        "\n",
        "Use case: HR analytics in companies.\n",
        "Features: Salary, job satisfaction, work experience, commute time.\n",
        "python\n",
        "\n",
        "# Example: Employee attrition prediction\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(employee_data, attrition_labels)  # 0 = Stay, 1 = Leave\n",
        " 8. Predicting Click-Through Rate (CTR)\n",
        "  Example: Predicting whether a user will click on an ad.\n",
        "\n",
        "Use case: Google Ads, Facebook Ads.\n",
        "Features: User demographics, past clicks, ad content.\n",
        "python\n",
        "\n",
        "# Example: Click-through rate prediction\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(ad_data, click_labels)  # 0 = No Click, 1 = Click\n",
        " 9. Image Classification\n",
        " Example: Recognizing handwritten digits (0-9).\n",
        "\n",
        "Use case: Optical Character Recognition (OCR), digit recognition in bank cheques.\n",
        "Features: Pixel intensity values.\n",
        "python\n",
        "\n",
        "# Example: Handwritten digit classification (0-9)\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "log_reg.fit(image_data, digit_labels)\n",
        " 10. Political Campaign Prediction\n",
        " Example: Predicting whether a voter will support a candidate.\n",
        "\n",
        "Use case: Political analytics and election forecasting.\n",
        "Features: Age, income, political preference, past voting history.\n",
        "python\n",
        "\n",
        "# Example: Predicting voter behavior\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(voter_data, support_labels)  # 0 = No Support, 1 = Support\n"
      ],
      "metadata": {
        "id": "WHVUgLhvG9R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.What is the difference between Softmax Regression and Logistic Regression.\n",
        "Ans.Softmax Regression vs. Logistic Regression\n",
        "Both Logistic Regression and Softmax Regression are used for classification, but they differ in how they handle the number of classes.\n",
        "\n",
        "Feature\tLogistic Regression\tSoftmax Regression (Multinomial Logistic Regression)\n",
        "Used for\tBinary classification (2 classes: 0 or 1)\tMulticlass classification (3+ classes)\n",
        "Output\tProbability of one class vs. the other\tProbability distribution over all classes\n",
        "Decision Rule\tUses Sigmoid function\tUses Softmax function\n",
        "Formula\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Softmax\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑧\n",
        "𝑖\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑒\n",
        "𝑧\n",
        "𝑗\n",
        "Softmax(z\n",
        "i\n",
        "​\n",
        " )=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " e\n",
        "z\n",
        "j\n",
        "​\n",
        "\n",
        "\n",
        "e\n",
        "z\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "Model Training\tFits a single decision boundary\tLearns one weight vector per class\n",
        "Multiclass Support\tRequires One-vs-Rest (OvR) approach\tDirectly supports multiple classes\n",
        " Example: Logistic Regression (Binary Classification)\n",
        " Use Case: Predict if an email is spam or not spam (0/1)\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Binary Classification (Spam/Not Spam)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)  # y contains 0 or 1\n",
        " Example: Softmax Regression (Multiclass Classification)\n",
        " Use Case: Predict the digit in handwritten digit recognition (0-9)\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Multiclass Classification (Digits 0-9)\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)  # y contains multiple classes (0,1,2,...,9)\n"
      ],
      "metadata": {
        "id": "M9WHoZILIjwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "Ans.Choosing Between One-vs-Rest (OvR) and Softmax (Multinomial) for Multiclass Classification\n",
        "When dealing with multiclass classification (3+ classes) in Logistic Regression, we have two main approaches:\n",
        "1 One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "2 Softmax (Multinomial) Regression\n",
        "\n",
        " Key Differences:\n",
        "Feature\tOne-vs-Rest (OvR)\tSoftmax (Multinomial)\n",
        "Concept\tTrains one binary classifier for each class (each class vs. rest).\tTrains a single model with Softmax activation to handle all classes together.\n",
        "Number of Models\tRequires K binary classifiers (one per class).\tUses one single model for all classes.\n",
        "Predictions\tEach classifier gives a probability, and the class with the highest probability is chosen.\tComputes a probability distribution over all classes using Softmax.\n",
        "Computational Complexity\tFaster training (each classifier is trained separately).\tMore complex (all weights optimized together).\n",
        "Best for\tSmall datasets, imbalanced data.\tLarge datasets, complex decision boundaries.\n",
        "Interpretability\tEasier to interpret since each class is treated separately.\tHarder to interpret as all classes influence the decision.\n",
        "Performance\tCan be less accurate if classes are highly related.\tTypically more accurate for well-distributed datasets.\n",
        " When to Choose OvR vs. Softmax\n",
        "Scenario\tBest Choice\n",
        "Small dataset, faster training needed\tOvR\n",
        "Large dataset, better accuracy needed\tSoftmax\n",
        "Highly imbalanced classes\tOvR (handles imbalance better)\n",
        "Classes are mutually exclusive (e.g., digit classification)\tSoftmax\n",
        "Interpretability is important\tOvR\n",
        " Example: Implementing OvR and Softmax in Python\n",
        "1 One-vs-Rest (OvR)\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train using One-vs-Rest (default for multiclass Logistic Regression)\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "model_ovr.fit(X_train, y_train)\n",
        "2 Softmax (Multinomial) Regression\n",
        "python\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train using Softmax Regression\n",
        "model_softmax = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model_softmax.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "7593KI45I4rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.How do we interpret coefficients in Logistic Regression?\n",
        "Ans.Interpreting Coefficients in Logistic Regression\n",
        "In Logistic Regression, the model outputs a probability, and the coefficients\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "  tell us how each feature influences the outcome. However, unlike Linear Regression, the interpretation is in terms of log-odds.\n",
        "\n",
        " 1. Logistic Regression Model Equation\n",
        "The probability of an event occurring is given by the Sigmoid function:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑌\n",
        "=\n",
        "1\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        ")\n",
        "P(Y=1)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Taking the log-odds transformation (logit function):\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        ".\n",
        ".\n",
        ".\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +...+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "Each coefficient\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "  represents the change in log-odds when the corresponding feature\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "​\n",
        "  increases by one unit, keeping other variables constant.\n",
        "\n",
        " 2. Interpreting Coefficients in Logistic Regression\n",
        "Coefficient\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        " \tInterpretation\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " >0\tThe feature increases the probability of the event happening (positive influence).\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " <0\tThe feature decreases the probability of the event happening (negative influence).\n",
        "𝛽\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "β\n",
        "i\n",
        "​\n",
        " =0\tThe feature has no effect on the outcome.\n",
        " Odds Ratio Interpretation\n",
        "The exponentiated coefficient (\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " ) tells us the odds ratio (OR):\n",
        "\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        ">\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " >1 → Feature increases odds of the event occurring.\n",
        "If\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "<\n",
        "1\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        " <1 → Feature decreases odds of the event occurring.\n",
        "Odds Ratio\n",
        "=\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "Odds Ratio=e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "\n",
        "3. Example Interpretation\n",
        "Example: Predicting Loan Default (Yes = 1, No = 0)\n",
        "Feature\tCoefficient\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "𝑒\n",
        "𝛽\n",
        "𝑖\n",
        "e\n",
        "β\n",
        "i\n",
        "​\n",
        "\n",
        "  (Odds Ratio)\tInterpretation\n",
        "Income\n",
        "−\n",
        "0.5\n",
        "−0.5\n",
        "𝑒\n",
        "−\n",
        "0.5\n",
        "=\n",
        "0.61\n",
        "e\n",
        "−0.5\n",
        " =0.61\tHigher income reduces the odds of defaulting.\n",
        "Credit Score\n",
        "0.3\n",
        "0.3\n",
        "𝑒\n",
        "0.3\n",
        "=\n",
        "1.35\n",
        "e\n",
        "0.3\n",
        " =1.35\tHigher credit score increases the odds of defaulting.\n",
        "Debt-to-Income Ratio\n",
        "1.2\n",
        "1.2\n",
        "𝑒\n",
        "1.2\n",
        "=\n",
        "3.32\n",
        "e\n",
        "1.2\n",
        " =3.32\tA higher debt-to-income ratio greatly increases the odds of default.\n",
        "🛠 Interpretation for Credit Score (\n",
        "𝛽\n",
        "=\n",
        "0.3\n",
        "β=0.3):\n",
        "\n",
        "A one-unit increase in credit score multiplies the odds of default by 1.35.\n",
        "If baseline default odds were 10% (0.1 probability), they would increase to ~13.5%.\n",
        "4. Standardization & Scaling Considerations\n",
        "Feature scaling matters: If variables are not standardized, coefficients can be misleading.\n",
        "Categorical variables: Need one-hot encoding before interpretation.\n",
        "Multicollinearity: If features are highly correlated, coefficients may be unstable.\n"
      ],
      "metadata": {
        "id": "hswoZc83JP36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "OoJ8lOfeJ_u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy\n",
        "Ans.import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "X-r4f73RKEKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy"
      ],
      "metadata": {
        "id": "IJX_lGOBKZAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Le66ev4_KfTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "Ans.import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "p3GSZfhJKmVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "Ans."
      ],
      "metadata": {
        "id": "q2uLQNTVMwua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "K3PwEebzM73S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'\n",
        "Ans."
      ],
      "metadata": {
        "id": "t0B2p0RHM86z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model for multiclass classification using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest (OvR): {accuracy:.4f}\")\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "id": "uLOq5VBHNM7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "u4lkNrmbNN7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both 'l1' and 'l2'\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get best parameters and accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "best_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Model Accuracy: {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "3ub5GOkeNfuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "IlaFLO7gNgtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, solver='lbfgs', multi_class='ovr')\n",
        "\n",
        "# Apply Stratified K-Fold Cross-Validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model, X_scaled, y, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(cv_scores)\n",
        "print(f\"Average Accuracy with Stratified K-Fold CV: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "02UHehr6NrQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.\n",
        "Ans."
      ],
      "metadata": {
        "id": "1gp7BU1RNsDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV file\n",
        "def load_data(csv_file):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    X = data.iloc[:, :-1]  # Features (all columns except the last)\n",
        "    y = data.iloc[:, -1]   # Target (last column)\n",
        "    return X, y\n",
        "\n",
        "# Load data\n",
        "csv_file = \"dataset.csv\"  # Replace with your CSV file name\n",
        "X, y = load_data(csv_file)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ilutVonTN5Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.\n",
        "Ans."
      ],
      "metadata": {
        "id": "EOFLB2ZpN9JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Apply RandomizedSearchCV to find the best hyperparameters\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_distributions=param_dist,\n",
        "                                   n_iter=10, cv=5, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get best parameters and accuracy\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "best_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Model Accuracy: {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "X8ArG-j-OJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "oOxraIV9OKow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features\n",
        "y = data.target  # Target (0, 1, or 2 for different iris species)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the One-vs-One (OvO) Logistic Regression model\n",
        "ovo_model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-One (OvO): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Zai3SznGOY4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classificationM\n",
        "Ans.import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dpKJl3FiOZ3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-ScoreM\n",
        "Ans."
      ],
      "metadata": {
        "id": "ByCMN3T1OrDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rzkft84hO3eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performanceM\n",
        "Ans."
      ],
      "metadata": {
        "id": "0YrmydTdO4de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model with class weights\n",
        "model = LogisticRegression(max_iter=200, class_weight='balanced')\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6eRI72yRPIYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performanceM\n",
        "Ans."
      ],
      "metadata": {
        "id": "JRy0_avXPJQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4_hcE3C5P0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scalingM\n",
        "Ans"
      ],
      "metadata": {
        "id": "1EZ2kEnZP1L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Logistic Regression without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Model Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with scaling\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Model Accuracy with Scaling: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_scaled > accuracy_no_scaling:\n",
        "    print(\"Feature scaling improved model performance.\")\n",
        "elif accuracy_scaled < accuracy_no_scaling:\n",
        "    print(\"Feature scaling reduced model performance.\")\n",
        "else:\n",
        "    print(\"Feature scaling had no effect on model performance.\")\n",
        "\n",
        "# Compute confusion matrices\n",
        "cm_no_scaling = confusion_matrix(y_test, y_pred_no_scaling)\n",
        "cm_scaled = confusion_matrix(y_test, y_pred_scaled)\n",
        "\n",
        "# Visualize confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.heatmap(cm_no_scaling, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'], ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix - No Scaling\")\n",
        "axes[0].set_xlabel(\"Predicted Label\")\n",
        "axes[0].set_ylabel(\"True Label\")\n",
        "\n",
        "sns.heatmap(cm_scaled, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'], ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix - With Scaling\")\n",
        "axes[1].set_xlabel(\"Predicted Label\")\n",
        "axes[1].set_ylabel(\"True Label\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1Rv-svgCQAos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM\n",
        "Ans."
      ],
      "metadata": {
        "id": "EsWQCTvBQQGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hZbrV2lVQe3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy\n",
        "Ans"
      ],
      "metadata": {
        "id": "JEIOLC8wQgfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5ZZtHnqsQmM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients\n",
        "Ans."
      ],
      "metadata": {
        "id": "YSZfib5QQybB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Identify important features based on model coefficients\n",
        "feature_importance = pd.DataFrame({\"Feature\": X.columns, \"Coefficient\": model.coef_[0]})\n",
        "feature_importance = feature_importance.sort_values(by=\"Coefficient\", ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=feature_importance, palette=\"coolwarm\")\n",
        "plt.title(\"Feature Importance based on Logistic Regression Coefficients\")\n",
        "plt.xlabel(\"Coefficient Value\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZN4Fo0mJRCqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score\n",
        "Ans"
      ],
      "metadata": {
        "id": "WdoltKZ9RDjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, cohen_kappa_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N_VnnGj8RIS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:\n",
        "Ans."
      ],
      "metadata": {
        "id": "bAulUfpDRTeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, cohen_kappa_score, precision_recall_curve\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall_vals, precision_vals, color='green', label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2rkciglHRhnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "a419Yz8URiiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, cohen_kappa_score, precision_recall_curve\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with different solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "solver_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    solver_results[solver] = accuracy\n",
        "    print(f\"Solver: {solver} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot solver comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(solver_results.keys(), solver_results.values(), color=['blue', 'green', 'red'])\n",
        "plt.xlabel('Solver')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Logistic Regression Solvers')\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall_vals, precision_vals, color='green', label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H68W7GMIR9SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)\n",
        "Ans."
      ],
      "metadata": {
        "id": "XtW-YJjfR-IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, cohen_kappa_score, precision_recall_curve, matthews_corrcoef\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "mcc_score = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall_vals, precision_vals, color='green', label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U7ULkfI_SMWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling\n",
        "Ans."
      ],
      "metadata": {
        "id": "Vqa2uVCcSNKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, cohen_kappa_score, precision_recall_curve, matthews_corrcoef\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Compare performance\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Raw Data\", \"Standardized Data\"], [accuracy_raw, accuracy_scaled], color=['blue', 'green'])\n",
        "plt.xlabel(\"Feature Scaling\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Impact of Feature Scaling on Logistic Regression Accuracy\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HjbvSxb4SiEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation\n",
        "Ans."
      ],
      "metadata": {
        "id": "912eiaLmSi0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define hyperparameter grid for C values\n",
        "param_grid = {\"C\": np.logspace(-4, 4, 20)}\n",
        "\n",
        "# Perform GridSearchCV to find optimal C\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get best parameter\n",
        "best_C = grid_search.best_params_[\"C\"]\n",
        "print(f\"Optimal C value: {best_C}\")\n",
        "\n",
        "# Train Logistic Regression with optimal C\n",
        "best_model = LogisticRegression(C=best_C, max_iter=200)\n",
        "best_model.fit(X_train_scaled, y_train)\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with optimal C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "NgQHtmWNSns_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.\n",
        "Ans."
      ],
      "metadata": {
        "id": "Yb-uIfURS0GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load Titanic dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
        "data = data[features + [\"Survived\"]]\n",
        "\n",
        "# Handle missing values\n",
        "data[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\n",
        "data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data[\"Sex\"] = label_encoder.fit_transform(data[\"Sex\"])\n",
        "data[\"Embarked\"] = label_encoder.fit_transform(data[\"Embarked\"])\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop(\"Survived\", axis=1)\n",
        "y = data[\"Survived\"]\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(C=1.0, max_iter=200)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, \"logistic_regression_model.joblib\")\n",
        "\n",
        "# Load the model back\n",
        "loaded_model = joblib.load(\"logistic_regression_model.joblib\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = loaded_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy after loading: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "lhWOvBI9S6rO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}