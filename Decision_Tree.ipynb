{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.What is a Decision Tree, and how does it work\n",
        "Ans.A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like model where decisions are made by splitting data based on feature values.\n",
        "\n",
        "How Does a Decision Tree Work?\n",
        "Start with the Root Node\n",
        "The entire dataset is considered at the root.\n",
        "Splitting Based on Features\n",
        "The algorithm selects the best feature to split the data using criteria like Gini Impurity or Entropy (Information Gain).\n",
        "Creating Internal Nodes and Branches\n",
        "Each split creates new internal nodes, leading to branches based on feature values.\n",
        "Reaching Leaf Nodes\n",
        "The tree keeps splitting until stopping criteria are met (e.g., max depth, minimum samples per split).\n",
        "Making Predictions\n",
        "A new data point is passed through the tree, following the decision rules, until it reaches a leaf node, where the final prediction is made."
      ],
      "metadata": {
        "id": "dn-Hj8xNO3dp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fao20wYZOgHA"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Visualize the Decision Tree\n",
        "plt.figure(figsize=(10,6))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.What are impurity measures in Decision Trees\n",
        "Ans.Impurity measures determine how well a dataset is split at each node of a Decision Tree. Lower impurity means better classification. The three main impurity measures are:\n",
        "\n",
        "1. Gini Impurity (Default in Scikit-learn)\n",
        "Measures the probability of incorrectly classifying a randomly chosen element.\n",
        "\n",
        "Formula:\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of class\n",
        "𝑖\n",
        "i.\n",
        "A lower Gini score means a purer node.\n",
        "Example in Python:\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini')  # Using Gini Impurity\n",
        "2. Entropy (Information Gain)\n",
        "Measures the disorder in a dataset. Lower entropy means purer splits.\n",
        "\n",
        "Formula:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "The algorithm chooses the split with the highest Information Gain, which is:\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "(\n",
        "𝑤\n",
        "𝑖\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        ")\n",
        "Information Gain=Entropy\n",
        "parent\n",
        "​\n",
        " −∑(w\n",
        "i\n",
        "​\n",
        " ×Entropy\n",
        "child\n",
        "​\n",
        " )\n",
        "Example in Python:\n",
        "python\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='entropy')  # Using Entropy\n",
        "3. Mean Squared Error (MSE) (For Regression Trees)\n",
        "Used for regression trees, MSE measures variance within nodes.\n",
        "\n",
        "Formula:\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "N\n",
        "1\n",
        "​\n",
        " ∑(y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "The lower the MSE, the better the split.\n",
        "Example in Python:\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "reg = DecisionTreeRegressor(criterion='mse')  # Using MSE for regression\n"
      ],
      "metadata": {
        "id": "akYYi_lKPB-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.What is the mathematical formula for Gini Impurity\n",
        "Ans.The Gini Impurity measures how often a randomly chosen element from a set would be incorrectly classified if it were randomly labeled according to the distribution of labels in the set.\n",
        "\n",
        "The formula for Gini Impurity is:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "𝑐\n",
        "c = Number of classes\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = Probability of class\n",
        "𝑖\n",
        "i in the current node\n",
        "Example Calculation\n",
        "Suppose we have a dataset with two classes:\n",
        "\n",
        "Class A: 60% (0.6 probability)\n",
        "Class B: 40% (0.4 probability)\n",
        "The Gini Impurity is calculated as:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.6\n",
        "2\n",
        "+\n",
        "0.4\n",
        "2\n",
        ")\n",
        "Gini=1−(0.6\n",
        "2\n",
        " +0.4\n",
        "2\n",
        " )\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.36\n",
        "+\n",
        "0.16\n",
        ")\n",
        "=1−(0.36+0.16)\n",
        "=\n",
        "1\n",
        "−\n",
        "0.52\n",
        "=1−0.52\n",
        "=\n",
        "0.48\n",
        "=0.48\n",
        "Python Implementation\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "def gini_impurity(probabilities):\n",
        "    return 1 - sum(p**2 for p in probabilities)\n",
        "\n",
        "# Example: Two-class dataset (60% and 40%)\n",
        "gini = gini_impurity([0.6, 0.4])\n",
        "print(\"Gini Impurity:\", gini)  # Output: 0.48"
      ],
      "metadata": {
        "id": "fgFQBGDcP9hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What is the mathematical formula for Entropy\n",
        "Ans.Entropy measures the disorder (impurity) in a dataset. It is used in Decision Trees to decide the best feature to split on.\n",
        "\n",
        "The formula for Entropy is:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "where:\n",
        "\n",
        "𝑐\n",
        "c = Number of classes\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = Probability of class\n",
        "𝑖\n",
        "i in the current node\n",
        "log\n",
        "⁡\n",
        "2\n",
        "log\n",
        "2\n",
        "​\n",
        "  = Logarithm base 2\n",
        "Example Calculation\n",
        "Suppose we have a dataset with two classes:\n",
        "\n",
        "Class A: 60% (0.6 probability)\n",
        "Class B: 40% (0.4 probability)\n",
        "The Entropy is calculated as:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        "+\n",
        "0.4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        ")\n",
        "Entropy=−(0.6log\n",
        "2\n",
        "​\n",
        " 0.6+0.4log\n",
        "2\n",
        "​\n",
        " 0.4)\n",
        "=\n",
        "−\n",
        "(\n",
        "0.6\n",
        "×\n",
        "−\n",
        "0.736\n",
        ")\n",
        "−\n",
        "(\n",
        "0.4\n",
        "×\n",
        "−\n",
        "1.322\n",
        ")\n",
        "=−(0.6×−0.736)−(0.4×−1.322)\n",
        "=\n",
        "0.441\n",
        "+\n",
        "0.528\n",
        "=0.441+0.528\n",
        "=\n",
        "0.97\n",
        "=0.97\n",
        "Python Implementation\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def entropy(probabilities):\n",
        "    return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
        "\n",
        "# Example: Two-class dataset (60% and 40%)\n",
        "entropy_value = entropy([0.6, 0.4])"
      ],
      "metadata": {
        "id": "TSpPoTs9QLEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5.What is Information Gain, and how is it used in Decision Trees\n",
        "Ans.Information Gain (IG) measures how much uncertainty (entropy) is reduced after splitting a dataset based on a feature. It is used in Decision Trees to determine the best feature for splitting at each node.\n",
        "\n",
        "The formula for Information Gain is:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "(\n",
        "𝑁\n",
        "𝑖\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        "IG=Entropy\n",
        "parent\n",
        "​\n",
        " −\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " (\n",
        "N\n",
        "N\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy\n",
        "child\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " )\n",
        "where:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        "  = Entropy before splitting\n",
        "𝑘\n",
        "k = Number of child nodes after the split\n",
        "𝑁\n",
        "𝑖\n",
        "N\n",
        "i\n",
        "​\n",
        "  = Number of samples in child node\n",
        "𝑖\n",
        "i\n",
        "𝑁\n",
        "N = Total number of samples in the parent node\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        "Entropy\n",
        "child\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        "  = Entropy of each child node\n",
        "How is Information Gain Used in Decision Trees?\n",
        "Calculate the Entropy of the Parent Node\n",
        "Before splitting, find the entropy of the entire dataset.\n",
        "Split the Dataset on a Feature\n",
        "Partition the data based on different values of the feature.\n",
        "Calculate the Weighted Entropy of Child Nodes\n",
        "Compute the entropy for each child node and weight it by the proportion of samples in that node.\n",
        "Compute Information Gain\n",
        "Subtract the weighted sum of child entropies from the parent entropy.\n",
        "Choose the Feature with the Highest Information Gain\n",
        "The feature that maximizes Information Gain is selected for splitting.\n",
        "Example Calculation\n",
        "Suppose we have a dataset with 10 samples:\n",
        "\n",
        "Before Splitting:\n",
        "\n",
        "6 belong to Class A, 4 belong to Class B\n",
        "Entropy = 0.97\n",
        "After Splitting on Feature X:\n",
        "\n",
        "Left Child (4 samples) → 3A, 1B, Entropy = 0.81\n",
        "Right Child (6 samples) → 3A, 3B, Entropy = 1.0\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "0.97\n",
        "−\n",
        "(\n",
        "4\n",
        "10\n",
        "×\n",
        "0.81\n",
        "+\n",
        "6\n",
        "10\n",
        "×\n",
        "1.0\n",
        ")\n",
        "IG=0.97−(\n",
        "10\n",
        "4\n",
        "​\n",
        " ×0.81+\n",
        "10\n",
        "6\n",
        "​\n",
        " ×1.0)\n",
        "=\n",
        "0.97\n",
        "−\n",
        "(\n",
        "0.324\n",
        "+\n",
        "0.6\n",
        ")\n",
        "=0.97−(0.324+0.6)\n",
        "=\n",
        "0.97\n",
        "−\n",
        "0.924\n",
        "=\n",
        "0.046\n",
        "=0.97−0.924=0.046\n",
        "Since Information Gain is low, Feature X is not a good split.\n",
        "\n",
        "Python Implementation of Information Gain\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def entropy(probabilities):\n",
        "    return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
        "\n",
        "def information_gain(parent_probs, child_groups):\n",
        "    parent_entropy = entropy(parent_probs)\n",
        "    total_samples = sum(sum(group) for group in child_groups)\n",
        "    \n",
        "    weighted_entropy = sum(\n",
        "        (sum(group) / total_samples) * entropy([x / sum(group) for x in group if sum(group) > 0])\n",
        "        for group in child_groups\n",
        "    )\n",
        "\n",
        "    return parent_entropy - weighted_entropy\n",
        "\n",
        "# Example: Parent node with (6A, 4B)\n",
        "parent_probs = [6/10, 4/10]\n",
        "\n",
        "# Split into two child nodes: (3A, 1B) and (3A, 3B)\n",
        "child_groups = [[3, 1], [3, 3]]\n",
        "\n",
        "ig = information_gain(parent_probs, child_groups)\n",
        "print(\"Information Gain:\", ig)  # Output: 0.046\n"
      ],
      "metadata": {
        "id": "Ly1wqiTaQXYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6.What is the difference between Gini Impurity and Entropy\n",
        "Ans.Both Gini Impurity and Entropy are impurity measures used in Decision Trees to determine the best feature for splitting data. However, they have key differences in calculation, interpretation, and behavior.\n",
        "\n",
        "1. Formula Comparison\n",
        "Measure\tFormula\tInterpretation\n",
        "Gini Impurity\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        " \tProbability of incorrectly classifying a randomly chosen element.\n",
        "Entropy\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\tMeasure of disorder in the dataset.\n",
        "2. Interpretation\n",
        "Gini Impurity:\n",
        "Measures how often a randomly chosen element would be misclassified if randomly labeled.\n",
        "Lower Gini = Purer node.\n",
        "Entropy:\n",
        "Measures the uncertainty in a dataset.\n",
        "Higher entropy means the data is more disordered, and lower entropy means it is more pure.\n",
        "3. Range of Values\n",
        "Measure\tMinimum Value (Pure Split)\tMaximum Value (Highest Impurity)\n",
        "Gini Impurity\t0 (Pure class)\t0.5 (Two equal classes)\n",
        "Entropy\t0 (Pure class)\t1 (Two equal classes)\n",
        "4. Computational Efficiency\n",
        "Gini is faster than Entropy because it does not require computing logarithms.\n",
        "Entropy is more mathematically rigorous, but it may lead to deeper trees compared to Gini.\n",
        "5. Example Calculation\n",
        "Example: Two-Class Dataset (60% A, 40% B)\n",
        "Gini Impurity Calculation:\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.6\n",
        "2\n",
        "+\n",
        "0.4\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.36\n",
        "+\n",
        "0.16\n",
        ")\n",
        "=\n",
        "0.48\n",
        "Gini=1−(0.6\n",
        "2\n",
        " +0.4\n",
        "2\n",
        " )=1−(0.36+0.16)=0.48\n",
        "Entropy Calculation:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.6\n",
        "+\n",
        "0.4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "0.4\n",
        ")\n",
        "=\n",
        "0.97\n",
        "Entropy=−(0.6log\n",
        "2\n",
        "​\n",
        " 0.6+0.4log\n",
        "2\n",
        "​\n",
        " 0.4)=0.97\n",
        "6. Python Implementation\n",
        "python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def gini_impurity(probabilities):\n",
        "    return 1 - sum(p**2 for p in probabilities)\n",
        "\n",
        "def entropy(probabilities):\n",
        "    return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
        "\n",
        "# Example: Two-class dataset (60% and 40%)\n",
        "probabilities = [0.6, 0.4]\n",
        "\n",
        "gini_value = gini_impurity(probabilities)\n",
        "entropy_value = entropy(probabilities)\n",
        "\n",
        "print(\"Gini Impurity:\", gini_value)  # Output: 0.48\n",
        "print(\"Entropy:\", entropy_value)  # Output: 0.97\n",
        "7. When to Use Gini or Entropy?\n",
        "Use Case\tRecommended Measure\n",
        "Faster computation\tGini Impurity\n",
        "More mathematically rigorous\tEntropy\n",
        "Similar classification performance\tBoth work well\n",
        "In scikit-learn, DecisionTreeClassifier defaults to Gini Impurity because it is computationally faster."
      ],
      "metadata": {
        "id": "6_hCl53sQp2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.What is the mathematical explanation behind Decision Trees\n",
        "Ans.A Decision Tree is a hierarchical model used for classification and regression tasks. It works by recursively splitting the dataset based on impurity measures such as Gini Impurity or Entropy (Information Gain).\n",
        "\n",
        "1. Splitting Criterion\n",
        "At each node, the algorithm chooses the best feature to split on by minimizing impurity.\n",
        "\n",
        "1.1 Entropy & Information Gain\n",
        "Entropy measures disorder (impurity) in a dataset:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the probability of class\n",
        "𝑖\n",
        "i.\n",
        "\n",
        "Information Gain (IG):\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "(\n",
        "𝑁\n",
        "𝑖\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        "IG=Entropy\n",
        "parent\n",
        "​\n",
        " −\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "​\n",
        " (\n",
        "N\n",
        "N\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy\n",
        "child\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " )\n",
        "A feature with higher IG is chosen for splitting.\n",
        "\n",
        "1.2 Gini Impurity\n",
        "Gini Impurity measures the probability of incorrect classification:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "A lower Gini value indicates a better split.\n",
        "\n",
        "2. Recursive Partitioning (Splitting Process)\n",
        "The dataset is recursively divided into subsets:\n",
        "\n",
        "Calculate impurity (Entropy or Gini) at the current node.\n",
        "Choose the feature that minimizes impurity (maximizes Information Gain).\n",
        "Split the data based on the chosen feature.\n",
        "Repeat until a stopping condition is met (e.g., max depth, min samples per node).\n",
        "3. Stopping Criteria (Preventing Overfitting)\n",
        "Max Depth (\n",
        "𝑑\n",
        "d): Stops splitting after a fixed depth.\n",
        "Min Samples per Leaf (\n",
        "𝑛\n",
        "n): A node must have at least\n",
        "𝑛\n",
        "n samples to split.\n",
        "Min Information Gain: Stop if IG is too low.\n",
        "Pruning: Removes unnecessary branches.\n",
        "4. Decision Rule (Making Predictions)\n",
        "In classification, the final leaf node assigns a majority class label.\n",
        "In regression, the leaf node outputs the mean value of the target variable.\n",
        "𝑦\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "leaf\n",
        "​\n",
        " =\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " y\n",
        "i\n",
        "​\n",
        "\n",
        "5. Python Implementation of Decision Trees\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Visualize the Decision Tree\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "plt.show()\n",
        "6. Summary Table\n",
        "Concept\tFormula\tPurpose\n",
        "Entropy\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "𝑝\n",
        "𝑖\n",
        "−∑p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " \tMeasures disorder in a dataset\n",
        "Gini Impurity\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "1−∑p\n",
        "i\n",
        "2\n",
        "​\n",
        " \tMeasures probability of misclassification\n",
        "Information Gain\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "𝑊\n",
        "𝑒\n",
        "𝑖\n",
        "𝑔\n",
        "ℎ\n",
        "𝑡\n",
        "𝑒\n",
        "𝑑\n",
        "\n",
        "𝑆\n",
        "𝑢\n",
        "𝑚\n",
        "\n",
        "𝑜\n",
        "𝑓\n",
        "\n",
        "𝐶\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑖\n",
        "𝑒\n",
        "𝑠\n",
        "IG=Entropy\n",
        "parent\n",
        "​\n",
        " −Weighted Sum of Child Entropies\tDetermines the best feature for splitting\n",
        "Stopping Condition\tMax depth, min samples per leaf, pruning\tPrevents overfitting"
      ],
      "metadata": {
        "id": "3sSjJH0kRAkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8.What is Pre-Pruning in Decision Trees\n",
        "Ans.Pre-pruning (Early Stopping) is a technique used to stop the growth of a Decision Tree early to prevent overfitting. It imposes constraints before the tree fully grows.\n",
        "\n",
        "Why Pre-Pruning?\n",
        "If a Decision Tree grows too deep:\n",
        " It fits the training data perfectly.\n",
        " But it may overfit and perform poorly on unseen data.\n",
        "\n",
        "Pre-pruning limits tree complexity by stopping splits based on predefined conditions.\n",
        "\n",
        "Pre-Pruning Techniques in Decision Trees\n",
        "Pre-Pruning Method\tDescription\tEffect\n",
        "Max Depth (\n",
        "𝑑\n",
        "d)\tLimits the tree's depth\tPrevents excessive branching\n",
        "Min Samples Split (\n",
        "𝑛\n",
        "n)\tMinimum samples required to split a node\tAvoids splitting small datasets\n",
        "Min Samples Leaf (\n",
        "𝑛\n",
        "n)\tMinimum samples required in a leaf node\tPrevents creation of small, biased leaves\n",
        "Min Impurity Decrease\tStops splitting if impurity reduction is too small\tPrevents unnecessary splits\n",
        "Mathematical Explanation\n",
        "A split occurs only if:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "(\n",
        "𝑁\n",
        "𝑖\n",
        "𝑁\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        ">\n",
        "Threshold\n",
        "IG=Entropy\n",
        "parent\n",
        "​\n",
        " −∑(\n",
        "N\n",
        "N\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " ×Entropy\n",
        "child\n",
        "i\n",
        "​\n",
        "\n",
        "​\n",
        " )>Threshold\n",
        "Where:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "IG = Information Gain\n",
        "𝑁\n",
        "𝑖\n",
        "N\n",
        "i\n",
        "​\n",
        "  = Samples in child node\n",
        "𝑁\n",
        "N = Total samples in parent node\n",
        "Threshold is set by min_impurity_decrease.\n",
        "If IG is too low, the tree stops growing at that point.\n",
        "\n",
        "Python Implementation of Pre-Pruning\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create Decision Tree with Pre-Pruning\n",
        "clf = DecisionTreeClassifier(\n",
        "    criterion='gini',   # Use Gini Impurity\n",
        "    max_depth=3,        # Limit tree depth\n",
        "    min_samples_split=5, # Minimum 5 samples to split\n",
        "    min_samples_leaf=2,  # Minimum 2 samples per leaf\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "clf.fit(X, y)\n",
        "Advantages of Pre-Pruning\n",
        " Reduces Overfitting – Prevents learning noise from training data.\n",
        " Improves Generalization – Works better on unseen data.\n",
        " Faster Training – Limits tree complexity.\n",
        "\n",
        "Disadvantages of Pre-Pruning\n",
        " May stop splitting too early, missing important patterns.\n",
        "Needs careful tuning of parameters.\n",
        "\n",
        "Pre-Pruning vs Post-Pruning\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "When applied?\tBefore growing the tree\tAfter full tree growth\n",
        "Purpose\tPrevents overfitting early\tRemoves unnecessary branches\n",
        "Risk\tMay underfit if too restrictive\tMay still overfit before pruning"
      ],
      "metadata": {
        "id": "AJcpy3MkReI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9.What is Post-Pruning in Decision Trees\n",
        "Ans.Post-pruning (Pruning after Training) is a technique used to reduce overfitting by removing unnecessary branches from a fully grown Decision Tree after it has been trained.\n",
        "\n",
        "Why Post-Pruning?\n",
        "A fully grown Decision Tree can:\n",
        " Fit the training data perfectly\n",
        " Capture noise and overfit on unseen data\n",
        "\n",
        "Post-pruning removes weak branches, making the model more generalized and improving accuracy on test data.\n",
        "\n",
        "Post-Pruning Techniques\n",
        "Post-Pruning Method\tDescription\tEffect\n",
        "Cost Complexity Pruning (CCP)\tRemoves nodes that don’t significantly reduce error\tSimplifies the tree\n",
        "Reduced Error Pruning\tPrunes a node if removing it does not increase test error\tPrevents overfitting\n",
        "Minimal Error Pruning\tUses cross-validation to decide pruning\tFinds optimal tree depth\n",
        "Mathematical Explanation\n",
        "1. Cost Complexity Pruning (CCP)\n",
        "The idea is to minimize:\n",
        "\n",
        "𝑅\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "+\n",
        "𝛼\n",
        "×\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "R(T)+α×∣T∣\n",
        "Where:\n",
        "\n",
        "𝑅\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "R(T) = Classification error of the tree\n",
        "𝑇\n",
        "T\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "∣T∣ = Number of terminal nodes (leaves)\n",
        "𝛼\n",
        "α = Pruning parameter (controls tree complexity)\n",
        "A node is removed if pruning reduces the cost function.\n",
        "\n",
        "Python Implementation of Post-Pruning\n",
        "Step 1: Train a Full Decision Tree\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train full tree\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", ccp_alpha=0.0)  # No pruning initially\n",
        "clf.fit(X_train, y_train)\n",
        "Step 2: Find the Best Pruning Parameter (\n",
        "𝛼\n",
        "α)\n",
        "python\n",
        "\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # Get a list of possible alpha values\n",
        "\n",
        "# Try different pruning levels and find the best one\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    print(f\"Alpha: {alpha}, Accuracy: {clf_pruned.score(X_test, y_test)}\")\n",
        "Step 3: Train the Pruned Tree\n",
        "python\n",
        "\n",
        "best_alpha = 0.01  # Choose the best alpha from the above step\n",
        "pruned_clf = DecisionTreeClassifier(ccp_alpha=best_alpha)\n",
        "pruned_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Pruned Tree Accuracy:\", pruned_clf.score(X_test, y_test))\n",
        "Advantages of Post-Pruning\n",
        "Better Generalization – Reduces overfitting by removing unnecessary splits.\n",
        " More Reliable – Uses real test data to decide pruning.\n",
        "Improves Performance – Creates smaller, interpretable trees.\n",
        "\n",
        "Disadvantages of Post-Pruning\n",
        " Computationally Expensive – Requires extra validation/testing.\n",
        " Choosing\n",
        "𝛼\n",
        "α is tricky – Needs cross-validation for best results."
      ],
      "metadata": {
        "id": "JlZkkYOvR6qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.What is the difference between Pre-Pruning and Post-Pruning\n",
        "Ans.Both Pre-Pruning and Post-Pruning are techniques to prevent overfitting in Decision Trees by controlling tree growth. However, they differ in when and how pruning is applied.\n",
        "\n",
        "1. Key Differences\n",
        "Feature\tPre-Pruning (Early Stopping)\tPost-Pruning (Pruning After Training)\n",
        "When is it applied?\tDuring tree construction\tAfter the tree is fully grown\n",
        "How it works?\tStops splitting if conditions are met\tGrows full tree, then removes unnecessary nodes\n",
        "Stopping Criteria\tMax depth, min samples per leaf, min impurity decrease\tCost complexity pruning (CCP), validation set pruning\n",
        "Risk\tUnderfitting if tree stops growing too early\tBetter generalization since the tree is fully grown before pruning\n",
        "Computational Cost\tFaster, requires fewer calculations\tMore expensive, requires cross-validation\n",
        "Accuracy Impact\tCan reduce accuracy if too strict\tTypically leads to better accuracy\n",
        "2. When to Use Which?\n",
        "Scenario\tRecommended Pruning Method\n",
        "Large dataset, need faster training\tPre-Pruning (limits depth)\n",
        "Overfitting on training data\tPost-Pruning (removes unnecessary nodes)\n",
        "Uncertain about tree depth\tPost-Pruning (allows full growth, then prunes)\n",
        "3. Python Implementation Example\n",
        "Pre-Pruning (Limit Tree Growth)\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Apply Pre-Pruning with max depth and min samples per split\n",
        "clf_pre = DecisionTreeClassifier(max_depth=3, min_samples_split=5, min_samples_leaf=2)\n",
        "clf_pre.fit(X_train, y_train)\n",
        "Post-Pruning (Cost Complexity Pruning)\n",
        "python\n",
        "\n",
        "# Train a full tree first\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Get pruning path\n",
        "path = clf_full.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "# Train multiple pruned trees and select the best\n",
        "best_alpha = 0.01  # Found via cross-validation\n",
        "clf_post = DecisionTreeClassifier(ccp_alpha=best_alpha)\n",
        "clf_post.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "P56N7VB9R2NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11.What is a Decision Tree Regressor\n",
        "Ans.A Decision Tree Regressor is a type of Decision Tree model used for regression tasks, meaning it predicts continuous numerical values rather than discrete classes. Instead of classifying data, it splits the dataset into regions and assigns an average value for each region.\n",
        "\n",
        "1. How Does a Decision Tree Regressor Work?\n",
        "Splitting the Data\n",
        "\n",
        "The algorithm recursively splits the dataset into subsets using a chosen feature.\n",
        "The split is chosen to minimize variance (impurity) in the target variable.\n",
        "Stopping Criteria\n",
        "\n",
        "The recursion stops when a stopping criterion is met (e.g., max depth, min samples per leaf).\n",
        "Prediction\n",
        "\n",
        "Each leaf node stores the mean of the target values in that region.\n",
        "2. Impurity Measure: Mean Squared Error (MSE)\n",
        "Unlike classification trees, which use Gini Impurity or Entropy, regression trees use Mean Squared Error (MSE) to determine the best split:\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "N\n",
        "1\n",
        "​\n",
        "  \n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "^\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "where:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  is the actual value\n",
        "𝑦\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  is the predicted mean value of the node\n",
        "𝑁\n",
        "N is the number of samples in the node\n",
        "The tree selects the feature and split that minimizes the total MSE.\n",
        "\n",
        "3. Python Implementation\n",
        "Example: Predicting House Prices\n",
        "python\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset (House size vs Price)\n",
        "X = np.array([[600], [800], [1000], [1200], [1500], [1800], [2000]])\n",
        "y = np.array([150, 200, 250, 300, 350, 400, 450])  # Prices in $1000s\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(max_depth=3)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "4. Visualization of Decision Tree Regressor\n",
        "Decision Trees create stepwise constant approximations of the target function. Below is an example of how a Decision Tree regressor might approximate a function:\n",
        "\n",
        "python\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Decision Tree Predictions\n",
        "X_range = np.linspace(500, 2100, 100).reshape(-1, 1)\n",
        "y_pred_range = regressor.predict(X_range)\n",
        "\n",
        "plt.scatter(X, y, color=\"red\", label=\"Actual Data\")\n",
        "plt.plot(X_range, y_pred_range, color=\"blue\", label=\"Decision Tree Prediction\")\n",
        "plt.xlabel(\"House Size (sq ft)\")\n",
        "plt.ylabel(\"Price ($1000s)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "5. Advantages of Decision Tree Regression\n",
        " Non-Linear Relationships – Can model non-linear dependencies between features and target variables.\n",
        " No Assumptions Needed – Unlike linear regression, it doesn't assume a specific relationship.\n",
        " Handles Categorical & Numerical Data – Works with both feature types.\n",
        "\n",
        "6. Disadvantages\n",
        " Overfitting – Trees can grow too deep, capturing noise rather than trends.\n",
        " Not Smooth – Predictions are piecewise constant, which may not capture trends well.\n",
        " Sensitive to Small Changes – Small variations in data can lead to different trees.\n",
        "\n",
        "7. Pruning for Decision Tree Regressors\n",
        "To prevent overfitting, we can use pre-pruning or post-pruning:\n",
        "\n",
        "python\n",
        "\n",
        "regressor = DecisionTreeRegressor(max_depth=3, min_samples_leaf=5)\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "haVDkEbwTZkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12.What are the advantages and disadvantages of Decision Trees\n",
        "Ans.Decision Trees are widely used for classification and regression tasks due to their simplicity and interpretability. However, they also have limitations.\n",
        "\n",
        "1. Advantages of Decision Trees\n",
        " 1. Easy to Understand & Interpret\n",
        "Decision Trees mimic human decision-making, making them easy to explain.\n",
        "You can visualize the tree structure.\n",
        " 2. Handles Both Numerical & Categorical Data\n",
        "Can be used for classification (categorical outputs) and regression (continuous outputs).\n",
        "Unlike linear regression, no need to transform categorical variables.\n",
        " 3. No Need for Feature Scaling\n",
        "Unlike algorithms like SVM or k-NN, no need for normalization or standardization.\n",
        " 4. Handles Missing Values\n",
        "Can split data even with missing values, unlike some models that require imputation.\n",
        " 5. Works Well with Non-Linear Relationships\n",
        "Captures non-linear patterns better than linear models.\n",
        " 6. Automatic Feature Selection\n",
        "Chooses the most important features automatically during splitting.\n",
        " 7. Fast Training & Prediction\n",
        "Computationally efficient for small datasets.\n",
        "2. Disadvantages of Decision Trees\n",
        " 1. Overfitting (High Variance)\n",
        "Deep trees can memorize training data, leading to poor generalization on unseen data.\n",
        "Solution: Use pruning (pre-pruning or post-pruning) or limit tree depth.\n",
        " 2. Sensitive to Small Changes in Data (Unstable Model)\n",
        "Small variations in data can lead to completely different trees.\n",
        "Solution: Use ensemble methods like Random Forest or Gradient Boosting.\n",
        " 3. Greedy Algorithm (Locally Optimal, Not Globally Optimal)\n",
        "The algorithm chooses the best split at each step but may not find the best overall tree.\n",
        "Solution: Use Random Forest to reduce dependency on single splits.\n",
        " 4. Biased with Imbalanced Data\n",
        "If some classes are more frequent than others, the tree may favor those classes.\n",
        "Solution: Use class weighting or resampling techniques.\n",
        "5. Large Trees Become Hard to Interpret\n",
        "A very deep tree can have hundreds of nodes, making it difficult to understand.\n",
        "Solution: Use feature selection or pruning."
      ],
      "metadata": {
        "id": "cyGZF43MT_HE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13.How does a Decision Tree handle missing values\n",
        "Ans.Decision Trees can handle missing values in various ways during training and prediction. Unlike some models (e.g., linear regression), Decision Trees can split data even if some feature values are missing.\n",
        "\n",
        "1. Handling Missing Values During Training\n",
        " 1. Ignoring Missing Values in Splitting\n",
        "Scikit-learn’s DecisionTreeClassifier does NOT handle missing values directly, so missing values need to be imputed first.\n",
        "If using a Decision Tree from other libraries (e.g., XGBoost, LightGBM), the tree can find the best split even if some values are missing.\n",
        "Solution:\n",
        "\n",
        "Fill missing values with the most frequent category (categorical data) or mean/median (numerical data).\n",
        "Use the SimpleImputer class from scikit-learn.\n",
        "Example: Imputing Missing Values Before Training\n",
        "\n",
        "python\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset with missing values (NaN)\n",
        "X = np.array([[1, 2], [3, np.nan], [5, 6], [np.nan, 8]])\n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "# Fill missing values using mean imputation\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_imputed, y)\n",
        " 2. Surrogate Splits (Used in Some Libraries like C4.5, CART)\n",
        "If a value is missing for the primary splitting feature, the tree can use an alternative (surrogate) feature that correlates with the primary feature.\n",
        "This avoids discarding the sample entirely.\n",
        " Scikit-learn does NOT use surrogate splits, but other libraries like C4.5 (used in Weka) and CART implement this.\n",
        "\n",
        "2. Handling Missing Values During Prediction\n",
        " 1. Assigning the Most Frequent Class (Classification) or Mean Value (Regression)\n",
        "If a missing value appears in a test sample, the tree can:\n",
        "\n",
        "Assign the most frequent class (for classification).\n",
        "Assign the average target value (for regression).\n",
        " 2. Dropping Features with High Missing Rates\n",
        "If a feature has too many missing values, it might not be useful for decision-making and can be excluded from training.\n",
        "3. Practical Considerations\n",
        "Method\tPros\tCons\n",
        "Mean/Median Imputation\tSimple, widely used\tCan introduce bias\n",
        "Most Frequent Value\tWorks well for categorical data\tCan distort class distribution\n",
        "Surrogate Splits\tKeeps missing data without discarding samples\tNot supported in scikit-learn\n",
        "Dropping Missing Features\tReduces complexity\tRisk of losing useful information\n",
        "4. Alternative: Using Random Forest or XGBoost\n",
        "Random Forest can handle missing values better by averaging multiple trees.\n",
        "XGBoost automatically learns the best direction for missing values instead of filling them manually."
      ],
      "metadata": {
        "id": "Q3B4BcR6U4Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14.How does a Decision Tree handle categorical features\n",
        "Ans.Decision Trees can handle categorical features (like \"Color = Red, Blue, Green\") differently depending on the implementation.\n",
        "\n",
        "1. Methods to Handle Categorical Features\n",
        " 1. Label Encoding (Ordinal Encoding)\n",
        "Assigns numerical labels to categories (e.g., \"Red\" → 0, \"Blue\" → 1, \"Green\" → 2).\n",
        "Works well if the categories have an inherent order (e.g., \"Low\" < \"Medium\" < \"High\").\n",
        "Risk: Can introduce incorrect numerical relationships if no natural order exists.\n",
        "Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = [\"Red\", \"Blue\", \"Green\", \"Blue\", \"Red\"]\n",
        "encoder = LabelEncoder()\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)  # Output: [2, 0, 1, 0, 2]\n",
        " 2. One-Hot Encoding (Dummy Variables)\n",
        "Creates binary columns for each category (e.g., \"Red\" → [1,0,0], \"Blue\" → [0,1,0], \"Green\" → [0,0,1]).\n",
        "Prevents false ordinal relationships introduced by Label Encoding.\n",
        "Risk: Increases the number of features (high dimensionality) if many categories exist.\n",
        "Example in Python:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[\"Red\"], [\"Blue\"], [\"Green\"], [\"Blue\"], [\"Red\"]])\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)\n",
        "Output:\n",
        "\n",
        "ini\n",
        "\n",
        "[[0. 0. 1.]  # Red\n",
        " [1. 0. 0.]  # Blue\n",
        " [0. 1. 0.]  # Green\n",
        " [1. 0. 0.]  # Blue\n",
        " [0. 0. 1.]] # Red\n",
        " 3. Decision Trees with Native Categorical Support\n",
        "Some implementations (e.g., LightGBM, CatBoost) directly handle categorical variables without encoding.\n",
        "Avoids creating unnecessary splits.\n",
        "Scikit-learn does NOT support categorical features natively (must use encoding first).\n",
        "Example (LightGBM):\n",
        "\n",
        "python\n",
        "\n",
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\"Color\": [\"Red\", \"Blue\", \"Green\", \"Blue\", \"Red\"], \"Size\": [10, 15, 20, 15, 10]})\n",
        "data[\"Color\"] = data[\"Color\"].astype(\"category\")  # Specify categorical column\n",
        "\n",
        "model = lgb.LGBMClassifier()\n",
        "model.fit(data[[\"Color\", \"Size\"]], [0, 1, 0, 1, 0])  # Train model\n",
        "2. Which Encoding Method Should You Use?\n",
        "Method\tWhen to Use\tPros\tCons\n",
        "Label Encoding\tWhen categories have an order\tSimple, efficient\tImplies false ranking if no order exists\n",
        "One-Hot Encoding\tWhen categories have no order\tAvoids false ranking\tCan create too many features\n",
        "Native Categorical Support (LightGBM, CatBoost)\tLarge datasets with categorical features\tAutomatically handles categories\tNot supported in scikit-learn\n",
        "3. Example: Using Categorical Features in a Decision Tree (Scikit-learn)\n",
        "Since scikit-learn does not support categorical features directly, you must encode them first:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([[\"Red\"], [\"Blue\"], [\"Green\"], [\"Blue\"], [\"Red\"]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# One-Hot Encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_encoded, y)\n"
      ],
      "metadata": {
        "id": "NX_MTwCmVP57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15.What are some real-world applications of Decision Trees?\n",
        "Ans.Decision Trees are widely used in various industries for classification and regression problems. Their ability to handle both numerical and categorical data makes them highly versatile.\n",
        "\n",
        "1. Healthcare\n",
        " Disease Diagnosis\n",
        "Used to classify whether a patient has a disease based on symptoms.\n",
        "Example: Predicting diabetes or heart disease based on patient records.\n",
        "Example:\n",
        "\n",
        "python\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Features: Age, Blood Pressure, Sugar Level\n",
        "X = [[45, 120, 85], [50, 140, 95], [60, 130, 110], [30, 110, 75]]\n",
        "y = [1, 1, 1, 0]  # 1 = Diabetic, 0 = Non-Diabetic\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "2. Finance & Banking\n",
        " Credit Scoring & Loan Approval\n",
        "Determines whether a person is eligible for a loan based on factors like income, credit score, and loan history.\n",
        "Example: Banks use decision trees to classify loan defaulters vs. non-defaulters.\n",
        " Fraud Detection\n",
        "Detects unusual transactions to flag potential fraud.\n",
        "Example: Credit card fraud detection based on spending patterns.\n",
        "3. Marketing & Sales\n",
        " Customer Segmentation\n",
        "Classifies customers based on their purchasing behavior.\n",
        "Example: Retail companies identify high-value customers for targeted ads.\n",
        " Recommendation Systems\n",
        "Predicts which products a user is likely to buy based on past purchases.\n",
        "Example:\n",
        "\n",
        "Amazon suggests products using decision tree-based models like XGBoost.\n",
        "4. Manufacturing & Quality Control\n",
        " Defect Detection in Products\n",
        "Used in factories to classify defective vs. non-defective products.\n",
        " Predictive Maintenance\n",
        "Predicts when a machine is likely to fail based on sensor data.\n",
        "5. Human Resources (HR)\n",
        " Employee Attrition Prediction\n",
        "Predicts whether an employee is likely to leave a company based on factors like salary, work hours, and promotions.\n",
        "6. Agriculture\n",
        " Crop Disease Prediction\n",
        "Predicts crop diseases based on environmental conditions.\n",
        " Yield Prediction\n",
        "Helps farmers estimate crop yield based on soil quality and weather data.\n",
        "7. Autonomous Vehicles\n",
        " Traffic Sign Recognition\n",
        "Used in self-driving cars to classify traffic signs.\n",
        "8. Education\n",
        "Student Performance Prediction\n",
        "Predicts whether a student will pass/fail based on attendance and study time.\n"
      ],
      "metadata": {
        "id": "SOi1lKMdVvQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical**"
      ],
      "metadata": {
        "id": "5Rz_WoWuWS86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16.Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy*\n",
        "Ans."
      ],
      "metadata": {
        "id": "2tz6ythQWWNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "xzcq3p2-WgAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17.Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances*\n",
        "Ans."
      ],
      "metadata": {
        "id": "ku7NSIS9Wg4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print Feature Importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_impor\n"
      ],
      "metadata": {
        "id": "Mt3FJvaQWn4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18.Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy*\n",
        "Ans."
      ],
      "metadata": {
        "id": "479gK1T0Wop8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with Entropy\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Gp0ztLWeWyAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*\n",
        "Ans."
      ],
      "metadata": {
        "id": "4OeiR6P6WzBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # Features and target (house prices)\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "mkKA31F0W6I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20.Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "Ans"
      ],
      "metadata": {
        "id": "pisxoDnmXCRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Generate Graphviz visualization\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True, rounded=True, special_characters=True\n",
        ")\n",
        "\n",
        "# Render and display the tree\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.view()  # Opens the tree visualization\n"
      ],
      "metadata": {
        "id": "RV9x1q9gXO-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21.Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree\n",
        "Ans."
      ],
      "metadata": {
        "id": "YJ9sz6bxXPxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max depth = 3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train Fully Grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)  # No max_depth\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "CqivUekiXaSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree*\n",
        "Ans."
      ],
      "metadata": {
        "id": "J7BfUvlXXdSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree with min_samples_split=5\n",
        "clf_limited = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train Default Decision Tree\n",
        "clf_default = DecisionTreeClassifier(random_state=42)  # Default parameters\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "id": "QI7AJIm4YJ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23.Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*\n",
        "Ans."
      ],
      "metadata": {
        "id": "aKB4sb5hYLnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree without feature scaling\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Decision Tree with feature scaling\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy without feature scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with feature scaling: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "XzBVl8P4YVPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q24.Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification*\n",
        "Ans."
      ],
      "metadata": {
        "id": "X_rcVvjmYaBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use One-vs-Rest (OvR) with Decision Tree Classifier\n",
        "clf_ovr = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "clf_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf_ovr.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy using One-vs-Rest (OvR): {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "cJQH_c1LYf_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q25.Write a Python program to train a Decision Tree Classifier and display the feature importance scores*\n",
        "Ans."
      ],
      "metadata": {
        "id": "Qr9AnuAAYmfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importance = clf.feature_importances_\n",
        "\n",
        "# Display feature importance\n",
        "for feature, importance in zip(iris.feature_names, feature_importance):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(iris.feature_names, feature_importance, color='skyblue')\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.title(\"Feature Importance in Decision Tree\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y6QO96KFZQao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q26.Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*\n",
        "Ans."
      ],
      "metadata": {
        "id": "0P0rz2UyaXsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth=5\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train Fully Grown Decision Tree Regressor (no depth limit)\n",
        "reg_full = DecisionTreeRegressor(random_state=42)  # No max_depth\n",
        "reg_full.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Print MSE comparison\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"MSE with fully grown tree: {mse_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "aUFX9ULtaoDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q27.Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy*\n",
        "ANs."
      ],
      "metadata": {
        "id": "S6MatdWRapKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree without pruning\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get Cost Complexity Pruning path (alpha values)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # List of alpha values\n",
        "\n",
        "# Store accuracy for different alpha values\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "# Train Decision Tree Classifier for each alpha value\n",
        "for alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on train & test data\n",
        "    y_train_pred = clf_pruned.predict(X_train)\n",
        "    y_test_pred = clf_pruned.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_scores.append(accuracy_score(y_train, y_train_pred))\n",
        "    test_scores.append(accuracy_score(y_test, y_test_pred))\n",
        "\n",
        "# Plot accuracy vs. ccp_alpha\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ccp_alphas, train_scores, marker='o', label=\"Training Accuracy\", color='blue')\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', label=\"Testing Accuracy\", color='red')\n",
        "plt.xlabel(\"Cost Complexity Pruning (ccp_alpha)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Decision Tree Accuracy\")\n",
        "plt.legend()\n",
        "plt.xscale(\"log\")  # Log scale for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Me17pzpwa2i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q28.Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score*\n",
        "Ans."
      ],
      "metadata": {
        "id": "NR-yJRGca7uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute Precision, Recall, and F1-Score\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "kOnazaCrbbM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q29.Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*\n",
        "Ans."
      ],
      "metadata": {
        "id": "DOFWLOL-bcH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix using Seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Decision Tree Classifier\")\n",
        "plt.show()\n",
        "\n",
        "# Print Classification Report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "B_uNkQVabrRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q30.Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split.\n",
        "Ans."
      ],
      "metadata": {
        "id": "OQmNmnGMbsD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target  # Features and labels\n",
        "\n",
        "# Split data into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for max_depth and min_samples_split\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],  # Different tree depths\n",
        "    'min_samples_split': [2, 5, 10]  # Minimum samples required to split a node\n",
        "}\n",
        "\n",
        "# Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model parameters\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "2Q0YqYjtcYp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}