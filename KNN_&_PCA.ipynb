{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is K-Nearest Neighbors (KNN) and how does it work?\n",
        "Ans.KNN is a lazy learning algorithm, meaning it doesn't learn a model during training. Instead, it memorizes the training dataset and uses it when making predictions.\n",
        "\n",
        "How Does KNN Work?\n",
        "Here's a step-by-step breakdown of how KNN works:\n",
        "\n",
        "Choose the number of neighbors (K): This is a user-defined constant. It represents how many nearby data points will be considered to make a prediction.\n",
        "\n",
        "Calculate distance: When given a new data point to classify, KNN calculates the distance (commonly Euclidean distance) between the new point and all points in the training dataset.\n",
        "\n",
        "EuclideanÂ Distance\n",
        "=\n",
        "(\n",
        "ð‘¥\n",
        "1\n",
        "âˆ’\n",
        "ð‘¦\n",
        "1\n",
        ")\n",
        "2\n",
        "+\n",
        "(\n",
        "ð‘¥\n",
        "2\n",
        "âˆ’\n",
        "ð‘¦\n",
        "2\n",
        ")\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "(\n",
        "ð‘¥\n",
        "ð‘›\n",
        "âˆ’\n",
        "ð‘¦\n",
        "ð‘›\n",
        ")\n",
        "2\n",
        "EuclideanÂ Distance=\n",
        "(x\n",
        "1\n",
        "â€‹\n",
        " âˆ’y\n",
        "1\n",
        "â€‹\n",
        " )\n",
        "2\n",
        " +(x\n",
        "2\n",
        "â€‹\n",
        " âˆ’y\n",
        "2\n",
        "â€‹\n",
        " )\n",
        "2\n",
        " +â‹¯+(x\n",
        "n\n",
        "â€‹\n",
        " âˆ’y\n",
        "n\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "â€‹\n",
        "\n",
        "Find the K nearest neighbors: Sort the distances and select the K closest data points.\n",
        "\n",
        "Make a prediction:\n",
        "\n",
        "Classification: The class most common among the K nearest neighbors is assigned to the new point (majority vote).\n",
        "\n",
        "Regression: The average (or weighted average) of the values of the K nearest neighbors is used.\n",
        " Example (Classification)\n",
        "Suppose you're classifying fruits based on size and color. You want to classify a new fruit. KNN:\n",
        "\n",
        "Checks the distance of this new fruit to every fruit in the training set.\n",
        "\n",
        "Picks the K closest fruits.\n",
        "\n",
        "Assigns the most frequent fruit type among those K neighbors.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbpTX_C8zoQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between KNN Classification and KNN Regression?\n",
        "Ans.K-Nearest Neighbors (KNN) uses the same underlying algorithm for both classification and regression, the key difference lies in how the prediction is made based on the K nearest neighbors.\n",
        "\n",
        "ðŸ§© Difference Between KNN Classification and KNN Regression\n",
        "Aspect\tKNN Classification\tKNN Regression\n",
        "Prediction Type\tPredicts a class/label\tPredicts a continuous value\n",
        "Output\tDiscrete (e.g., \"dog\", \"cat\", \"apple\")\tContinuous (e.g., 42.7, 15.3)\n",
        "Decision Rule\tMajority vote among K neighbors\tAverage (or weighted average) of neighbors' values\n",
        "Loss Function\tClassification error (e.g., accuracy)\tRegression error (e.g., MSE, MAE)\n",
        "Use Case Examples\tEmail spam detection, disease diagnosis\tPredicting house prices, stock forecasting\n",
        "Boundary Behavior\tCreates distinct class regions\tProduces smooth continuous curves\n"
      ],
      "metadata": {
        "id": "__Xa7Xh00A-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is the role of the distance metric in KNN?\n",
        "Ans.In KNN, we classify or predict a new data point based on the nearest data points in the training set. The distance metric defines what we mean by \"nearest.\"\n",
        "Example: Euclidean vs. Manhattan\n",
        "Imagine youâ€™re in a city with grid-like roads:\n",
        "\n",
        "Euclidean Distance: \"As the crow flies\" (straight-line distance).\n",
        "\n",
        "Manhattan Distance: The path youâ€™d actually walk on city blocks (like a taxi).\n",
        "\n",
        "In high-dimensional data, Manhattan can sometimes be more stable because Euclidean gets distorted due to the curse of dimensionality."
      ],
      "metadata": {
        "id": "3rXBhcO00ZH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What is the Curse of Dimensionality in KNN?\n",
        "Ans.he Curse of Dimensionality is a critical concept in KNN (and many other machine learning algorithms). It refers to the problems and challenges that arise when working with high-dimensional data (i.e., data with many features).\n",
        "\n",
        "As the number of features (dimensions) increases:\n",
        "\n",
        "Data becomes sparse: In high dimensions, data points are more spread out, making it harder to find \"close\" neighbors.\n",
        "\n",
        "Distance metrics lose meaning: In higher dimensions, distances between data points tend to become very similar, reducing the effectiveness of KNN which relies on distinguishing between \"near\" and \"far\".\n",
        "\n",
        "Computational cost increases: More dimensions = more calculations for each distance = slower predictions.\n",
        "\n",
        "Risk of overfitting: With more features, KNN may fit noise instead of learning useful patterns, especially with small datasets.\n",
        "Intuition\n",
        "Imagine trying to find neighbors in:\n",
        "\n",
        "Dimension\tPoints Needed to Fill the Space Well\n",
        "1D\t10 points\n",
        "2D\t100 points\n",
        "3D\t1,000 points\n",
        "100D\t10Â¹â°â° points (!), which is impractical\n",
        "So, to maintain the same data density, data requirements grow exponentially with dimensions.\n",
        "\n",
        " How It Affects KNN\n",
        "KNN depends on \"local neighborhoods\".\n",
        "\n",
        "In high dimensions, all points tend to be far from each other, so \"nearest neighbors\" may not actually be similar.\n",
        "\n",
        "This leads to poor generalization and lower accuracy.\n",
        "\n",
        " How to Deal With It\n",
        "Feature selection: Choose only the most relevant features.\n",
        "\n",
        "Dimensionality reduction: Use techniques like:\n",
        "\n",
        "PCA (Principal Component Analysis)\n",
        "\n",
        "t-SNE\n",
        "\n",
        "LDA (Linear Discriminant Analysis)\n",
        "\n",
        "Normalize features: Helps mitigate distance distortion.\n",
        "\n",
        "Use other algorithms: Some models (e.g., decision trees, random forests) may handle high-dimensional data better than KNN."
      ],
      "metadata": {
        "id": "_Au6RbPi02A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How can we choose the best value of K in KNN?\n",
        "Ans.Choosing the best value of K in K-Nearest Neighbors (KNN) is super importantâ€”it can make a huge difference in your modelâ€™s performance. The value of K controls the bias-variance trade-off, so choosing it wisely helps balance underfitting vs. overfitting.\n",
        "\n",
        "Find the K that gives the best accuracy on unseen data (i.e., generalizes well)\n",
        "Here's How to Choose the Best K:\n",
        "1. Use Cross-Validation (CV)\n",
        "K-Fold Cross-Validation is the standard way to test multiple values of K.\n",
        "\n",
        "Try several values of K (like 1 to 30), evaluate performance on validation sets, and choose the K with the best average accuracy (or lowest error).\n",
        "\n",
        "2. Plot the Error vs. K Curve\n",
        "Try a range of K values (e.g., 1 to 30).\n",
        "\n",
        "For each K, compute the validation error or accuracy.\n",
        "\n",
        "Plot it:\n",
        "\n",
        "If K is too small (e.g., 1): Model overfits, high variance.\n",
        "\n",
        "If K is too large: Model underfits, high bias.\n",
        "\n",
        "Ideal K is where the error is minimized, just before the curve levels off.\n",
        "\n",
        "3. Use Odd Values of K (for Classification)\n",
        "Avoid ties in majority voting by using odd K (like 3, 5, 7) especially for binary classification.\n",
        "\n",
        "If you have multi-class classification, this is less critical."
      ],
      "metadata": {
        "id": "HbAQ7Fbw1LXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr-jnHywzZvA"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "k_range = range(1, 31)\n",
        "scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    score = cross_val_score(knn, X_train, y_train, cv=5)\n",
        "    scores.append(score.mean())\n",
        "\n",
        "plt.plot(k_range, scores)\n",
        "plt.xlabel('Value of K')\n",
        "plt.ylabel('Cross-Validated Accuracy')\n",
        "plt.title('K vs Accuracy')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What are KD Tree and Ball Tree in KNN?\n",
        "Ans.When using KNN with large datasets, searching through every training point to find the nearest neighbors becomes slow and inefficient. Thatâ€™s where KD Tree and Ball Tree come in â€” theyâ€™re data structures that help speed up nearest neighbor search.\n",
        "\n",
        " What is a KD Tree?\n",
        "KD Tree stands for k-dimensional tree.\n",
        "\n",
        "Itâ€™s a binary tree used to partition data in a k-dimensional space.\n",
        "\n",
        "At each level of the tree, the data is split along one of the dimensions (e.g., x, y, z).\n",
        "\n",
        "It organizes the points to reduce the number of distance calculations needed when searching for nearest neighbors.\n",
        "\n",
        " How It Works:\n",
        "Choose a dimension (e.g., x-axis) and split the data at the median.\n",
        "\n",
        "Recursively split left and right subspaces along other dimensions.\n",
        "\n",
        "When querying, the tree is searched in a way that prunes large chunks of data that can't possibly be the nearest neighbors.\n",
        "\n",
        " Best For:\n",
        "Low to medium dimensions (up to ~20).\n",
        "\n",
        "Faster than brute-force for moderate-sized datasets.\n",
        "\n",
        " What is a Ball Tree?\n",
        "Ball Tree is another tree-based structure, but instead of splitting on axes, it divides space into nested balls (spheres).\n",
        "\n",
        "Each node contains a \"ball\" (a point + radius) that encloses a subset of the data.\n",
        "\n",
        "During search, it uses triangle inequality to prune entire balls that are too far from the query point.\n",
        "\n",
        " How It Works:\n",
        "Build tree by grouping nearby points into balls.\n",
        "\n",
        "Recursively divide each ball into smaller sub-balls.\n",
        "\n",
        "During search, skip balls that are too far from the query.\n",
        "\n",
        " Best For:\n",
        "High-dimensional data.\n",
        "\n",
        "Non-axis-aligned data distributions.\n",
        "\n",
        " KD Tree vs. Ball Tree\n",
        "Feature\tKD Tree\tBall Tree\n",
        "Split method\tAxis-aligned median splits\tSpherical partitions (balls)\n",
        "Best for\tLow to medium dimensions\tHigh dimensions\n",
        "Speed\tVery fast for low-dim data\tBetter for complex, high-dim data\n",
        "Scikit-learn support\talgorithm='kd_tree'\talgorithm='ball_tree'\n"
      ],
      "metadata": {
        "id": "SdKhGpu51lsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Use KD Tree\n",
        "knn_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "\n",
        "# Use Ball Tree\n",
        "knn_ball = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n"
      ],
      "metadata": {
        "id": "iTv5XtwY15_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.When should you use KD Tree vs. Ball Tree?\n",
        "Ans.Choosing between KD Tree and Ball Tree depends mainly on your datasetâ€™s dimensionality, size, and the distribution of data.\n",
        "When to Use KD Tree:\n",
        "Your dataset has less than ~20 features.\n",
        "\n",
        "Data is well-distributed (not too skewed or clustered).\n",
        "\n",
        "You want fast, exact nearest neighbor queries.\n",
        "\n",
        "Your application needs deterministic performance (no randomness).\n",
        "\n",
        "Example use cases:\n",
        "\n",
        "Image classification with simple color histograms.\n",
        "\n",
        "2D/3D spatial data (e.g., map coordinates, game development).\n",
        "\n",
        " When to Use Ball Tree:\n",
        "Your dataset has more than ~20 features (high-dimensional).\n",
        "\n",
        "Data is not aligned to axes or is clustered/spherical in nature.\n",
        "\n",
        "You need better scalability in complex or messy spaces.\n",
        "\n",
        "Example use cases:\n",
        "\n",
        "Text embeddings (e.g., word2vec, BERT features).\n",
        "\n",
        "Bioinformatics data.\n",
        "\n",
        "Complex similarity search where distances arenâ€™t cleanly separable.\n",
        "\n"
      ],
      "metadata": {
        "id": "t2cX0GhO1793"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What are the disadvantages of KNN?\n",
        "Ans.K-Nearest Neighbors (KNN) is simple and powerful, it has several key disadvantages that can limit its use in real-world applicationsâ€”especially as datasets grow in size and complexity.\n",
        "Disadvantages of KNN\n",
        "1. ðŸ¢ Slow Prediction Time (Lazy Learning)\n",
        "KNN doesnâ€™t \"learn\" during training. It stores the entire training set and defers all computation until prediction.\n",
        "\n",
        "At prediction time, it must compute the distance between the new point and every training point.\n",
        "\n",
        "Time complexity:\n",
        "ð‘‚\n",
        "(\n",
        "ð‘›\n",
        "â‹…\n",
        "ð‘‘\n",
        ")\n",
        "O(nâ‹…d), where\n",
        "\n",
        "ð‘›\n",
        "n = number of training samples\n",
        "\n",
        "ð‘‘\n",
        "d = number of features\n",
        "\n",
        " Big issue with large datasets.\n",
        "\n",
        "2. Sensitive to Feature Scale\n",
        "If features are not normalized (e.g., height in cm vs. weight in kg), the algorithm will favor features with larger scales.\n",
        "\n",
        "Distance calculations (like Euclidean) can get distorted.\n",
        "\n",
        "Solution: Use StandardScaler or MinMaxScaler before fitting KNN.\n",
        "\n",
        "3.  Curse of Dimensionality\n",
        "In high-dimensional spaces, all points tend to be far apart.\n",
        "\n",
        "The concept of \"nearest\" becomes less meaningful, leading to poor accuracy.\n",
        "\n",
        "Distances start to converge, making it hard to distinguish between classes.\n",
        "\n",
        " Dimensionality reduction (like PCA) can help.\n",
        "\n",
        "4.  Memory Inefficient\n",
        "KNN stores all training data, which means:\n",
        "\n",
        "High memory usage\n",
        "\n",
        "Slow for large datasets\n",
        "\n",
        "Consider using KD-Tree, Ball Tree, or approximate nearest neighbor techniques.\n",
        "\n",
        "5. Sensitive to Noise & Outliers\n",
        "A few noisy data points can skew predictions, especially with small K values.\n",
        "\n",
        "KNN doesnâ€™t inherently filter or handle noise.\n",
        "\n",
        " Using a larger K or distance-weighted voting can reduce this effect.\n",
        "\n",
        "6. No Model Interpretability\n",
        "KNN doesnâ€™t produce a model with parameters or coefficients.\n",
        "\n",
        "Hard to explain why a prediction was made (other than pointing to neighbors).\n",
        "\n",
        "7. Choosing the Right K is Tricky\n",
        "Small K â†’ overfitting\n",
        "\n",
        "Large K â†’ underfitting\n",
        "\n",
        "There's no universal \"best K\" â€” it must be tuned for each problem.\n",
        "\n",
        "Summary Table\n",
        "Disadvantage\tImpact\tPossible Fix\n",
        "Slow prediction\tPoor scalability\tUse KD Tree, Ball Tree, or ANN\n",
        "Needs feature scaling\tSkewed distance calculations\tNormalize/standardize features\n",
        "High dimensionality issues\tLower accuracy\tApply PCA or feature selection\n",
        "Sensitive to noise\tInconsistent predictions\tUse larger K or weighted KNN\n",
        "High memory usage\tDoesnâ€™t scale to big data\tUse approximate neighbors\n"
      ],
      "metadata": {
        "id": "zyeB8gLJ2LaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.How does feature scaling affect KNN?\n",
        "Ans.feature scaling is critical in K-Nearest Neighbors (KNN) because the algorithm relies on distance calculations (like Euclidean, Manhattan, etc.) to determine which neighbors are \"closest.\"\n",
        "\n",
        "If your features are on different scales or units, KNN can make very biased decisions.\n",
        "\n",
        " Why Feature Scaling Matters in KNN\n",
        "In KNN, we compute distance like this:\n",
        "\n",
        "EuclideanÂ distance\n",
        "=\n",
        "(\n",
        "ð‘¥\n",
        "1\n",
        "âˆ’\n",
        "ð‘¦\n",
        "1\n",
        ")\n",
        "2\n",
        "+\n",
        "(\n",
        "ð‘¥\n",
        "2\n",
        "âˆ’\n",
        "ð‘¦\n",
        "2\n",
        ")\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "(\n",
        "ð‘¥\n",
        "ð‘›\n",
        "âˆ’\n",
        "ð‘¦\n",
        "ð‘›\n",
        ")\n",
        "2\n",
        "EuclideanÂ distance=\n",
        "(x\n",
        "1\n",
        "â€‹\n",
        " âˆ’y\n",
        "1\n",
        "â€‹\n",
        " )\n",
        "2\n",
        " +(x\n",
        "2\n",
        "â€‹\n",
        " âˆ’y\n",
        "2\n",
        "â€‹\n",
        " )\n",
        "2\n",
        " +â‹¯+(x\n",
        "n\n",
        "â€‹\n",
        " âˆ’y\n",
        "n\n",
        "â€‹\n",
        " )\n",
        "2\n",
        "\n",
        "â€‹\n",
        "\n",
        "If one feature has a much larger range, it will dominate the distance â€” even if it's not the most important feature.\n",
        "\n",
        "Example:\n",
        "Suppose youâ€™re classifying fruits using:\n",
        "\n",
        "Feature\tScale\n",
        "Height\t1 to 10 cm\n",
        "Weight\t100 to 1000 g\n",
        "Without scaling, weight will completely outweigh height in the distance metric.\n",
        "\n",
        "So, even if height is a better predictor, KNN might ignore it due to scale differences. Not good.\n",
        "\n",
        " Feature Scaling Fixes This\n",
        "Common techniques:\n",
        "\n",
        "Technique\tWhat It Does\tUse When\n",
        "Min-Max Scaling\tScales features to [0, 1] range\tGeneral use\n",
        "Standardization (Z-score)\tCenters around 0 with std. dev. 1\tWhen data is normally distributed\n",
        "Robust Scaler\tUses median and IQR (handles outliers well)\tWhen data has outliers\n",
        "Scikit-learn Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Wrap scaling + KNN in a pipeline\n",
        "model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "5IUGTUIc2oD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What is PCA (Principal Component Analysis)?\n",
        "Ans.Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a dataset with many features into a new set of features called principal components, which capture the most important patterns or directions of variance in the data.\n",
        "\n",
        "Why Use PCA\n",
        "To reduce the number of features while preserving as much information as possible.\n",
        "\n",
        "To remove noise and redundancy.\n",
        "\n",
        "To visualize high-dimensional data in 2D or 3D.\n",
        "\n",
        "To speed up machine learning algorithms and reduce overfitting.\n",
        "\n",
        "How PCA Works (Step-by-Step)\n",
        "Standardize the Data: PCA is sensitive to scale, so features are typically standardized to have zero mean and unit variance.\n",
        "\n",
        "Compute the Covariance Matrix: This matrix shows how the features vary with each other.\n",
        "\n",
        "Calculate Eigenvalues and Eigenvectors: These represent the magnitude and direction of variance.\n",
        "\n",
        "Select Principal Components: Choose the top k eigenvectors (based on the largest eigenvalues) that capture the most variance.\n",
        "\n",
        "Transform the Data: Project the original data onto the new axes (principal components) to get the reduced dataset.\n",
        "\n",
        "Key Terms\n",
        "Principal Components: New axes or directions that capture the most variance in the data.\n",
        "\n",
        "Explained Variance: How much of the total variance is captured by each principal component.\n",
        "\n",
        "Dimensionality Reduction: Keeping only the top k principal components instead of all original features.\n",
        "\n",
        "When to Use PCA\n",
        "When you have many features and want to reduce them to a smaller set.\n",
        "\n",
        "When features are highly correlated.\n",
        "\n",
        "Before applying algorithms that are sensitive to the curse of dimensionality, such as KNN or clustering."
      ],
      "metadata": {
        "id": "SPeqTanv3KMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How does PCA work?\n",
        "Ans.Standardize the Data\n",
        "PCA is affected by the scale of the features, so we first standardize the data:\n",
        "\n",
        "For each feature, subtract the mean and divide by the standard deviation.\n",
        "\n",
        "This gives each feature a mean of 0 and standard deviation of 1.\n",
        "\n",
        "2. Compute the Covariance Matrix\n",
        "The covariance matrix shows how the features vary with respect to one another.\n",
        "\n",
        "It helps us understand the relationships and dependencies between features.\n",
        "\n",
        "If your data matrix is X (after standardization), the covariance matrix is:\n",
        "\n",
        "Cov\n",
        "(\n",
        "ð‘‹\n",
        ")\n",
        "=\n",
        "1\n",
        "ð‘›\n",
        "âˆ’\n",
        "1\n",
        "ð‘‹\n",
        "ð‘‡\n",
        "ð‘‹\n",
        "Cov(X)=\n",
        "nâˆ’1\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "T\n",
        " X\n",
        "3. Calculate Eigenvalues and Eigenvectors\n",
        "Eigenvectors give the directions (principal components).\n",
        "\n",
        "Eigenvalues tell how much variance is explained by each principal component.\n",
        "\n",
        "These are computed from the covariance matrix.\n",
        "\n",
        "The first principal component corresponds to the eigenvector with the largest eigenvalue.\n",
        "\n",
        "The second principal component is the one with the second-largest eigenvalue, and so on.\n",
        "\n",
        "4. Select Top k Principal Components\n",
        "Sort eigenvectors by their eigenvalues in descending order.\n",
        "\n",
        "Choose the top k eigenvectors (where k is the number of dimensions you want to keep).\n",
        "\n",
        "These form the new basis for your data.\n",
        "\n",
        "5. Transform the Data\n",
        "Multiply the original standardized data by the matrix of selected eigenvectors.\n",
        "\n",
        "This projects the data onto the new lower-dimensional space.\n",
        "\n",
        "The transformed data is:\n",
        "\n",
        "ð‘\n",
        "=\n",
        "ð‘‹\n",
        "â‹…\n",
        "ð‘Š\n",
        "Z=Xâ‹…W\n",
        "Where:\n",
        "\n",
        "X is the standardized data matrix.\n",
        "\n",
        "W is the matrix of top k eigenvectors.\n",
        "\n",
        "Z is the reduced dataset in the new coordinate system.\n",
        "\n"
      ],
      "metadata": {
        "id": "bMjqLFuJ3PNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.What is the geometric intuition behind PCA?\n",
        "Ans.The geometric intuition behind Principal Component Analysis (PCA) is that it finds a new coordinate system (a new set of axes) for your data that captures the directions of maximum variance. These new axes are orthogonal (perpendicular) and are called principal components.\n",
        "\n",
        "Hereâ€™s a step-by-step geometric interpretation:\n",
        "\n",
        "1. Data as Points in Space\n",
        "Imagine your dataset as a cloud of points in a multi-dimensional space:\n",
        "\n",
        "Each data point is a vector.\n",
        "\n",
        "If you have 2 features, youâ€™re in a 2D space; 3 features â†’ 3D space; more features â†’ high-dimensional space.\n",
        "\n",
        "2. Principal Components as New Axes\n",
        "PCA rotates this cloud of points so that:\n",
        "\n",
        "The first principal component (PC1) points in the direction of maximum variance in the data.\n",
        "\n",
        "The second principal component (PC2) is orthogonal to the first and points in the direction of the next highest variance.\n",
        "\n",
        "This continues for all components.\n",
        "\n",
        "Think of fitting a line or plane through the data that best explains how it is spread out.\n",
        "\n",
        "3. Projection Onto the Principal Components\n",
        "Once you have these new axes:\n",
        "\n",
        "You project the original data onto them.\n",
        "\n",
        "This gives you new coordinates for each data point in the rotated system.\n",
        "\n",
        "If you keep only the top k components, you effectively compress the data while preserving most of its structure.\n",
        "\n",
        "4. Dimensionality Reduction\n",
        "Geometrically, reducing dimensions means:\n",
        "\n",
        "Instead of describing each point by its coordinates in the full space (e.g., 10D),\n",
        "\n",
        "You describe it using its coordinates along the most meaningful directions (e.g., top 2 or 3 principal components).\n",
        "\n",
        "This flattens the data onto a lower-dimensional subspace that preserves its overall shape and variation.\n",
        "\n",
        "Visual Example (in 2D):\n",
        "Imagine an elliptical cloud of points stretched diagonally in 2D space:\n",
        "\n",
        "The longest axis of the ellipse is PC1 (most variance).\n",
        "\n",
        "The shorter axis, perpendicular to PC1, is PC2 (second most variance).\n",
        "\n",
        "PCA rotates the axes to align with those directions.\n",
        "\n",
        "If you drop PC2 and keep only PC1, youâ€™ve reduced the 2D data to 1D while retaining most of its structure."
      ],
      "metadata": {
        "id": "VgEe9gis3fAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is the difference between Feature Selection and Feature Extraction?\n",
        "Ans.Feature Selection and Feature Extraction are both techniques used for dimensionality reduction, but they work in very different ways.\n",
        "\n",
        "1. Feature Selection\n",
        "Definition:\n",
        "Feature selection is the process of selecting a subset of existing features from the dataset that are most relevant to the target variable.\n",
        "\n",
        "How it works:\n",
        "\n",
        "It keeps the original features unchanged.\n",
        "\n",
        "It removes irrelevant, redundant, or less important features.\n",
        "\n",
        "Methods:\n",
        "\n",
        "Filter methods: Based on statistical tests (e.g., correlation, chi-square)\n",
        "\n",
        "Wrapper methods: Use machine learning models to test different subsets (e.g., recursive feature elimination)\n",
        "\n",
        "Embedded methods: Feature importance from models (e.g., Lasso, tree-based models)\n",
        "\n",
        "Example: From a dataset with 10 features, you select the top 5 based on correlation with the target.\n",
        "\n",
        "Use when:\n",
        "\n",
        "You want to improve model interpretability.\n",
        "\n",
        "You prefer to keep features in their original form.\n",
        "\n",
        "2. Feature Extraction\n",
        "Definition:\n",
        "Feature extraction creates new features by transforming or combining the original ones.\n",
        "\n",
        "How it works:\n",
        "\n",
        "It projects the data into a new feature space.\n",
        "\n",
        "It captures the most important information in fewer features.\n",
        "\n",
        "Methods:\n",
        "\n",
        "Principal Component Analysis (PCA)\n",
        "\n",
        "Linear Discriminant Analysis (LDA)\n",
        "\n",
        "Autoencoders (in deep learning)\n",
        "\n",
        "Example: From 10 original features, PCA creates 3 new features (principal components) that summarize the most variance.\n",
        "\n",
        "Use when:\n",
        "\n",
        "You want to reduce dimensionality while retaining most of the dataâ€™s information.\n",
        "\n",
        "Youâ€™re okay with losing the original feature meanings.\n",
        "\n",
        "Key Differences\n",
        "Aspect\tFeature Selection\tFeature Extraction\n",
        "What it does\tSelects a subset of original features\tCreates new features from existing ones\n",
        "Original features\tRetained\tTransformed or combined\n",
        "Interpretability\tHigh\tLower\n",
        "Goal\tKeep useful features\tCapture key patterns in fewer dimensions\n",
        "Examples\tCorrelation filtering, Lasso\tPCA, LDA, Autoencoders\n"
      ],
      "metadata": {
        "id": "xkOX8p623tgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What are Eigenvalues and Eigenvectors in PCA?\n",
        "Ans.In Principal Component Analysis (PCA), eigenvalues and eigenvectors are central concepts that help us understand the structure and variance in the data.\n",
        "\n",
        "Hereâ€™s what they mean and how they relate to PCA:\n",
        "What Is an Eigenvector?\n",
        "An eigenvector is a direction in the data space that remains unchanged when a linear transformation (like rotation or scaling) is applied. In the context of PCA, eigenvectors represent the principal componentsâ€”the new axes along which the data has the most variance.\n",
        "\n",
        "Each eigenvector defines a new axis (principal component) in the transformed feature space.\n",
        "\n",
        "The eigenvectors are orthogonal (perpendicular) to each other.\n",
        "\n",
        "2. What Is an Eigenvalue?\n",
        "An eigenvalue tells you how much variance in the data is captured by its corresponding eigenvector.\n",
        "\n",
        "A large eigenvalue means that the corresponding eigenvector (principal component) captures a lot of the spread or variability in the data.\n",
        "\n",
        "A small eigenvalue means it captures very little variance.\n",
        "\n",
        "3. How PCA Uses Eigenvalues and Eigenvectors\n",
        "Hereâ€™s how PCA applies these concepts:\n",
        "\n",
        "Compute the covariance matrix of the standardized data.\n",
        "\n",
        "Calculate the eigenvalues and eigenvectors of this matrix.\n",
        "\n",
        "Sort the eigenvectors by descending eigenvalues.\n",
        "\n",
        "Select the top k eigenvectors (with the largest eigenvalues) to form the new feature space.\n",
        "\n",
        "Project the original data onto these eigenvectors to reduce dimensionality.\n",
        "\n",
        "4. Interpretation\n",
        "Eigenvectors = directions of maximum variance (principal components)\n",
        "\n",
        "Eigenvalues = amount of variance explained in each of those directions\n",
        "\n",
        "For example:\n",
        "\n",
        "If the first eigenvalue is much larger than the rest, the first principal component captures most of the data's variance.\n",
        "\n",
        "The ratio of each eigenvalue to the sum of all eigenvalues gives the explained variance ratio.\n",
        "\n"
      ],
      "metadata": {
        "id": "vnrajbQa35kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA?\n",
        "Ans.Deciding how many principal components to keep in PCA depends on how much of the total variance you want to preserve in the data. The goal is to reduce dimensions while retaining the most important information.\n",
        "\n",
        "Here are the most common methods to choose the number of components:\n",
        "\n",
        "1. Explained Variance Threshold\n",
        "Each principal component explains a certain percentage of the total variance.\n",
        "\n",
        "You can choose to keep components that together explain a target amount, such as 95% of the total variance.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Calculate the explained variance ratio for each component.\n",
        "\n",
        "Compute the cumulative sum.\n",
        "\n",
        "Choose the smallest number of components such that the cumulative variance reaches your threshold.\n",
        "\n",
        "Example (cumulative variance):\n",
        "\n",
        "Component\tExplained Variance (%)\tCumulative (%)\n",
        "PC1\t55\t55\n",
        "PC2\t25\t80\n",
        "PC3\t10\t90\n",
        "PC4\t5\t95\n",
        "Here, you would choose 4 components to retain 95% of the variance.\n",
        "\n",
        "2. Scree Plot (Elbow Method)\n",
        "Plot the eigenvalues (or explained variance) in decreasing order.\n",
        "\n",
        "Look for the point where the curve starts to level offâ€”the \"elbow\".\n",
        "\n",
        "Choose the number of components before the elbow, where adding more components gives diminishing returns.\n",
        "\n",
        "This is a visual method and works well when the drop in variance is clear.\n",
        "\n",
        "3. Kaiser Criterion (for standardized data)\n",
        "Keep components with eigenvalues > 1.\n",
        "\n",
        "This rule is based on the idea that a component should carry more information than an individual original feature.\n",
        "\n",
        "This method is often used in factor analysis but can be applied to PCA in some cases.\n",
        "\n",
        "4. Cross-Validation (For Model Performance)\n",
        "Use PCA as part of a pipeline in a machine learning model.\n",
        "\n",
        "Try different numbers of components and evaluate model performance using cross-validation.\n",
        "\n",
        "Choose the number that gives the best performance"
      ],
      "metadata": {
        "id": "hAELh8z74Nva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Can PCA be used for classification?\n",
        "Ans.PCA itself is not a classification algorithm, but it can be used as a preprocessing step to improve classification tasks.\n",
        "\n",
        "Hereâ€™s how and when PCA can be helpful in classification:\n",
        "\n",
        "1. How PCA Helps in Classification\n",
        "PCA transforms the original high-dimensional data into a lower-dimensional space by capturing the directions of maximum variance. This helps in:\n",
        "\n",
        "Reducing dimensionality, which simplifies the problem space\n",
        "\n",
        "Removing noise and redundant features\n",
        "\n",
        "Speeding up training time for classification algorithms\n",
        "\n",
        "Preventing overfitting, especially when the number of features is much larger than the number of samples\n",
        "\n",
        "2. When to Use PCA Before Classification\n",
        "Use PCA when:\n",
        "\n",
        "You have a large number of features that might be correlated\n",
        "\n",
        "Your model performs poorly due to high dimensionality\n",
        "\n",
        "You want to visualize the data in 2D or 3D before classification\n",
        "\n",
        "You aim to improve training speed on large datasets\n",
        "\n",
        "3. How It Works in a Classification Pipeline\n",
        "Standardize the dataset\n",
        "\n",
        "Apply PCA to reduce dimensions\n",
        "\n",
        "Feed the reduced dataset into a classifier (e.g., logistic regression, SVM, decision tree)"
      ],
      "metadata": {
        "id": "fmnVnRvE4Xbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PCA(n_components=10),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "PdqPNHAx4maL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What are the limitations of PCA?\n",
        "Ans.Principal Component Analysis (PCA) is a powerful tool, but it comes with several limitations. Understanding these helps decide when it is appropriate to use PCA and when it might hurt model performance or interpretation.\n",
        "\n",
        "1. PCA Is Linear\n",
        "PCA assumes that the relationships in the data are linear.\n",
        "\n",
        "It cannot capture nonlinear patterns or structures.\n",
        "\n",
        "If your data lies on a nonlinear manifold (like curves or spirals), PCA will not model it well.\n",
        "\n",
        "2. Loss of Interpretability\n",
        "After transformation, the principal components are combinations of original features.\n",
        "\n",
        "It becomes difficult to interpret what each component means in real-world terms.\n",
        "\n",
        "This can be a problem in applications where understanding feature impact is important (e.g. healthcare, finance).\n",
        "\n",
        "3. Variance Does Not Always Equal Importance\n",
        "PCA ranks components based on variance, not on their relevance to a target variable.\n",
        "\n",
        "A feature with high variance might not be useful for prediction or classification.\n",
        "\n",
        "PCA is unsupervised, so it ignores class labels or outcomes during transformation.\n",
        "\n",
        "4. Sensitive to Scaling\n",
        "PCA is sensitive to the scale of features.\n",
        "\n",
        "If features are not standardized, those with larger scales will dominate the principal components.\n",
        "\n",
        "Always standardize or normalize data before applying PCA.\n",
        "\n",
        "5. Assumes Gaussian Distributions\n",
        "PCA works best when the features are normally distributed.\n",
        "\n",
        "If the data is heavily skewed or has outliers, PCA may produce misleading components.\n",
        "\n",
        "6. Affected by Outliers\n",
        "Outliers can distort the covariance matrix and significantly influence the principal components.\n",
        "\n",
        "Preprocessing steps like outlier removal or robust PCA variants may be needed.\n",
        "\n",
        "7. Information Loss\n",
        "Reducing dimensions means some information is always lost.\n",
        "\n",
        "If too few components are retained, the reduced dataset may not capture enough structure for downstream tasks.\n",
        "\n",
        "8. Not Ideal for Categorical Data\n",
        "PCA works with numerical data.\n",
        "\n",
        "Applying PCA to categorical features (even if one-hot encoded) may produce unreliable or meaningless results."
      ],
      "metadata": {
        "id": "dqtWTR3O4uSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.How do KNN and PCA complement each other?\n",
        "Ans.K-Nearest Neighbors (KNN) and Principal Component Analysis (PCA) can complement each other effectively when used together, especially in high-dimensional datasets. Here's how they work well in combination:\n",
        "\n",
        "1. Reducing Dimensionality Before KNN\n",
        "KNN is sensitive to the number of featuresâ€”as the dimensionality increases, the distance between data points becomes less meaningful. This is known as the curse of dimensionality.\n",
        "\n",
        "How PCA helps:\n",
        "\n",
        "PCA reduces the number of dimensions by capturing the most important patterns in the data.\n",
        "\n",
        "This makes distance calculations in KNN more reliable and meaningful.\n",
        "\n",
        "It also reduces computational cost and noise, improving KNN's performance.\n",
        "\n",
        "2. Improving Classification Accuracy\n",
        "In high-dimensional spaces, irrelevant or redundant features can distort the distance metric used in KNN.\n",
        "\n",
        "PCA removes correlated and low-variance features, helping KNN focus on the most informative ones.\n",
        "\n",
        "As a result, KNN may classify points more accurately when PCA is applied first.\n",
        "\n",
        "3. Faster Computation in KNN\n",
        "KNN stores all training data and performs lazy learning, meaning it computes distances at prediction time. This can be slow for high-dimensional data.\n",
        "\n",
        "PCA reduces the number of features, which:\n",
        "\n",
        "Speeds up prediction time\n",
        "\n",
        "Reduces memory usage\n",
        "\n",
        "Makes KNN feasible on larger datasets\n",
        "\n",
        "4. Noise Reduction\n",
        "KNN can be negatively affected by noisy features. PCA acts as a denoising tool by:\n",
        "\n",
        "Projecting data onto components that capture meaningful variance\n",
        "\n",
        "Ignoring small, potentially noisy variations in the data\n",
        "\n",
        "This makes KNN more robust.\n",
        "\n",
        "5. Visualization and Analysis\n",
        "PCA reduces data to 2D or 3D, allowing you to visualize the data distribution.\n",
        "\n",
        "This helps in understanding how KNN might behave, such as whether classes are separable or clustered.\n",
        "\n",
        "Typical Pipeline\n",
        "Standardize the data (PCA requires it)\n",
        "\n",
        "Apply PCA to reduce dimensions\n",
        "\n",
        "Use KNN for classification or regression on the reduced data"
      ],
      "metadata": {
        "id": "I9za2gsz54C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PCA(n_components=10),\n",
        "    KNeighborsClassifier(n_neighbors=5)\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "qMC4ehKL6IRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.How does KNN handle missing values in a dataset?\n",
        "Ans.K-Nearest Neighbors (KNN) does not handle missing values nativelyâ€”meaning, if your dataset contains missing values, KNN will not work properly unless you handle those missing values first.\n",
        "\n",
        "However, there are a few ways to deal with missing values before or with KNN:\n",
        "\n",
        "1. Preprocessing: Imputation Before Using KNN\n",
        "Before using KNN for classification or regression, you should fill in missing values. Some common imputation strategies:\n",
        "\n",
        "Mean/Median Imputation: Replace missing values with the mean or median of the column.\n",
        "\n",
        "Mode Imputation: For categorical features, use the most frequent value.\n",
        "\n",
        "KNN Imputation (see below): Use a KNN-based imputation method to estimate missing values.\n",
        "\n",
        "2. KNN Imputation (Separate Use of KNN)\n",
        "This is a different use of KNN: not as a classifier or regressor, but as an imputer.\n",
        "\n",
        "How it works:\n",
        "\n",
        "For a sample with a missing value, find its k nearest neighbors using the non-missing features.\n",
        "\n",
        "Use the corresponding values from neighbors to fill in the missing data."
      ],
      "metadata": {
        "id": "TgYaj1VY6VZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_filled = imputer.fit_transform(X_with_missing)\n"
      ],
      "metadata": {
        "id": "UY7PLM_U6fE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Dropping Rows or Features (Only If Necessary)\n",
        "If only a few rows or columns have missing values, you might drop them.\n",
        "\n",
        "But this can lead to data loss, especially in small datasets."
      ],
      "metadata": {
        "id": "pLzyDnY66iuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "ANsPrincipal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are both dimensionality reduction techniques, but they are fundamentally different in their goals, assumptions, and methods.\n",
        "\n",
        "Here's a clear breakdown of the key differences between PCA and LDA:\n",
        "\n",
        "1. Supervision\n",
        "Aspect\tPCA\tLDA\n",
        "Type\tUnsupervised\tSupervised\n",
        "Uses class labels?\tNo\tYes\n",
        "PCA does not use target (class) labels.\n",
        "\n",
        "LDA uses target labels to maximize class separability.\n",
        "\n",
        "2. Goal\n",
        "Aspect\tPCA\tLDA\n",
        "Primary objective\tMaximize variance in the data\tMaximize class separability\n",
        "Projection direction\tDirections of maximum variance\tDirections that best separate classes\n",
        "PCA focuses on capturing as much information (spread) as possible.\n",
        "\n",
        "LDA focuses on finding the axes that best discriminate between classes.\n",
        "\n",
        "3. How It Works\n",
        "Step\tPCA\tLDA\n",
        "Computation basis\tEigenvectors of the covariance matrix\tEigenvectors of scatter matrices (within and between classes)\n",
        "Optimization target\tTotal variance\tRatio of between-class variance to within-class variance\n",
        "4. Number of Components You Can Get\n",
        "Aspect\tPCA\tLDA\n",
        "Max number of components\tâ‰¤ number of features\tâ‰¤ (number of classes âˆ’ 1)\n",
        "LDAâ€™s number of useful components is limited by the number of classes in your dataset.\n",
        "\n",
        "PCA can return more components depending on the dataset's dimensionality.\n",
        "\n",
        "5. Interpretability\n",
        "PCA components are combinations of original features with no class-specific meaning.\n",
        "\n",
        "LDA components are more interpretable in classification tasks, as they focus on separating the classes.\n",
        "\n",
        "6. Performance Use Case\n",
        "Best used for...\tPCA\tLDA\n",
        "Data visualization, compression\tWhen labels are not available\tWhen labels are available and classes need separation\n",
        "Example Scenario\n",
        "Use PCA if you want to reduce dimensions of raw data without labels or for data compression.\n",
        "\n",
        "Use LDA if you are preparing data for a classification task and want to enhance class separability.\n",
        "\n"
      ],
      "metadata": {
        "id": "aZtFQqOO6l4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice**\n"
      ],
      "metadata": {
        "id": "iCsTBws96wdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Train a KNN Classifier on the Iris dataset and print model accuracy?\n",
        "Ans"
      ],
      "metadata": {
        "id": "gWtZkaG1682E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "accuracy\n"
      ],
      "metadata": {
        "id": "57U-izp57ItK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)?\n",
        "Ans."
      ],
      "metadata": {
        "id": "F9jGQEp17J00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a synthetic regression dataset\n",
        "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN regressor\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate using MSE\n",
        "y_pred = knn_regressor.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "mse\n"
      ],
      "metadata": {
        "id": "2sISuI8c974D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "vzD4TwtD98y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train KNN classifier using Euclidean distance (default)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Train KNN classifier using Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "accuracy_euclidean, accuracy_manhattan\n"
      ],
      "metadata": {
        "id": "9c7JZyAd-CS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you'd like, I can still show you the code to run locally so you can compare the accuracy of KNN using Euclidean and Manhattan distance metrics on the Iris dataset. Let me know if you want that."
      ],
      "metadata": {
        "id": "YK5Ohgau-WkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the Iris dataset for classification\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier using Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Train KNN classifier using Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "accuracy_euclidean, accuracy_manhattan\n"
      ],
      "metadata": {
        "id": "jCUoNoPi-PDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Train a KNN Classifier with different values of K and visualize decision boundaried\n",
        "Ans."
      ],
      "metadata": {
        "id": "JQJxnJkl-ZsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Only use the first two features for 2D plotting\n",
        "y = iris.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Set up mesh grid for plotting decision boundaries\n",
        "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Plot for different values of K\n",
        "plt.figure(figsize=(15, 4))\n",
        "for i, k in enumerate([1, 5, 15]):\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.subplot(1, 3, i + 1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', cmap=plt.cm.RdYlBu)\n",
        "    plt.title(f'KNN (k={k})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GMaYHOqG-i1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
        "Ans"
      ],
      "metadata": {
        "id": "FlotvA8x-qMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Load the Iris dataset again\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate KNN without scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Train and evaluate KNN with feature scaling\n",
        "pipeline_scaled = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))\n",
        "pipeline_scaled.fit(X_train, y_train)\n",
        "y_pred_scaled = pipeline_scaled.predict(X_test)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "accuracy_unscaled, accuracy_scaled\n"
      ],
      "metadata": {
        "id": "w73FY99d-ueO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
        "Ans"
      ],
      "metadata": {
        "id": "YrqYufFM-6pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X, _ = make_classification(n_samples=300, n_features=5, n_informative=3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained Variance Ratio per Component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Component {i + 1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "id": "hOdfA8kH_Y4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Apply PCA before training a KNN Classifier and compare accuracy with and without PCA5\n",
        "Ans."
      ],
      "metadata": {
        "id": "fmSZgIgG_Zua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load and split the Iris dataset again\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN without PCA\n",
        "knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_pca.fit(X_train_scaled, y_train)\n",
        "y_pred_no_pca = knn_no_pca.predict(X_test_scaled)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "\n",
        "# Apply PCA and train KNN with PCA-transformed data\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_with_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_with_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_with_pca.predict(X_test_pca)\n",
        "accuracy_with_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "accuracy_no_pca, accuracy_with_pca\n"
      ],
      "metadata": {
        "id": "8zGo2_n__ni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV5\n",
        "Ans."
      ],
      "metadata": {
        "id": "5we7hEVC_pM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline with scaling and KNN\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define the parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': [1, 3, 5, 7, 9],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_accuracy = grid_search.score(X_test, y_test)\n",
        "print(\"Test Set Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "vTUu2CbQ_69W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Train a KNN Classifier and check the number of misclassified samples?\n",
        "Ans."
      ],
      "metadata": {
        "id": "8UVDzS7i_7xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load and preprocess the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Calculate number of misclassified samples\n",
        "misclassified = (y_test != y_pred).sum()\n",
        "\n",
        "misclassified\n"
      ],
      "metadata": {
        "id": "R_gR1_b6AIfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Train a PCA model and visualize the cumulative explained variance.\n",
        "Ans."
      ],
      "metadata": {
        "id": "Ha4TbRG4AMFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
        "plt.title('Cumulative Explained Variance by PCA Components')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.95, color='r', linestyle=':')\n",
        "plt.text(1, 0.95, '95% threshold', color='red', fontsize=9, va='bottom')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E0w5AkxFAdI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "1Li7lXWbAhH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN with 'uniform' weights\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train_scaled, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test_scaled)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# Train KNN with 'distance' weights\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train_scaled, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test_scaled)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "accuracy_uniform, accuracy_distance\n"
      ],
      "metadata": {
        "id": "xwhALwJGAyg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "Ans."
      ],
      "metadata": {
        "id": "7H03gpMEAzdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=20, random_state=42)\n",
        "\n",
        "# Split and scale the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Evaluate performance for different values of K\n",
        "k_values = list(range(1, 21))\n",
        "mse_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    model = KNeighborsRegressor(n_neighbors=k)\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(k_values, mse_scores, marker='o')\n",
        "plt.title(\"KNN Regressor: Effect of K on MSE\")\n",
        "plt.xlabel(\"Number of Neighbors (K)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NdwqfY85A_9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Implement KNN Imputation for handling missing values in a dataset?\n",
        "Ans."
      ],
      "metadata": {
        "id": "OWX0n6b4BGib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load Iris dataset and introduce missing values\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Randomly set 5% of values to NaN\n",
        "mask = np.random.rand(*X.shape) < 0.05\n",
        "X_missing = X.mask(mask)\n",
        "\n",
        "print(\"Data with Missing Values:\")\n",
        "print(X_missing.head())\n",
        "\n",
        "# Apply KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "X_imputed = imputer.fit_transform(X_missing)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n",
        "\n",
        "print(\"\\nData after KNN Imputation:\")\n",
        "print(X_imputed_df.head())\n"
      ],
      "metadata": {
        "id": "SXB1vvqABZCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Train a PCA model and visualize the data projection onto the first two principal components?\n",
        "Ans."
      ],
      "metadata": {
        "id": "PsMNnBgEBZ_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the projection\n",
        "plt.figure(figsize=(8, 6))\n",
        "for target, label in enumerate(target_names):\n",
        "    plt.scatter(\n",
        "        X_pca[y == target, 0],\n",
        "        X_pca[y == target, 1],\n",
        "        label=label,\n",
        "        alpha=0.7\n",
        "    )\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Projection of the Iris Dataset')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qE0nNT0ZBlhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance?\n",
        "Ans."
      ],
      "metadata": {
        "id": "oFYkyEy7BmoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with KD Tree\n",
        "knn_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "knn_kd.fit(X_train_scaled, y_train)\n",
        "y_pred_kd = knn_kd.predict(X_test_scaled)\n",
        "accuracy_kd = accuracy_score(y_test, y_pred_kd)\n",
        "\n",
        "# KNN with Ball Tree\n",
        "knn_ball = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "knn_ball.fit(X_train_scaled, y_train)\n",
        "y_pred_ball = knn_ball.predict(X_test_scaled)\n",
        "accuracy_ball = accuracy_score(y_test, y_pred_ball)\n",
        "\n",
        "# Print results\n",
        "print(\"KD Tree Accuracy:\", accuracy_kd)\n",
        "print(\"Ball Tree Accuracy:\", accuracy_ball)\n"
      ],
      "metadata": {
        "id": "xsw-on_gBxpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Train a PCA model on a high-dimensional dataset and visualize the Scree plot?\n",
        "Ans"
      ],
      "metadata": {
        "id": "5LqxiUNbByub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load high-dimensional dataset (64 features)\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Plot the Scree plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
        "plt.title(\"Scree Plot: Explained Variance by PCA Components\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zXl7kCLSCGA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
        "Ans."
      ],
      "metadata": {
        "id": "y0vM9W-4CN-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "yRiMMtaACdyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
        "Ans."
      ],
      "metadata": {
        "id": "o0pw_67hCeme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different PCA components and track accuracy\n",
        "component_range = range(1, X.shape[1] + 1)\n",
        "accuracy_scores = []\n",
        "\n",
        "for n_components in component_range:\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "upFsblXiCoaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Train a KNN Classifier with different leaf_size values and compare accuracy"
      ],
      "metadata": {
        "id": "nQMd3oGsCpj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Try different leaf_size values\n",
        "leaf_sizes = range(5, 51, 5)\n",
        "accuracy_scores = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree', leaf_size=leaf_size)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(leaf_sizes, accuracy_scores, marker='o')\n",
        "plt.title(\"KNN Classifier Accuracy vs. leaf_size\")\n",
        "plt.xlabel(\"leaf_size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w2C-TntHCxnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Train a PCA model and visualize how data points are transformed before and after PCA\n",
        "Ans."
      ],
      "metadata": {
        "id": "T5oPbJKBDBTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce to 2 dimensions\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot original feature space (first 2 features) and PCA space\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Original space (first 2 standardized features)\n",
        "for target in set(y):\n",
        "    axes[0].scatter(\n",
        "        X_scaled[y == target, 0],\n",
        "        X_scaled[y == target, 1],\n",
        "        label=target_names[target],\n",
        "        alpha=0.7\n",
        "    )\n",
        "axes[0].set_title(\"Original Feature Space (First 2 Scaled Features)\")\n",
        "axes[0].set_xlabel(\"Feature 1\")\n",
        "axes[0].set_ylabel(\"Feature 2\")\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# PCA space\n",
        "for target in set(y):\n",
        "    axes[1].scatter(\n",
        "        X_pca[y == target, 0],\n",
        "        X_pca[y == target, 1],\n",
        "        label=target_names[target],\n",
        "        alpha=0.7\n",
        "    )\n",
        "axes[1].set_title(\"PCA-Transformed Space (First 2 Components)\")\n",
        "axes[1].set_xlabel(\"Principal Component 1\")\n",
        "axes[1].set_ylabel(\"Principal Component 2\")\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kQL1mUDfDXFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report\n",
        "Ans."
      ],
      "metadata": {
        "id": "y52Iu90WDadT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "target_names = wine.target_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "report = classification_report(y_test, y_pred, target_names=target_names)\n",
        "\n",
        "print(\"Classification Report for KNN on Wine Dataset:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "rcBmuo0EDr5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a KNN Regressor and analyze the effect of different distance metrics on prediction error\n",
        "Ans."
      ],
      "metadata": {
        "id": "Jj09c6DbDt_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create synthetic regression data\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=15, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Distance metrics to compare\n",
        "metrics = ['euclidean', 'manhattan', 'chebyshev']\n",
        "mse_scores = []\n",
        "\n",
        "# Train and evaluate KNN Regressor with each metric\n",
        "for metric in metrics:\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, metric=metric)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"{metric.capitalize()} Distance - Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Optional: plot the results\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(metrics, mse_scores, color='skyblue')\n",
        "plt.title(\"KNN Regressor - MSE by Distance Metric\")\n",
        "plt.xlabel(\"Distance Metric\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dDw7dQ48D9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.Train a KNN Classifier and evaluate using ROC-AUC score?\n",
        "Ans."
      ],
      "metadata": {
        "id": "J42oknhwD-o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = knn.predict_proba(X_test_scaled)[:, 1]  # Probability for positive class\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "DjgIYjZREXxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a PCA model and visualize the variance captured by each principal component\n",
        "Ans."
      ],
      "metadata": {
        "id": "L34S-UnuEbMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train PCA model\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot the explained variance ratio\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "        pca.explained_variance_ratio_,\n",
        "        alpha=0.7,\n",
        "        color='skyblue')\n",
        "plt.title(\"Explained Variance by Each Principal Component\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explained Variance Ratio\")\n",
        "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "f22HFfH4ErkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Train a KNN Classifier and perform feature selection before training\n",
        "Ans."
      ],
      "metadata": {
        "id": "5N9oJ1r8Esea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Feature selection: select top 8 features\n",
        "selector = SelectKBest(score_func=f_classif, k=8)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Train KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = knn.predict(X_test_selected)\n",
        "print(\"Classification Report after Feature Selection:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "id": "0DP284AtE4_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a PCA model and visualize the data reconstruction error after reducing dimensions?\n",
        "Ans."
      ],
      "metadata": {
        "id": "MNeMafHUFDJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce to a chosen number of components (e.g., 2)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Reconstruct data from reduced representation\n",
        "X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "# Compute reconstruction error (mean squared error per sample)\n",
        "reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)\n",
        "\n",
        "# Plot reconstruction error\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(reconstruction_error, bins=30, color='salmon', edgecolor='black', alpha=0.7)\n",
        "plt.title(\"Distribution of Reconstruction Errors (PCA with 2 Components)\")\n",
        "plt.xlabel(\"Reconstruction Error\")\n",
        "plt.ylabel(\"Number of Samples\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nB5brB5cFJyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train a KNN Classifier and visualize the decision boundary?\n",
        "Ans."
      ],
      "metadata": {
        "id": "ZSR2Gs_yFODj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "# Standardize and reduce to 2 components using PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_pca, y)\n",
        "\n",
        "# Create mesh grid\n",
        "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "\n",
        "# Predict on grid points\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "for idx, label in enumerate(np.unique(y)):\n",
        "    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1],\n",
        "                label=target_names[label], edgecolor='k')\n",
        "plt.title(\"KNN Decision Boundary (PCA-reduced Wine Data)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K6ZOBqBfFWr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "Ans."
      ],
      "metadata": {
        "id": "JgTFmFEAFX1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA with all components\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot cumulative variance vs number of components\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', color='navy')\n",
        "plt.title(\"Cumulative Explained Variance by Number of PCA Components\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.axhline(y=0.95, color='red', linestyle='--', label='95% Variance Threshold')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qgjTTMDxFiWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}